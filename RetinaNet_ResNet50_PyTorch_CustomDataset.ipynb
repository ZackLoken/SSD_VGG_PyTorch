{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Reading and Cleaning Annotation Data for Custom PyTorch Object Detection**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.cache_size = 0  # disable cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # restrict cuda to gpu 0\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # set CUDA kernel to synchronous\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion(); # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load annotation data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading JSON as dictionary\n",
    "def read_json(filename: str) -> dict:\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Reading {filename} file encountered an error: {e}\")\n",
    "    return data\n",
    "\n",
    "# Function to create a DataFrame from a list of records\n",
    "def create_dataframe(data: list) -> pd.DataFrame:\n",
    "    # Normalize the column levels and create a DataFrame\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Main function to iterate over files in directory and add to df\n",
    "def main():\n",
    "    # Assign directory and empty list for collecting records\n",
    "    directory = \"C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Annotations/\"  # annotation directory\n",
    "    records = []\n",
    "    \n",
    "    # Iterate over files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            # Read the JSON file as python dictionary \n",
    "            data = read_json(filename=f)\n",
    "        \n",
    "            # Create the dataframe for the array items in annotations key \n",
    "            df = create_dataframe(data=data['annotations'])\n",
    "            df.insert(loc=0, column='img_name', value=f'{f[-30:-5]}.JPG')\n",
    "        \n",
    "            df.rename(columns={\n",
    "                \"img_name\": \"img_name\",\n",
    "                \"name\": \"label\",\n",
    "                \"bounding_box.h\": \"bbox_height\",\n",
    "                \"bounding_box.w\": \"bbox_width\",\n",
    "                \"bounding_box.x\": \"bbox_x_topLeft\",\n",
    "                \"bounding_box.y\": \"bbox_y_topLeft\",\n",
    "                \"polygon.paths\": \"polygon_path\"\n",
    "            }, inplace=True)\n",
    "            \n",
    "            # Append the records to the list\n",
    "            records.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping non-file: {filename}\")\n",
    "\n",
    "    # Concatenate all records into a single DataFrame\n",
    "    annos_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "    # Convert x, y, h, w to xmin, ymin, xmax, ymax\n",
    "    annos_df['xmin'] = annos_df['bbox_x_topLeft']\n",
    "    annos_df['ymin'] = annos_df['bbox_y_topLeft']\n",
    "    annos_df['xmax'] = annos_df['bbox_x_topLeft'] + annos_df['bbox_width']\n",
    "    annos_df['ymax'] = annos_df['bbox_y_topLeft'] + annos_df['bbox_height']\n",
    "  \n",
    "    # Drop unnecessary columns \n",
    "    annos_df = annos_df.drop(columns=['bbox_height', 'bbox_width', 'bbox_x_topLeft', \n",
    "                                      'bbox_y_topLeft', 'id', 'slot_names', 'polygon_path'])\n",
    "        \n",
    "    return annos_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-process annotation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique image names\n",
    "unique_img_names = df['img_name'].unique()\n",
    "\n",
    "invalid_img_names = []\n",
    "for img_name in unique_img_names:\n",
    "    img_path = f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Images/{img_name}'\n",
    "    img = Image.open(img_path)\n",
    "    if img.size == (5184, 3888):\n",
    "        invalid_img_names.append(img_name)\n",
    "\n",
    "# remove invalid images from df\n",
    "df = df[~df['img_name'].isin(invalid_img_names)]\n",
    "\n",
    "img_classes_to_remove = ['WTDE', 'TURT', 'NUTR', 'ANHI', 'CAGO', \n",
    "                         'DCCO', 'GWFG', 'GBHE', 'COGA', 'PBGR'] # remove images with these classes\n",
    "\n",
    "for class_label in img_classes_to_remove:\n",
    "    # Get all image names with the class\n",
    "    images_with_class = df[df['label'] == class_label]['img_name'].unique()\n",
    "\n",
    "    # Remove all rows for img\n",
    "    df = df[~df['img_name'].isin(images_with_class)]\n",
    "\n",
    "# remove images containing only hens\n",
    "hen_images_no_other_class = df[(df['label'] == 'Hen') & (~df['img_name'].isin(df[df['label'] != 'Hen']['img_name']))]['img_name'].unique()\n",
    "df = df[~df['img_name'].isin(hen_images_no_other_class)]\n",
    "\n",
    "# Separate classes with less than 100 instances\n",
    "class_counts = df['label'].value_counts()\n",
    "other_classes = class_counts[class_counts < 100].index.tolist()\n",
    "positive_classes = class_counts[class_counts >= 100].index.tolist()\n",
    "\n",
    "# print class counts for each label\n",
    "print(\"Number of instances per class in cleaned dataset:\")\n",
    "for label in df['label'].unique():\n",
    "    print(f'{label}: {len(df[df[\"label\"] == label])}')\n",
    "\n",
    "# print other and positive classes\n",
    "print()\n",
    "print(f'Other classes: {other_classes}')\n",
    "print(f'Positive classes: {positive_classes}')\n",
    "\n",
    "# remove images with other classes\n",
    "for class_label in other_classes:\n",
    "    # Get all image names with the class\n",
    "    images_with_class = df[df['label'] == class_label]['img_name'].unique()\n",
    "\n",
    "    # Remove all rows for img\n",
    "    df = df[~df['img_name'].isin(images_with_class)]\n",
    "\n",
    "# confirm the only classes in df are positive classes\n",
    "assert len(df['label'].unique()) == len(positive_classes)\n",
    "\n",
    "# encode labels as int (reserve 0 for 'background')\n",
    "df['target'] = pd.Categorical(df['label']).codes + 1\n",
    "\n",
    "# filter out images with invalid bounding boxes\n",
    "df = df.groupby('img_name').filter(lambda x: ((x['xmin'] < x['xmax']) & (x['ymin'] < x['ymax'])).all())\n",
    "\n",
    "# Create a dictionary using df['label'] as the keys and df['target'] as the values\n",
    "label_dict = dict(zip(df['target'], df['label']))\n",
    "\n",
    "# Drop the original 'label' column from df\n",
    "df = df.drop(['label'], axis=1)\n",
    "\n",
    "# Rename 'target' column to 'label'\n",
    "df.rename(columns={'target': 'label'}, inplace=True)\n",
    "\n",
    "# Save df as csv in directory\n",
    "df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter images after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store unique img_names in filtered df as array\n",
    "img_names = df['img_name'].unique().tolist()\n",
    "\n",
    "# Create a new directory called 'filtered_images'\n",
    "new_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images'\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "else:\n",
    "    for file in os.listdir(new_dir):\n",
    "        os.remove(os.path.join(new_dir, file))\n",
    "\n",
    "# Copy images in img_names to new directory\n",
    "for img in img_names:\n",
    "    shutil.copy2(f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Images/{img}', new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Transform and Augment Image and Annotation Data for Custom PyTorch Object Detection**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "import torchvision.transforms.v2 as T\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAVdroneDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset Loader for Waterfowl Drone Imagery\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transforms):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            root_dir (string): Directory containing all images.\n",
    "            transforms (callable): Transformation to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.unique_image_names = self.df['img_name'].unique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_name = self.unique_image_names[idx]\n",
    "\n",
    "        # Isolate first row to prevent multiple instances of the same image\n",
    "        row = self.df[self.df['img_name'] == image_name].iloc[0]\n",
    "\n",
    "        image_path = os.path.join(self.root_dir, row['img_name'])\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # Convert to Tensor\n",
    "\n",
    "        # Bounding boxes and labels\n",
    "        boxes = self.df[self.df['img_name'] == image_name][['xmin', 'ymin', 'xmax', 'ymax']].values \n",
    "        labels = self.df[self.df['img_name'] == image_name]['label'].values\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)  # (n_objects)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Calculate area\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # Assume no crowd annotations\n",
    "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': tv_tensors.BoundingBoxes(boxes, format=tv_tensors.BoundingBoxFormat.XYXY, canvas_size=(image.shape[1], image.shape[2])),\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train: bool):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train (bool): Whether the transform is for training or validation/testing.\n",
    "    \"\"\"\n",
    "    transforms_list = [T.ToImage()]\n",
    "    \n",
    "    if train:\n",
    "        transforms_list.append(T.RandomIoUCrop(min_scale=0.5, max_scale=1.25)) \n",
    "        transforms_list.append(T.RandomApply([T.ColorJitter(brightness=0.2, contrast=0.25, saturation=0.2, hue=0.01)], p=0.5))\n",
    "        transforms_list.append(T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3))  \n",
    "        transforms_list.append(T.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3))\n",
    "        transforms_list.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms_list.append(T.ClampBoundingBoxes())  # Clamp bounding boxes to image boundaries\n",
    "        transforms_list.append(T.SanitizeBoundingBoxes())\n",
    "    \n",
    "    transforms_list.append(\n",
    "        T.Resize(\n",
    "            size=(810,),\n",
    "            max_size=1440,\n",
    "            interpolation=torchvision.transforms.InterpolationMode.BICUBIC\n",
    "        )\n",
    "    )\n",
    "    transforms_list.append(\n",
    "        T.ToDtype(\n",
    "            dtype=torch.float32,\n",
    "            scale=True\n",
    "        )\n",
    "    )\n",
    "    transforms_list.append(\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return T.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions for plotting image and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes are values in label_dict\n",
    "classes = list(label_dict.values())\n",
    "\n",
    "# reverse label dictionary for mapping predictions to classes\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "# distinct colors \n",
    "bbox_colors = [\n",
    "    \"#FF0000\",  # Red\n",
    "    \"#00FF00\",  # Green\n",
    "    \"#FFFF00\",  # Yellow\n",
    "    \"#FF00FF\",  # Magenta\n",
    "    \"#00FFFF\",  # Cyan\n",
    "    \"#FFC0CB\",  # Pink\n",
    "    \"#FFA500\",  # Orange\n",
    "    \"#800080\",  # Purple\n",
    "    \"#FFFFFF\",  # White\n",
    "    \"#FFD700\",  # Gold\n",
    "]\n",
    "\n",
    "# label color map for plotting color-coded boxes by class\n",
    "label_color_map = {k: bbox_colors[i] for i, k in enumerate(label_dict.keys())}\n",
    "\n",
    "# function for reshaping boxes \n",
    "def get_box(boxes):\n",
    "    boxes = np.array(boxes)\n",
    "    boxes = boxes.astype('float').reshape(-1, 4)\n",
    "    if boxes.shape[0] == 1 : return boxes\n",
    "    return np.squeeze(boxes)\n",
    "\n",
    "\n",
    "# function for plotting image\n",
    "def img_show(image, ax = None, figsize = (6, 9)):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.imshow(image)\n",
    "    return ax\n",
    " \n",
    "\n",
    "def plot_bbox(ax, boxes, labels):\n",
    "    # add box to the image and use label_color_map to color-code by bounding box class if exists else 'black'\n",
    "    ax.add_patch(plt.Rectangle((boxes[:, 0], boxes[:, 1]), boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1],\n",
    "                    fill = False,\n",
    "                    color = label_color_map[labels.item()] if labels.item() in label_color_map else 'black', \n",
    "                    linewidth = 1.25))\n",
    "    # add label text to bounding box using label_dict if label exists else labels\n",
    "    ax.text(boxes[:, 2], boxes[:, 3], \n",
    "            (label_dict[labels.item()] if labels.item() in label_dict else labels.item()),\n",
    "            fontsize = 8,\n",
    "            bbox = dict(facecolor = 'white', alpha = 0.8, pad = 0, edgecolor = 'none'),\n",
    "            color = 'black')\n",
    "\n",
    "\n",
    "# function for plotting all boxes and labels on the image using get_polygon, img_show, and plot_mask functions\n",
    "def plot_detections(image, boxes, labels, ax = None):\n",
    "    ax = img_show(image.permute(1, 2, 0), ax = ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = get_box(boxes[i])\n",
    "        plot_bbox(ax, box, labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot sample batch to confirm data loads and transforms correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample batch of data to custom PyTorch Dataset and Transform\n",
    "sample_dataset = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv', \n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images', \n",
    "                                transforms = get_transform(train = True))\n",
    "\n",
    "sample_data_loader = torch.utils.data.DataLoader(sample_dataset, batch_size = 8, shuffle=True, \n",
    "                                                collate_fn = utils.collate_fn, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_name = 'DJI_20221216103820_0121_Z.JPG'\n",
    "# image_path = f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/{image_name}'\n",
    "# image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# # Convert image to tensor\n",
    "# transform = T.ToTensor()\n",
    "# image_tensor = transform(image)\n",
    "\n",
    "# # get labels and bboxes\n",
    "# boxes = df[df['img_name'] == image_name][['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "# labels = df[df['img_name'] == image_name]['label'].values\n",
    "\n",
    "# # plot image with bounding boxes\n",
    "# plot_detections(image_tensor, boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store images and annotation targets from sample batch\n",
    "batch = next(iter(sample_data_loader))\n",
    "images, targets = batch\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "images = [np.clip(image, 0, 1) for image in images]\n",
    "\n",
    "# Plot all samples from batch in a grid of subplots\n",
    "plt.figure(figsize=(16, int(sample_data_loader.batch_size) * 5))\n",
    "for i in range(int(sample_data_loader.batch_size)):\n",
    "    ax = plt.subplot(int(sample_data_loader.batch_size), 2, 1 + i)\n",
    "    plot_detections(images[i], targets[i]['boxes'], targets[i]['labels'], ax=ax)\n",
    "    # Query the dataset to get the image name for the given image_id\n",
    "    image_id = targets[i]['image_id'].item()  # Convert tensor to integer\n",
    "    image_name = sample_dataset.unique_image_names[image_id]\n",
    "    plt.title(image_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use stratified sampling to split multi-label dataset into train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set random number generator for reproducible data splits\n",
    "rng = np.random.default_rng(np.random.MT19937(np.random.SeedSequence(710)))\n",
    "\n",
    "# Group annotations by image\n",
    "image_groups = df.groupby('img_name')\n",
    "\n",
    "# Create a dictionary to store the class distribution for each image\n",
    "image_class_distribution = {}\n",
    "\n",
    "# Populate the dictionary with class distributions\n",
    "for image_name, group in image_groups:\n",
    "    labels = group['label'].tolist()\n",
    "    image_class_distribution[image_name] = labels\n",
    "\n",
    "# Create a list of all image names and their corresponding labels\n",
    "all_images = list(image_class_distribution.keys())\n",
    "all_labels = [image_class_distribution[image] for image in all_images]\n",
    "\n",
    "# Use the most frequent label for each image for stratification\n",
    "representative_labels = [max(set(labels), key=labels.count) for labels in all_labels]\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.05\n",
    "\n",
    "# Perform stratified split using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=int(1/test_ratio), shuffle=True, random_state=710)\n",
    "\n",
    "train_val_indices, test_indices = next(skf.split(all_images, representative_labels))\n",
    "\n",
    "# Further split train+val into train and validation sets\n",
    "train_val_images = [all_images[idx] for idx in train_val_indices]\n",
    "train_val_labels = [representative_labels[idx] for idx in train_val_indices]\n",
    "\n",
    "skf_val = StratifiedKFold(n_splits=int(1/(val_ratio/(train_ratio + val_ratio))), shuffle=True, random_state=710)\n",
    "train_indices, val_indices = next(skf_val.split(train_val_images, train_val_labels))\n",
    "\n",
    "# Map image names to unique indices\n",
    "image_to_unique_index = {image: idx for idx, image in enumerate(df['img_name'].unique())}\n",
    "\n",
    "# Create lists of unique indices for each split\n",
    "train_indices = [image_to_unique_index[train_val_images[idx]] for idx in train_indices]\n",
    "val_indices = [image_to_unique_index[train_val_images[idx]] for idx in val_indices]\n",
    "test_indices = [image_to_unique_index[all_images[idx]] for idx in test_indices]\n",
    "\n",
    "# Function to get class distribution\n",
    "def get_class_distribution(images, image_class_distribution):\n",
    "    class_counts = defaultdict(int)\n",
    "    for image in images:\n",
    "        for label in image_class_distribution[image]:\n",
    "            class_counts[label] += 1\n",
    "    return class_counts\n",
    "\n",
    "# Get train, val, and test images\n",
    "train_images = [all_images[idx] for idx in train_indices]\n",
    "val_images = [all_images[idx] for idx in val_indices]\n",
    "test_images = [all_images[idx] for idx in test_indices]\n",
    "\n",
    "train_class_distribution = get_class_distribution(train_images, image_class_distribution)\n",
    "val_class_distribution = get_class_distribution(val_images, image_class_distribution)\n",
    "test_class_distribution = get_class_distribution(test_images, image_class_distribution)\n",
    "\n",
    "class_indices = {label: [] for label in df['label'].unique()}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    class_indices[row['label']].append(idx)\n",
    "\n",
    "train_class_distribution = {k: v / len(class_indices[k]) for k, v in train_class_distribution.items()}\n",
    "val_class_distribution = {k: v / len(class_indices[k]) for k, v in val_class_distribution.items()}\n",
    "test_class_distribution = {k: v / len(class_indices[k]) for k, v in test_class_distribution.items()}\n",
    "\n",
    "print(\"Train class distribution:\", dict(sorted(train_class_distribution.items())))\n",
    "print(\"Validation class distribution:\", dict(sorted(val_class_distribution.items())))\n",
    "print(\"Test class distribution:\", dict(sorted(test_class_distribution.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create weighted random sampler to handle class imbalances during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate class weights dynamically\n",
    "def calculate_class_weights(labels, hen_label_int, background_label_int):\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Remove the \"Hen\" class from the counts\n",
    "    hen_count = class_counts.pop(hen_label_int, None)\n",
    "    \n",
    "    # Identify the count for the second most-frequent class\n",
    "    second_most_frequent_class_count = max(class_counts.values())\n",
    "    \n",
    "    # Calculate the weight for the \"Hen\" class\n",
    "    hen_weight = second_most_frequent_class_count / hen_count if hen_count else 1.0\n",
    "\n",
    "    \n",
    "    # Assign weights to all classes\n",
    "    class_weights = {label: sum(class_counts.values()) / count for label, count in class_counts.items()}\n",
    "    \n",
    "    # Normalize weights to range [1, 2]\n",
    "    min_weight = min(class_weights.values())\n",
    "    max_weight = max(class_weights.values())\n",
    "    class_weights = {label: 1 + (weight - min_weight) / (max_weight - min_weight) for label, weight in class_weights.items()}\n",
    "    \n",
    "    # Add weight for the \"Hen\" class\n",
    "    class_weights[hen_label_int] = hen_weight + 0.1\n",
    "\n",
    "    # weight = 20% for the background class (penalize for false positives)\n",
    "    class_weights[background_label_int] = 0.2\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Store train labels for each image\n",
    "train_labels = [label for image in train_images for label in image_class_distribution[image]]\n",
    "\n",
    "# Calculate class weights dynamically\n",
    "hen_label_int = [key for key, value in label_dict.items() if value == 'Hen'][0]  # Get the integer label for \"Hen\"\n",
    "background_label_int = 0  # Assuming background is class 0\n",
    "class_weights = calculate_class_weights(train_labels, hen_label_int, background_label_int)\n",
    "\n",
    "# replace REDH value with 1.5 using label_dict\n",
    "redh_label_int = [key for key, value in label_dict.items() if value == 'REDH'][0]\n",
    "class_weights[redh_label_int] = 1.5\n",
    "\n",
    "# Convert class weights to a list in the correct order\n",
    "unique_labels = sorted(set(train_labels))\n",
    "train_class_weights = [class_weights[label] for label in unique_labels]\n",
    "\n",
    "# Add weight for the background class\n",
    "train_class_weights = [class_weights[background_label_int]] + train_class_weights  # Background is class 0\n",
    "train_class_weights = torch.tensor(train_class_weights, dtype=torch.float32)\n",
    "\n",
    "# print class counts and class weight for each class\n",
    "print(\"Train class instances and weights: \")\n",
    "for label in unique_labels:\n",
    "    print(f\"{label_dict[label]}: count = {train_labels.count(label)}, weight = {train_class_weights[label]}\")\n",
    "\n",
    "\n",
    "# Calculate sample weights for each image in the training dataset\n",
    "train_sample_weights = []\n",
    "for image_name in train_images:\n",
    "    labels = image_class_distribution[image_name]\n",
    "    sample_weight = sum(train_class_weights[label] for label in labels) / len(labels)\n",
    "    train_sample_weights.append(sample_weight)\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Anchor Sizes and Aspect Ratios of Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# resized_bounding_boxes = []\n",
    "\n",
    "# for images, targets in sample_data_loader:\n",
    "#     for target in targets:\n",
    "#         for box in target['boxes']:\n",
    "#             resized_bounding_boxes.append(box)\n",
    "\n",
    "# # Convert to numpy array\n",
    "# resized_bounding_boxes = np.array(resized_bounding_boxes)\n",
    "\n",
    "# # Print the resized bounding box dimensions\n",
    "# print(resized_bounding_boxes[:5])\n",
    "\n",
    "# Convert to [width, height] format\n",
    "# widths = resized_bounding_boxes[:, 2] - resized_bounding_boxes[:, 0]\n",
    "# heights = resized_bounding_boxes[:, 3] - resized_bounding_boxes[:, 1]\n",
    "# bounding_boxes_wh = np.stack((widths, heights), axis=1)\n",
    "\n",
    "# # filter out bounding boxes with width or height less than 25\n",
    "# bounding_boxes_wh = bounding_boxes_wh[(bounding_boxes_wh[:, 0] >= 25) & (bounding_boxes_wh[:, 1] >= 25)]\n",
    "\n",
    "# # Perform k-means clustering to find anchor sizes\n",
    "# num_clusters = 5  # Number of anchor sizes\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=710).fit(bounding_boxes_wh)\n",
    "# anchor_sizes = kmeans.cluster_centers_\n",
    "\n",
    "# # Print the anchor sizes\n",
    "# print(\"Anchor Sizes (width, height):\")\n",
    "# print(anchor_sizes)\n",
    "\n",
    "# # Determine aspect ratios from the anchor sizes\n",
    "# anchor_aspect_ratios = anchor_sizes[:, 0] / anchor_sizes[:, 1]\n",
    "\n",
    "# # Print the aspect ratios\n",
    "# print(\"Anchor Aspect Ratios:\")\n",
    "# print(anchor_aspect_ratios)\n",
    "\n",
    "# sorted_widths = np.sort(bounding_boxes_wh[:, 0])\n",
    "# sorted_heights = np.sort(bounding_boxes_wh[:, 1])\n",
    "\n",
    "# print(\"Smallest Widths:\")\n",
    "# print(sorted_widths[:5])\n",
    "# print(\"Largest Widths:\")\n",
    "# print(sorted_widths[-5:])\n",
    "# print(\"Smallest Heights:\")\n",
    "# print(sorted_heights[:5])\n",
    "# print(\"Largest Heights:\")\n",
    "# print(sorted_heights[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Custom RetinaNet with ResNet FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead, RetinaNetRegressionHead\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "import torchvision.models.detection._utils as det_utils\n",
    "from torchvision.ops import sigmoid_focal_loss, FrozenBatchNorm2d\n",
    "import torchvision.ops.boxes as box_ops\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "from pt_soft_nms import batched_soft_nms\n",
    "\n",
    "\n",
    "def _sum(x: List[torch.Tensor]) -> torch.Tensor:\n",
    "    res = x[0]\n",
    "    for i in x[1:]:\n",
    "        res = res + i\n",
    "    return res\n",
    "\n",
    "\n",
    "class CustomRetinaNetClassificationHead(RetinaNetClassificationHead):\n",
    "    def __init__(self, in_channels, num_anchors, num_classes, alpha=0.25, gamma_loss=2.0, prior_probability=0.01, \n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None, dropout_prob=0.25, class_weights=None, label_smoothing=0.1):\n",
    "        super().__init__(in_channels, num_anchors, num_classes, prior_probability, norm_layer)\n",
    "        self.alpha = alpha\n",
    "        self.gamma_loss = gamma_loss\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.class_weights = class_weights\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def compute_loss(self, targets, head_outputs, matched_idxs):\n",
    "        losses = []\n",
    "\n",
    "        cls_logits = head_outputs[\"cls_logits\"]\n",
    "\n",
    "        for i, (targets_per_image, cls_logits_per_image, matched_idxs_per_image) in enumerate(zip(targets, cls_logits, matched_idxs)):\n",
    "            # determine only the foreground\n",
    "            foreground_idxs_per_image = matched_idxs_per_image >= 0\n",
    "            num_foreground = foreground_idxs_per_image.sum()\n",
    "\n",
    "            # create the target classification\n",
    "            gt_classes_target = torch.zeros_like(cls_logits_per_image)\n",
    "            gt_classes_target += self.label_smoothing / (self.num_classes - 1) # smoothing for negative classes\n",
    "            gt_classes_target[\n",
    "                foreground_idxs_per_image,\n",
    "                targets_per_image[\"labels\"][matched_idxs_per_image[foreground_idxs_per_image]],\n",
    "            ] = 1.0 - self.label_smoothing # smoothing for positive classes\n",
    "\n",
    "            # find indices for which anchors should be ignored\n",
    "            valid_idxs_per_image = matched_idxs_per_image != self.BETWEEN_THRESHOLDS\n",
    "\n",
    "            # get the class weights for the valid indices\n",
    "            if self.class_weights is not None:\n",
    "                valid_labels = targets_per_image[\"labels\"][matched_idxs_per_image[valid_idxs_per_image]]\n",
    "                weights = self.class_weights.to(valid_labels.device)[valid_labels]\n",
    "            else:\n",
    "                weights = torch.ones_like(valid_idxs_per_image, dtype=torch.float32)\n",
    "\n",
    "            # compute the classification loss with custom alpha, gamma_loss, and class weights\n",
    "            losses.append(\n",
    "                (sigmoid_focal_loss(\n",
    "                    cls_logits_per_image[valid_idxs_per_image],\n",
    "                    gt_classes_target[valid_idxs_per_image],\n",
    "                    alpha=self.alpha,\n",
    "                    gamma=self.gamma_loss,\n",
    "                    reduction=\"none\",\n",
    "                ) * weights.unsqueeze(1)).sum() / max(1, num_foreground)\n",
    "            )\n",
    "\n",
    "        return _sum(losses) / len(targets)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_cls_logits = []\n",
    "        for features in x:\n",
    "            cls_logits = self.conv(features)\n",
    "            cls_logits = self.dropout(cls_logits)  # Apply dropout\n",
    "            cls_logits = self.cls_logits(cls_logits)\n",
    "\n",
    "            # Permute classification output from (N, A * K, H, W) to (N, HWA, K).\n",
    "            N, _, H, W = cls_logits.shape\n",
    "            cls_logits = cls_logits.view(N, -1, self.num_classes, H, W)\n",
    "            cls_logits = cls_logits.permute(0, 3, 4, 1, 2)\n",
    "            cls_logits = cls_logits.reshape(N, -1, self.num_classes)  # Size=(N, HWA, K)\n",
    "\n",
    "            all_cls_logits.append(cls_logits)\n",
    "\n",
    "\n",
    "        return torch.cat(all_cls_logits, dim=1)\n",
    "\n",
    "\n",
    "class CustomRetinaNetRegressionHead(RetinaNetRegressionHead):\n",
    "    def __init__(self, in_channels, num_anchors, norm_layer: Optional[Callable[..., nn.Module]] = None, _loss_type=\"smooth_l1\", beta_loss=0.5, lambda_loss=1.5, dropout_prob=0.25):\n",
    "        super().__init__(in_channels, num_anchors, norm_layer)\n",
    "        self._loss_type = _loss_type\n",
    "        self.beta_loss = beta_loss # beta < 1 helps counter early plateauing\n",
    "        self.lambda_loss = lambda_loss # lambda > 1 places more emphasis on localization loss\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "    \n",
    "    def compute_loss(self, targets, head_outputs, anchors, matched_idxs):\n",
    "        # type: (List[Dict[str, torch.Tensor]], Dict[str, torch.Tensor], List[torch.Tensor], List[torch.Tensor]) -> torch.Tensor\n",
    "        losses = []\n",
    "\n",
    "        bbox_regression = head_outputs[\"bbox_regression\"]\n",
    "\n",
    "        for targets_per_image, bbox_regression_per_image, anchors_per_image, matched_idxs_per_image in zip(\n",
    "            targets, bbox_regression, anchors, matched_idxs\n",
    "        ):\n",
    "            # determine only the foreground indices, ignore the rest\n",
    "            foreground_idxs_per_image = torch.where(matched_idxs_per_image >= 0)[0]\n",
    "            num_foreground = foreground_idxs_per_image.numel()\n",
    "\n",
    "            # select only the foreground boxes\n",
    "            matched_gt_boxes_per_image = targets_per_image[\"boxes\"][matched_idxs_per_image[foreground_idxs_per_image]]\n",
    "            bbox_regression_per_image = bbox_regression_per_image[foreground_idxs_per_image, :]\n",
    "            anchors_per_image = anchors_per_image[foreground_idxs_per_image, :]\n",
    "\n",
    "            # compute the loss\n",
    "            losses.append(\n",
    "                    det_utils._box_loss(\n",
    "                    self._loss_type,\n",
    "                    self.box_coder,\n",
    "                    anchors_per_image,\n",
    "                    matched_gt_boxes_per_image,\n",
    "                    bbox_regression_per_image,\n",
    "                    cnf={'beta': self.beta_loss}, \n",
    "                ) * self.lambda_loss / max(1, num_foreground)\n",
    "            )\n",
    "\n",
    "        return _sum(losses) / max(1, len(targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_bbox_regression = []\n",
    "        for features in x:\n",
    "            bbox_regression = self.conv(features)\n",
    "            bbox_regression = self.dropout(bbox_regression)  # Apply dropout\n",
    "            bbox_regression = self.bbox_reg(bbox_regression)\n",
    "\n",
    "            # Permute bbox regression output from (N, 4 * A, H, W) to (N, HWA, 4).\n",
    "            N, _, H, W = bbox_regression.shape\n",
    "            bbox_regression = bbox_regression.view(N, -1, 4, H, W)\n",
    "            bbox_regression = bbox_regression.permute(0, 3, 4, 1, 2)\n",
    "            bbox_regression = bbox_regression.reshape(N, -1, 4)  # Size=(N, HWA, 4)\n",
    "\n",
    "            all_bbox_regression.append(bbox_regression)\n",
    "\n",
    "        return torch.cat(all_bbox_regression, dim=1)\n",
    "\n",
    "class CustomRetinaNet(RetinaNet):\n",
    "    def filter_overlapping_predictions(self, boxes, scores, labels, max_iou_threshold=0.7):\n",
    "        \"\"\"Remove boxes that overlap too much after soft-NMS\"\"\"\n",
    "        if len(boxes) <= 1:\n",
    "            return torch.arange(len(boxes), device=boxes.device)\n",
    "            \n",
    "        keep_indices = []\n",
    "        \n",
    "        # Sort by score descending\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        while len(sorted_indices) > 0:\n",
    "            # Keep highest scoring box\n",
    "            current_idx = sorted_indices[0]\n",
    "            keep_indices.append(current_idx)\n",
    "            \n",
    "            if len(sorted_indices) == 1:\n",
    "                break\n",
    "                \n",
    "            # Get IoUs of highest scoring box with all remaining boxes\n",
    "            current_box = boxes[current_idx].unsqueeze(0)\n",
    "            other_boxes = boxes[sorted_indices[1:]]\n",
    "            ious = box_ops.box_iou(current_box, other_boxes).squeeze(0)\n",
    "            \n",
    "            # Keep boxes with IoU below threshold with the current box\n",
    "            keep_mask = ious <= max_iou_threshold\n",
    "            sorted_indices = sorted_indices[1:][keep_mask]\n",
    "        \n",
    "        return torch.tensor(keep_indices, device=boxes.device)\n",
    "\n",
    "    def postprocess_detections(self, head_outputs, anchors, image_shapes):\n",
    "        # type: (Dict[str, List[torch.Tensor]], List[List[torch.Tensor]], List[Tuple[int, int]]) -> List[Dict[str, torch.Tensor]]\n",
    "        class_logits = head_outputs[\"cls_logits\"]\n",
    "        box_regression = head_outputs[\"bbox_regression\"]\n",
    "\n",
    "        num_images = len(image_shapes)\n",
    "\n",
    "        detections: List[Dict[str, torch.Tensor]] = []\n",
    "\n",
    "        for index in range(num_images):\n",
    "            box_regression_per_image = [br[index] for br in box_regression]\n",
    "            logits_per_image = [cl[index] for cl in class_logits]\n",
    "            anchors_per_image, image_shape = anchors[index], image_shapes[index]\n",
    "\n",
    "            image_boxes = []\n",
    "            image_scores = []\n",
    "            image_labels = []\n",
    "\n",
    "            for box_regression_per_level, logits_per_level, anchors_per_level in zip(\n",
    "                box_regression_per_image, logits_per_image, anchors_per_image\n",
    "            ):\n",
    "                num_classes = logits_per_level.shape[-1]\n",
    "\n",
    "                # remove low scoring boxes\n",
    "                scores_per_level = torch.sigmoid(logits_per_level).flatten()\n",
    "                keep_idxs = scores_per_level > self.score_thresh\n",
    "                scores_per_level = scores_per_level[keep_idxs]\n",
    "                topk_idxs = torch.where(keep_idxs)[0]\n",
    "\n",
    "                # keep only topk scoring predictions\n",
    "                num_topk = det_utils._topk_min(topk_idxs, self.topk_candidates, 0)\n",
    "                scores_per_level, idxs = scores_per_level.topk(num_topk)\n",
    "                topk_idxs = topk_idxs[idxs]\n",
    "\n",
    "                anchor_idxs = torch.div(topk_idxs, num_classes, rounding_mode=\"floor\")\n",
    "                labels_per_level = topk_idxs % num_classes\n",
    "\n",
    "                boxes_per_level = self.box_coder.decode_single(\n",
    "                    box_regression_per_level[anchor_idxs], anchors_per_level[anchor_idxs]\n",
    "                )\n",
    "                boxes_per_level = box_ops.clip_boxes_to_image(boxes_per_level, image_shape)\n",
    "\n",
    "                image_boxes.append(boxes_per_level)\n",
    "                image_scores.append(scores_per_level)\n",
    "                image_labels.append(labels_per_level)\n",
    "\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)\n",
    "            image_scores = torch.cat(image_scores, dim=0)\n",
    "            image_labels = torch.cat(image_labels, dim=0)\n",
    "\n",
    "            # soft non-maximum suppression\n",
    "            keep = batched_soft_nms(image_boxes, image_scores, image_labels, sigma=0.5, score_threshold=0.15)\n",
    "            \n",
    "            # Apply additional IoU-based filtering for each class\n",
    "            filtered_keep = []\n",
    "            for class_id in image_labels[keep].unique():\n",
    "                class_mask = image_labels[keep] == class_id\n",
    "                class_indices = torch.where(class_mask)[0]\n",
    "                \n",
    "                # Only apply filtering if we have multiple boxes for this class\n",
    "                if len(class_indices) > 1:\n",
    "                    class_boxes = image_boxes[keep][class_indices]\n",
    "                    class_scores = image_scores[keep][class_indices]\n",
    "                    class_labels = image_labels[keep][class_indices]\n",
    "                    \n",
    "                    # Get indices to keep after filtering\n",
    "                    class_keep = self.filter_overlapping_predictions(\n",
    "                        class_boxes, \n",
    "                        class_scores, \n",
    "                        class_labels, \n",
    "                        max_iou_threshold=0.6\n",
    "                    )\n",
    "                    \n",
    "                    # Map back to original indices\n",
    "                    filtered_keep.append(keep[class_indices[class_keep]])\n",
    "                else:\n",
    "                    # If only one box, keep it\n",
    "                    filtered_keep.append(keep[class_indices])\n",
    "            \n",
    "            # Combine all kept indices and sort by score\n",
    "            if filtered_keep:\n",
    "                keep = torch.cat(filtered_keep)\n",
    "                keep = keep[image_scores[keep].argsort(descending=True)]\n",
    "                \n",
    "            # Limit to max detections per image\n",
    "            keep = keep[: self.detections_per_img]\n",
    "\n",
    "            detections.append(\n",
    "                {\n",
    "                    \"boxes\": image_boxes[keep],\n",
    "                    \"scores\": image_scores[keep],\n",
    "                    \"labels\": image_labels[keep],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retinanet_model(depth, num_classes=12, min_size=810, max_size=1440, image_mean=[0, 0, 0], image_std=[1, 1, 1], score_thresh=0.1, nms_thresh=0.4, \n",
    "                        detections_per_img=200, fg_iou_thresh=0.4, bg_iou_thresh=0.2, topk_candidates=200, alpha=0.75, gamma_loss=3.0, class_weights=None,\n",
    "                        beta_loss=0.5, lambda_loss=1.5, dropout_prob=0.25):\n",
    "    \n",
    "    trainable_backbone_layers = 0 # set constant, adjust later with function\n",
    "\n",
    "    # Create the backbone with FPN\n",
    "    if depth == 18:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet18', \n",
    "                                       weights=torchvision.models.ResNet18_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 34:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet34', \n",
    "                                       weights=torchvision.models.ResNet34_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 50:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet50', \n",
    "                                       weights=torchvision.models.ResNet50_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 101:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet101', \n",
    "                                       weights=torchvision.models.ResNet101_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 152:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet152', \n",
    "                                       weights=torchvision.models.ResNet152_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model depth\")\n",
    "\n",
    "    # Create the RetinaNet model with the custom backbone\n",
    "    model = CustomRetinaNet(backbone, \n",
    "                      num_classes=num_classes,\n",
    "                      min_size=min_size, # same size as resize in transform to keep aspect ratio\n",
    "                      max_size=max_size,\n",
    "                      image_mean=image_mean,\n",
    "                      image_std=image_std,\n",
    "                      score_thresh=score_thresh, \n",
    "                      nms_thresh=nms_thresh, \n",
    "                      detections_per_img=detections_per_img,\n",
    "                      fg_iou_thresh=fg_iou_thresh,\n",
    "                      bg_iou_thresh=bg_iou_thresh,\n",
    "                      topk_candidates=topk_candidates\n",
    "                      )\n",
    "\n",
    "    # Replace the classification head with the custom one\n",
    "    in_channels = model.head.classification_head.cls_logits.in_channels\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "    model.head.classification_head = CustomRetinaNetClassificationHead(in_channels, \n",
    "                                                                       num_anchors, \n",
    "                                                                       num_classes, \n",
    "                                                                       alpha=alpha, \n",
    "                                                                       gamma_loss=gamma_loss, \n",
    "                                                                       dropout_prob=dropout_prob,\n",
    "                                                                       class_weights=class_weights)\n",
    "\n",
    "    # Replace the regression head with the custom one\n",
    "    model.head.regression_head = CustomRetinaNetRegressionHead(in_channels, \n",
    "                                                               num_anchors, \n",
    "                                                               _loss_type=\"smooth_l1\",\n",
    "                                                               beta_loss=beta_loss,\n",
    "                                                               lambda_loss=lambda_loss,\n",
    "                                                               dropout_prob=dropout_prob)\n",
    "    \n",
    "    model.anchor_generator = AnchorGenerator(sizes=((32, 40, 50), (64, 80, 101), (128, 161, 203), (256, 322, 406), (512, 645, 812)), \n",
    "                                             aspect_ratios=((0.75, 1.25, 1.75), (0.75, 1.25, 1.75), (0.75, 1.25, 1.75), (0.75, 1.25, 1.75), (0.75, 1.25, 1.75)))\n",
    "\n",
    "    return model\n",
    "\n",
    "print(get_retinanet_model(depth=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Tune Model Hyperparameters using Ray Tune**</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class for tuning RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import ray.cloudpickle as pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import math\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "from engine_gradientAccumulation import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "\n",
    "# Set random seed for reproducible training\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def extract_per_class_metrics(coco_eval, coco_gt):\n",
    "    per_class_metrics = {}\n",
    "\n",
    "    # Create a list of category IDs in the order they appear in the evaluation results\n",
    "    cat_ids = list(coco_gt.cats.keys())\n",
    "    cat_id_to_index = {cat_id: idx for idx, cat_id in enumerate(cat_ids)}\n",
    "\n",
    "    for cat_id, idx in cat_id_to_index.items():\n",
    "        try:\n",
    "            precision = coco_eval.coco_eval['bbox'].eval['precision'][:, :, idx, 0, 2]\n",
    "            recall = coco_eval.coco_eval['bbox'].eval['recall'][:, idx, 0, 2]\n",
    "\n",
    "            per_class_metrics[cat_id] = {\n",
    "                'precision': precision.mean(),\n",
    "                'recall': recall.mean()\n",
    "            }\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError for category ID {cat_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return per_class_metrics\n",
    "\n",
    "\n",
    "def adjust_trainable_layers(model, trainable_layers):\n",
    "    \"\"\"\n",
    "    Adjust the trainable layers in the RetinaNet backbone (model.backbone.body).\n",
    "    Unfreeze the last `trainable_layers` residual blocks and replace their FrozenBatchNorm2d layers.\n",
    "    When trainable_layers=5, also unfreeze conv1 and replace bn1 with trainable BatchNorm2d.\n",
    "    \"\"\"\n",
    "    def convert_frozen_bn(frozen_bn):\n",
    "        device = frozen_bn.running_mean.device  # Get the device of the frozen BN layer\n",
    "        num_features = frozen_bn.weight.shape[0]\n",
    "        bn = torch.nn.BatchNorm2d(num_features)\n",
    "        \n",
    "        # Move the new BN to the same device before initializing\n",
    "        bn = bn.to(device)\n",
    "        \n",
    "        # Initialize with existing batch norm parameters\n",
    "        bn.running_mean = frozen_bn.running_mean.clone()\n",
    "        bn.running_var = frozen_bn.running_var.clone()\n",
    "        torch.nn.init.normal_(bn.weight, mean=1.0, std=0.02)\n",
    "        torch.nn.init.constant_(bn.bias, 0)\n",
    "        \n",
    "        return bn\n",
    "\n",
    "    # Collect backbone blocks\n",
    "    backbone_layers = []\n",
    "    for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        if hasattr(model.backbone.body, layer_name):\n",
    "            backbone_layers.append(getattr(model.backbone.body, layer_name))\n",
    "\n",
    "    if trainable_layers > 5:\n",
    "        print(f\"Requested trainable_layers ({trainable_layers}) exceeds available layers (5). Using 5 instead.\")\n",
    "        trainable_layers = 5\n",
    "\n",
    "    # Unfreeze the last `trainable_layers` blocks\n",
    "    for block in backbone_layers[-trainable_layers:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Replace FrozenBatchNorm2d in backbone layers\n",
    "    for name, module in model.backbone.body.named_modules():\n",
    "        if isinstance(module, FrozenBatchNorm2d):\n",
    "            if 'layer' in name:\n",
    "                layer_num = int(name.split('.')[0][-1])\n",
    "                if layer_num > (4 - trainable_layers):\n",
    "                    parent_name = '.'.join(name.split('.')[:-1])\n",
    "                    module_name = name.split('.')[-1]\n",
    "                    parent = dict(model.backbone.body.named_modules())[parent_name]\n",
    "                    setattr(parent, module_name, convert_frozen_bn(module))\n",
    "            elif trainable_layers == 5 and name == 'bn1':\n",
    "                # Replace the initial frozen batch norm layer\n",
    "                model.backbone.body.bn1 = convert_frozen_bn(module)\n",
    "                # Ensure conv1 is trainable\n",
    "                model.backbone.body.conv1.weight.requires_grad = True\n",
    "\n",
    "\n",
    "class RetinaNetTuner:\n",
    "    def __init__(self, num_samples, restore_path=\"\"):\n",
    "        self.num_samples = num_samples\n",
    "        self.restore_path = restore_path\n",
    "\n",
    "    def create_coco_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            train_future = executor.submit(get_coco_api_from_dataset, train_dataset)\n",
    "            val_future = executor.submit(get_coco_api_from_dataset, val_dataset)\n",
    "            test_future = executor.submit(get_coco_api_from_dataset, test_dataset)\n",
    "            train_coco_ds = train_future.result()\n",
    "            val_coco_ds = val_future.result()\n",
    "            test_coco_ds = test_future.result()\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds\n",
    "    \n",
    "    def train_lr_finder(self, config):\n",
    "        class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "            def inputs_labels_from_batch(self, batch_data):\n",
    "                inputs = [image.to('cuda:0') for image in batch_data[0]]\n",
    "                labels = [{k: v.to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "                return inputs, labels\n",
    "\n",
    "        dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "        accumulation_steps = 1  ## FIXME: hardcoded for now\n",
    "\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=config[\"batch_size\"],\n",
    "                                                        sampler=config[\"train_sampler\"],\n",
    "                                                        collate_fn=utils.collate_fn,\n",
    "                                                        num_workers=0, pin_memory=True)\n",
    "\n",
    "        model = get_retinanet_model(\n",
    "            depth=50,\n",
    "            num_classes=len(config[\"class_weights\"]),\n",
    "            score_thresh=config[\"score_thresh\"],\n",
    "            nms_thresh=config[\"nms_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=200,\n",
    "            alpha=config[\"alpha\"],\n",
    "            gamma_loss=config[\"gamma_loss\"],\n",
    "            class_weights=config[\"class_weights\"],\n",
    "            beta_loss=config[\"beta_loss\"],\n",
    "            lambda_loss=config[\"lambda_loss\"],\n",
    "            dropout_prob=config[\"dropout\"],\n",
    "        ).to('cuda:0')\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params, lr=1e-7, momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "        grad_scaler = torch.GradScaler()\n",
    "\n",
    "        class CustomLRFinder(LRFinder):\n",
    "            def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "                super().__init__(model, optimizer, criterion, device)\n",
    "                self.amp_backend = amp_backend\n",
    "                self.amp_config = amp_config\n",
    "                self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "            def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                for _ in range(accumulation_steps):\n",
    "                    inputs, labels = next(train_iter)\n",
    "                    inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                    with torch.autocast(device_type=\"cuda:0\"):\n",
    "                        outputs = self.model(inputs, labels)\n",
    "                        loss = sum(loss for loss in outputs.values())\n",
    "\n",
    "                    loss /= accumulation_steps\n",
    "                    self.grad_scaler.scale(loss).backward()\n",
    "                    total_loss += loss\n",
    "\n",
    "                self.grad_scaler.step(self.optimizer)\n",
    "                self.grad_scaler.update()\n",
    "\n",
    "                return total_loss.item()\n",
    "\n",
    "        lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "        lr_finder.range_test(train_iter, end_lr=1, num_iter=len(data_loader_train), step_mode='exp', accumulation_steps=accumulation_steps) # num_iter = len(dataloader) to sample from full train dataset\n",
    "        suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "        lr_finder.reset()\n",
    "\n",
    "        # return default if torch lr finder fails\n",
    "        try:\n",
    "            if isinstance(suggested_lr, tuple):\n",
    "                axes, suggested_lr_value = suggested_lr\n",
    "                return suggested_lr_value\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during learning rate finding: {e}\")\n",
    "            # Return a default learning rate if an error occurs\n",
    "            return 5e-4\n",
    "    \n",
    "    def train_MAVdroneDataset(self, config):\n",
    "        import pickle, tempfile\n",
    "        from pathlib import Path\n",
    "        set_seed(710)\n",
    "\n",
    "        dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "        data_loader_val = ray.get(config[\"data_loader_val_ref\"])\n",
    "        train_coco_ds = ray.get(config[\"train_coco_ds_ref\"])\n",
    "        val_coco_ds = ray.get(config[\"val_coco_ds_ref\"])\n",
    "\n",
    "        training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 1, \"trainable_layers\": 0, \"improvement_threshold\": 0.01, \"variance_threshold\": 1e-4}, \n",
    "        # {\"step\": 1, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 2, \"trainable_layers\": 1, \"improvement_threshold\": 0.008, \"variance_threshold\": 5e-5}, \n",
    "        # {\"step\": 2, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 4, \"trainable_layers\": 2, \"improvement_threshold\": 0.005, \"variance_threshold\": 2.5e-5}, \n",
    "        # {\"step\": 3, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 8, \"trainable_layers\": 3, \"improvement_threshold\": 0.003, \"variance_threshold\": 1e-5}, \n",
    "        # {\"step\": 4, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 16, \"trainable_layers\": 4, \"improvement_threshold\": 0.002, \"variance_threshold\": 5e-6}, \n",
    "        # {\"step\": 5, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 32, \"trainable_layers\": 5, \"improvement_threshold\": 0.001, \"variance_threshold\": 2.5e-6}, # bs 64\n",
    "    ]\n",
    "\n",
    "        # Instantiate model and optimizer only once\n",
    "        model = get_retinanet_model(\n",
    "            depth=50,\n",
    "            num_classes=len(config[\"class_weights\"]),\n",
    "            score_thresh=config[\"score_thresh\"],\n",
    "            nms_thresh=config[\"nms_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=200,\n",
    "            alpha=config[\"alpha\"],\n",
    "            gamma_loss=config[\"gamma_loss\"],\n",
    "            class_weights=config[\"class_weights\"],\n",
    "            beta_loss=config[\"beta_loss\"],\n",
    "            lambda_loss=config[\"lambda_loss\"],\n",
    "            dropout_prob=config[\"dropout\"],\n",
    "        )\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=config[\"lr\"],\n",
    "                                    momentum=config[\"momentum\"],\n",
    "                                    weight_decay=config[\"weight_decay\"],\n",
    "                                    nesterov=True)\n",
    "        \n",
    "        # Check for an existing checkpoint and load state if available.\n",
    "        checkpoint = train.get_checkpoint()\n",
    "        if checkpoint:\n",
    "            with checkpoint.as_directory() as checkpoint_dir:\n",
    "                data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                with open(data_path, \"rb\") as fp:\n",
    "                    checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"] + 1\n",
    "            current_step = checkpoint_state[\"current_step\"]\n",
    "            # Load model and optimizer state.\n",
    "            model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "            if checkpoint_state[\"current_step\"] == current_step:\n",
    "                optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            current_step = 0\n",
    "\n",
    "        while current_step < len(training_steps):\n",
    "            ts = training_steps[current_step]\n",
    "            batch_size = ts[\"batch_size\"]\n",
    "            print_freq = ts[\"print_freq\"]\n",
    "            accumulation_steps = ts[\"accumulation_steps\"]\n",
    "            backbone_layers = ts[\"backbone_layers\"]\n",
    "            improvement_threshold = ts[\"improvement_threshold\"]\n",
    "            variance_threshold = ts[\"variance_threshold\"]\n",
    "\n",
    "            scaled_lr = config[\"lr\"] * math.sqrt((batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps)\n",
    "\n",
    "            # Adjust the trainable layers if needed.\n",
    "            adjust_trainable_layers(model, backbone_layers)\n",
    "\n",
    "            step_optimizer = optimizer\n",
    "            params_new = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.SGD(\n",
    "                params_new,\n",
    "                lr=scaled_lr,\n",
    "                momentum=step_optimizer.param_groups[0]['momentum'],\n",
    "                weight_decay=step_optimizer.param_groups[0]['weight_decay'],\n",
    "                nesterov=step_optimizer.param_groups[0]['nesterov']\n",
    "            )\n",
    "            old_state = step_optimizer.state_dict()[\"state\"]\n",
    "            for group in optimizer.param_groups:\n",
    "                for p in group[\"params\"]:\n",
    "                    pid = id(p)\n",
    "                    if pid in old_state:\n",
    "                        optimizer.state[p] = old_state[pid]\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=batch_size,\n",
    "                sampler=config[\"train_sampler\"],\n",
    "                collate_fn=utils.collate_fn,\n",
    "                num_workers=0, pin_memory=True\n",
    "            )\n",
    "\n",
    "            print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}')\n",
    "            print()\n",
    "\n",
    "            # Plateau detection variables\n",
    "            window_loss = []\n",
    "            window_f1 = []\n",
    "            window_size = 5\n",
    "            minimum_epochs = 15  # Do not check plateau until these many epochs\n",
    "            step_epoch_counter = 0\n",
    "\n",
    "            # Early stopping variables\n",
    "            alpha = 0.1\n",
    "            patience = 3\n",
    "\n",
    "            ema_loss = None\n",
    "            ema_f1 = None\n",
    "            non_improving_counter = 0\n",
    "\n",
    "            while True:\n",
    "                print(f\"Epoch {start_epoch}, Step: {ts['step']}, Memory: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "                print()\n",
    "\n",
    "                train_metric_logger, val_metric_logger = train_one_epoch(\n",
    "                    model, optimizer, data_loader, device, start_epoch,\n",
    "                    print_freq, accumulation_steps, data_loader_val, step_epoch_counter\n",
    "                )\n",
    "                print()\n",
    "\n",
    "                train_coco_evaluator, val_coco_evaluator = evaluate(\n",
    "                    model, data_loader_val, val_coco_ds, device, data_loader, train_coco_ds\n",
    "                )\n",
    "                print()\n",
    "\n",
    "                train_class_metrics = extract_per_class_metrics(train_coco_evaluator, train_coco_ds)\n",
    "                val_class_metrics = extract_per_class_metrics(val_coco_evaluator, val_coco_ds)\n",
    "                train_class_metrics = {label_dict[k]: v for k, v in train_class_metrics.items()}\n",
    "                val_class_metrics = {label_dict[k]: v for k, v in val_class_metrics.items()}\n",
    "\n",
    "                print(\"Training Class Metrics:\")\n",
    "                for name, m in train_class_metrics.items():\n",
    "                    print(f\"Class: {name}, Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}\")\n",
    "                print(\"\\nValidation Class Metrics:\")\n",
    "                for name, m in val_class_metrics.items():\n",
    "                    print(f\"Class: {name}, Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}\")\n",
    "                print()\n",
    "\n",
    "                current_loss = val_metric_logger.loss.avg\n",
    "                window_loss.append(current_loss)\n",
    "                if len(window_loss) > window_size:\n",
    "                    window_loss.pop(0)\n",
    "                current_f1 = calculate_f1_score(val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                    val_coco_evaluator.coco_eval['bbox'].stats[8])\n",
    "                window_f1.append(current_f1)\n",
    "                if len(window_f1) > window_size:\n",
    "                    window_f1.pop(0)\n",
    "                \n",
    "                checkpoint_data = {\n",
    "                    \"epoch\": start_epoch,\n",
    "                    \"current_step\": current_step,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                }\n",
    "\n",
    "                with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                    data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                    with open(data_path, \"wb\") as fp:\n",
    "                        pickle.dump(checkpoint_data, fp)\n",
    "                    train.report(\n",
    "                        {\"epoch\": start_epoch,\n",
    "                        \"current_step\": current_step,\n",
    "                        \"train_loss\": train_metric_logger.loss.avg,\n",
    "                        \"val_loss\": current_loss,\n",
    "                        \"train_mAP\": train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                        \"val_mAP\": val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                        \"train_mAR\": train_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                        \"val_mAR\": val_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                        \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                        train_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                        \"val_f1\": current_f1},\n",
    "                        checkpoint=train.Checkpoint.from_directory(checkpoint_dir),\n",
    "                    )\n",
    "\n",
    "                print(f\"Epoch {start_epoch}: Current Loss = {current_loss:.4f},\", end=\" \")\n",
    "\n",
    "                if step_epoch_counter >= minimum_epochs and len(window_loss) == window_size:\n",
    "                    if ema_loss is None:\n",
    "                        ema_loss = current_loss\n",
    "                        ema_f1 = current_f1\n",
    "                        relative_improvement = 1.0\n",
    "                        relative_f1_improvement = 1.0\n",
    "                    else:\n",
    "                        # Update EMAs\n",
    "                        prev_ema = ema_loss\n",
    "                        prev_f1_ema = ema_f1\n",
    "                        ema_loss = alpha * current_loss + (1 - alpha) * prev_ema\n",
    "                        ema_f1 = alpha * current_f1 + (1 - alpha) * prev_f1_ema\n",
    "                        \n",
    "                        # Calculate improvements\n",
    "                        relative_improvement = (prev_ema - ema_loss) / prev_ema\n",
    "                        relative_f1_improvement = (ema_f1 - prev_f1_ema) / prev_f1_ema\n",
    "                        \n",
    "                        # Check both metrics for improvement\n",
    "                        if (relative_improvement < improvement_threshold and \n",
    "                            relative_f1_improvement < improvement_threshold):\n",
    "                            non_improving_counter += 1\n",
    "                        else:\n",
    "                            non_improving_counter = 0\n",
    "\n",
    "                    loss_variance = np.var(window_loss)\n",
    "                    f1_variance = np.var(window_f1)\n",
    "                    \n",
    "                    # Check both metrics for plateau\n",
    "                    should_break = ((non_improving_counter >= patience) or \n",
    "                                (loss_variance < variance_threshold and f1_variance < variance_threshold))\n",
    "                    \n",
    "                    print(f\"EMA Loss = {ema_loss:.4f}, Loss Improvement = {relative_improvement:.4f},\", end=\" \")\n",
    "                    print(f\"EMA F1 = {ema_f1:.4f}, F1 Improvement = {relative_f1_improvement:.4f},\", end=\" \")\n",
    "                    print(f\"Loss Var = {loss_variance:.6f}, F1 Var = {f1_variance:.6f}, \", end=\"\")\n",
    "                    print(f\"Non-improvement Count = {non_improving_counter}\")\n",
    "                else:\n",
    "                    should_break = False\n",
    "                    print(\"\")\n",
    "\n",
    "                start_epoch += 1\n",
    "                step_epoch_counter += 1\n",
    "\n",
    "                if should_break:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    print(\"Plateau reached; moving to next training step.\\n\")\n",
    "                    break\n",
    "\n",
    "            current_step += 1\n",
    "\n",
    "        print('Tuning Trial Complete!')\n",
    "\n",
    "    def trial_dirname_creator(self, trial):\n",
    "        return f\"{trial.trial_id}\"\n",
    "\n",
    "    def run(self):\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "\n",
    "        dataset = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=True)\n",
    "        )\n",
    "\n",
    "        dataset_val = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_test = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_train = torch.utils.data.Subset(dataset, train_indices)\n",
    "        dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "        dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "            dataset_val, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        data_loader_test = torch.utils.data.DataLoader(\n",
    "            dataset_test, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        train_coco_ds, val_coco_ds, test_coco_ds = self.create_coco_datasets(dataset_train, dataset_val, dataset_test)\n",
    "\n",
    "        dataset_train_ref = ray.put(dataset_train)\n",
    "        data_loader_val_ref = ray.put(data_loader_val)\n",
    "        data_loader_test_ref = ray.put(data_loader_test)\n",
    "        train_coco_ds_ref = ray.put(train_coco_ds)\n",
    "        val_coco_ds_ref = ray.put(val_coco_ds)\n",
    "        test_coco_ds_ref = ray.put(test_coco_ds)\n",
    "\n",
    "        config = {\n",
    "            # \"lr\": tune.sample_from(lambda config: self.train_lr_finder(config)),\n",
    "            \"lr\": tune.loguniform(0.00005, 0.005),\n",
    "            \"momentum\": tune.uniform(0.8, 0.99),\n",
    "            \"weight_decay\": tune.loguniform(0.0001, 0.005),\n",
    "            \"alpha\": tune.uniform(0.5, 0.9),\n",
    "            \"gamma_loss\": tune.uniform(2.5, 4.5),\n",
    "            \"dropout\": tune.uniform(0.1, 0.5),\n",
    "            \"score_thresh\": tune.uniform(0.5, 0.8),\n",
    "            \"nms_thresh\": tune.uniform(0.1, 0.3),\n",
    "            \"fg_iou_thresh\": tune.uniform(0.4, 0.7),\n",
    "            \"bg_iou_thresh\": tune.uniform(0.1, 0.4),\n",
    "            \"beta_loss\": tune.uniform(0.25, 0.75),\n",
    "            \"lambda_loss\": tune.uniform(1.5, 2.0),\n",
    "            \"dataset_train_ref\": dataset_train_ref,\n",
    "            \"data_loader_val_ref\": data_loader_val_ref,\n",
    "            \"data_loader_test_ref\": data_loader_test_ref,\n",
    "            \"train_coco_ds_ref\": train_coco_ds_ref,\n",
    "            \"val_coco_ds_ref\": val_coco_ds_ref,\n",
    "            \"test_coco_ds_ref\": test_coco_ds_ref,\n",
    "            \"train_sampler\": train_sampler,\n",
    "            \"class_weights\": train_class_weights\n",
    "        }\n",
    "\n",
    "        if tune.Tuner.can_restore(os.path.abspath(self.restore_path)):\n",
    "            tuner = tune.Tuner.restore(\n",
    "                os.path.abspath(self.restore_path),\n",
    "                trainable=self.train_MAVdroneDataset,\n",
    "                param_space=config,\n",
    "                resume_unfinished=True,\n",
    "                resume_errored=False\n",
    "            )\n",
    "            print(f\"Tuner Restored from {self.restore_path}\")\n",
    "        else:\n",
    "            algo = TuneBOHB(\n",
    "                seed=710\n",
    "            )\n",
    "\n",
    "            algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "\n",
    "            scheduler = HyperBandForBOHB(\n",
    "                time_attr=\"training_iteration\",\n",
    "                reduction_factor=4,\n",
    "                stop_last_trials=False,\n",
    "            )\n",
    "\n",
    "            reporter = tune.JupyterNotebookReporter(overwrite=True,\n",
    "                metric_columns=[\"epoch\", \"current_step\", \"train_loss\", \"val_loss\", \"train_mAP\", \"val_mAP\", \"train_mAR\", \"val_mAR\", \"train_f1\", \"val_f1\"],\n",
    "                parameter_columns=[\"lr\", \"momentum\", \"weight_decay\", \"alpha\", \"gamma_loss\", \"dropout\", \"score_thresh\", \"nms_thresh\", \"fg_iou_thresh\", \"bg_iou_thresh\", \"beta_loss\", \"lambda_loss\"],\n",
    "                print_intermediate_tables=True,\n",
    "                sort_by_metric=True\n",
    "            )\n",
    "\n",
    "            tuner = tune.Tuner(\n",
    "                tune.with_resources(\n",
    "                    self.train_MAVdroneDataset,\n",
    "                    resources={\"cpu\": 36.0, \"gpu\": 1.0}\n",
    "                ),\n",
    "                run_config=train.RunConfig(\n",
    "                    name=f\"BOHB_RetinaNet_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                    failure_config=train.FailureConfig(max_failures=1),\n",
    "                    progress_reporter=reporter,\n",
    "                ),\n",
    "                tune_config=tune.TuneConfig(\n",
    "                    mode=\"min\",\n",
    "                    metric=\"val_loss\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    num_samples=int(self.num_samples),\n",
    "                    trial_dirname_creator=self.trial_dirname_creator\n",
    "                ),\n",
    "                param_space=config\n",
    "            )\n",
    "        results = tuner.fit()\n",
    "\n",
    "        best_trial = results.get_best_result(\"val_f1\", \"max\")\n",
    "\n",
    "        print(\"Best trial config: {}\".format(best_trial.config))\n",
    "        print()\n",
    "        print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "        print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "        print(\"Best trial final training mAP: {}\".format(best_trial.metrics[\"train_mAP\"]))\n",
    "        print(\"Best trial final validation mAP: {}\".format(best_trial.metrics[\"val_mAP\"]))\n",
    "        print(\"Best trial final training mAR: {}\".format(best_trial.metrics[\"train_mAR\"]))\n",
    "        print(\"Best trial final validation mAR: {}\".format(best_trial.metrics[\"val_mAR\"]))\n",
    "        print(\"Best trial final training f1-score: {}\".format(best_trial.metrics[\"train_f1\"]))\n",
    "        print(\"Best trial final validation f1-score: {}\".format(best_trial.metrics[\"val_f1\"]))\n",
    "        \n",
    "        print()\n",
    "\n",
    "        best_checkpoint = best_trial.get_best_checkpoint(metric=\"val_f1\", mode=\"max\")\n",
    "\n",
    "        self.test_best_model(best_trial, best_checkpoint)\n",
    "\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial\n",
    "\n",
    "    def test_best_model(self, best_trial, best_checkpoint):\n",
    "        best_model = get_retinanet_model(depth=50,\n",
    "                                         num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                                         score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                                         nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                                         detections_per_img=200,\n",
    "                                         fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "                                         bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "                                         topk_candidates=200,\n",
    "                                         alpha=best_trial.config[\"alpha\"],\n",
    "                                         gamma_loss=best_trial.config[\"gamma_loss\"],\n",
    "                                         class_weights=None,\n",
    "                                         beta_loss=best_trial.config[\"beta_loss\"],\n",
    "                                         lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "                                         dropout_prob=best_trial.config[\"dropout\"])\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        best_model.to(device)\n",
    "\n",
    "        with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                best_checkpoint_data = pickle.load(fp)\n",
    "            best_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "\n",
    "        data_loader_test = ray.get(best_trial.config[\"data_loader_test_ref\"])\n",
    "        test_coco_ds = ray.get(best_trial.config[\"test_coco_ds_ref\"])\n",
    "\n",
    "        test_results = evaluate(best_model, data_loader_test, test_coco_ds, device, train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "        print(f'Best trial test set mAP: {test_results.coco_eval[\"bbox\"].stats[0]}')\n",
    "        print(f'Best trial test set mAR: {test_results.coco_eval[\"bbox\"].stats[8]}')\n",
    "        print(f'Best trial test set f1-score: {calculate_f1_score(test_results.coco_eval[\"bbox\"].stats[0], test_results.coco_eval[\"bbox\"].stats[8])}')\n",
    "\n",
    "        # Get per-class metrics\n",
    "        test_class_metrics = extract_per_class_metrics(test_results, test_coco_ds)\n",
    "\n",
    "        test_class_metrics = {label_dict[k]: v for k, v in test_class_metrics.items()}\n",
    "\n",
    "        print(\"Test Set Class Metrics:\")\n",
    "        for class_name, metrics in test_class_metrics.items():\n",
    "            print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "        print()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     trainer = RetinaNetTuner(num_samples=50, restore_path=\"C:/Users/exx/ray_results/FALSE\")\n",
    "#     train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestTrial:\n",
    "    def __init__(self):\n",
    "        # Initialize default config first\n",
    "        self.config = {\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.90,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"alpha\": 0.35,\n",
    "            \"gamma_loss\": 2.5,\n",
    "            \"dropout\": 0.25,\n",
    "            \"score_thresh\": 0.5,\n",
    "            \"nms_thresh\": 0.05,\n",
    "            \"fg_iou_thresh\": 0.5,\n",
    "            \"bg_iou_thresh\": 0.4,\n",
    "            \"beta_loss\": 0.6,\n",
    "            \"lambda_loss\": 0.9,\n",
    "            \"class_weights\": train_class_weights,\n",
    "            \"train_sampler\": train_sampler,\n",
    "        }\n",
    "\n",
    "        # self.config[\"lr\"] = self.train_lr_finder()\n",
    "\n",
    "    def train_lr_finder(self):\n",
    "            class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "                def inputs_labels_from_batch(self, batch_data):\n",
    "                    inputs = [image.to('cuda:0') for image in batch_data[0]]\n",
    "                    labels = [{k: v.to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "                    return inputs, labels\n",
    "\n",
    "            dataset_train = MAVdroneDataset(\n",
    "                csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "                transforms=get_transform(train=True)\n",
    "            )\n",
    "            accumulation_steps = 1  ## FIXME: hardcoded for now\n",
    "\n",
    "            data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8,\n",
    "                                                            sampler=self.config[\"train_sampler\"],\n",
    "                                                            collate_fn=utils.collate_fn,\n",
    "                                                            num_workers=0, pin_memory=True)\n",
    "\n",
    "            model = get_retinanet_model(\n",
    "                depth=50,\n",
    "                num_classes=len(self.config[\"class_weights\"]),\n",
    "                score_thresh=self.config[\"score_thresh\"],\n",
    "                nms_thresh=self.config[\"nms_thresh\"],\n",
    "                detections_per_img=200,\n",
    "                fg_iou_thresh=self.config[\"fg_iou_thresh\"],\n",
    "                bg_iou_thresh=self.config[\"bg_iou_thresh\"],\n",
    "                topk_candidates=200,\n",
    "                alpha=self.config[\"alpha\"],\n",
    "                gamma_loss=self.config[\"gamma_loss\"],\n",
    "                class_weights=self.config[\"class_weights\"],\n",
    "                beta_loss=self.config[\"beta_loss\"],\n",
    "                lambda_loss=self.config[\"lambda_loss\"],\n",
    "                dropout_prob=self.config[\"dropout\"],\n",
    "            ).to('cuda:0')\n",
    "\n",
    "            params = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.SGD(\n",
    "                params, lr=1e-7, momentum=self.config[\"momentum\"], weight_decay=self.config[\"weight_decay\"]\n",
    "            )\n",
    "\n",
    "            train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "            grad_scaler = torch.GradScaler()\n",
    "\n",
    "            class CustomLRFinder(LRFinder):\n",
    "                def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "                    super().__init__(model, optimizer, criterion, device)\n",
    "                    self.amp_backend = amp_backend\n",
    "                    self.amp_config = amp_config\n",
    "                    self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "                def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "                    self.model.train()\n",
    "                    total_loss = 0\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    for _ in range(accumulation_steps):\n",
    "                        inputs, labels = next(train_iter)\n",
    "                        inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                        with torch.autocast(device_type=\"cuda:0\"):\n",
    "                            outputs = self.model(inputs, labels)\n",
    "                            loss = sum(loss for loss in outputs.values())\n",
    "\n",
    "                        loss /= accumulation_steps\n",
    "                        self.grad_scaler.scale(loss).backward()\n",
    "                        total_loss += loss\n",
    "\n",
    "                    self.grad_scaler.step(self.optimizer)\n",
    "                    self.grad_scaler.update()\n",
    "\n",
    "                    return total_loss.item()\n",
    "\n",
    "            lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "            lr_finder.range_test(train_iter, end_lr=0.01, num_iter=500, step_mode='exp', accumulation_steps=accumulation_steps)\n",
    "            suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "            lr_finder.reset()\n",
    "\n",
    "            # return default if torch lr finder fails\n",
    "            try:\n",
    "                if isinstance(suggested_lr, tuple):\n",
    "                    axes, suggested_lr_value = suggested_lr\n",
    "                    return suggested_lr_value\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Error during learning rate finding: {e}\")\n",
    "                # Return a default learning rate if an error occurs\n",
    "                return 5e-4\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_trial = BestTrial()\n",
    "    print(\"Best trial config:\")\n",
    "    for key, value in best_trial.config.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Train Model Using Tuned Hyperparameters**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "\n",
    "def visualize_predictions(model, data_loader, device, epoch, num_samples=2, label_dict=None, bbox_colors=None, plot=False,\n",
    "                          output_dir='prediction_visualizations'):\n",
    "    # Define ImageNet normalization parameters\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def denormalize(img_tensor):\n",
    "        img = img_tensor.clone().cpu().numpy().transpose(1, 2, 0)\n",
    "        img = img * imagenet_std + imagenet_mean\n",
    "        return np.clip(img, 0, 1)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get dataset length and generate random indices\n",
    "    dataset_size = len(data_loader.dataset)\n",
    "    random_indices = random.sample(range(dataset_size), min(num_samples, dataset_size))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in random_indices:\n",
    "            # Get the batch index and position within batch\n",
    "            batch_idx = idx // data_loader.batch_size\n",
    "            pos_in_batch = idx % data_loader.batch_size\n",
    "            \n",
    "            for i, (images, targets) in enumerate(data_loader):\n",
    "                if i == batch_idx:\n",
    "                    images = [img.to(device) for img in images]\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    b = pos_in_batch\n",
    "                    if b >= len(images):  # Skip if batch is smaller than expected\n",
    "                        continue\n",
    "                        \n",
    "                    img = denormalize(images[b])\n",
    "                    gt_boxes = targets[b]['boxes'].cpu().numpy()\n",
    "                    gt_labels = targets[b]['labels'].cpu().numpy()\n",
    "                    pred_boxes = outputs[b]['boxes'].cpu().numpy()\n",
    "                    pred_labels = outputs[b]['labels'].cpu().numpy()\n",
    "                    pred_scores = outputs[b]['scores'].cpu().numpy()\n",
    "                    \n",
    "                    # Get original image dimensions for high-res output\n",
    "                    height, width = img.shape[0], img.shape[1]\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(width/50, height/100))  # Scale figure appropriately\n",
    "                    \n",
    "                    # Plot ground truth boxes\n",
    "                    ax1.imshow(img)\n",
    "                    for box, label in zip(gt_boxes, gt_labels):\n",
    "                        gt_text = label_dict[label] if label_dict is not None and label in label_dict else str(label)\n",
    "                        gt_color = bbox_colors[label] if bbox_colors is not None and label < len(bbox_colors) else 'black'\n",
    "                        rect = plt.Rectangle((box[0], box[1]),\n",
    "                                          box[2] - box[0],\n",
    "                                          box[3] - box[1],\n",
    "                                          linewidth=1.25, edgecolor=gt_color, facecolor='none')\n",
    "                        ax1.add_patch(rect)\n",
    "                        ax1.text(box[2], box[3],\n",
    "                                f'{gt_text}',\n",
    "                                fontsize=8,\n",
    "                                bbox=dict(facecolor='white', alpha=0.8, pad=0, edgecolor='none'),\n",
    "                                color='black')\n",
    "                    ax1.set_title(f'Ground Truth\\nEpoch {epoch}, Image {idx}')\n",
    "                    \n",
    "                    # Plot predicted boxes\n",
    "                    ax2.imshow(img)\n",
    "                    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "                        pred_text = f\"{label_dict[label] if label_dict is not None and label in label_dict else label}: {score:.2f}\"\n",
    "                        pred_color = bbox_colors[label] if bbox_colors is not None and label < len(bbox_colors) else 'black'\n",
    "                        rect = plt.Rectangle((box[0], box[1]),\n",
    "                                          box[2] - box[0],\n",
    "                                          box[3] - box[1],\n",
    "                                          linewidth=1.25, edgecolor=pred_color, facecolor='none')\n",
    "                        ax2.add_patch(rect)\n",
    "                        ax2.text(box[2], box[3],\n",
    "                                pred_text,\n",
    "                                fontsize=8,\n",
    "                                bbox=dict(facecolor='white', alpha=0.8, pad=0, edgecolor='none'),\n",
    "                                color='black')\n",
    "                    ax2.set_title(f'Predictions\\nEpoch {epoch}, Image {idx}')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    # Save high-resolution figure\n",
    "                    filename = f\"epoch{epoch:03d}_img{idx:05d}.png\"\n",
    "                    filepath = os.path.join(output_dir, filename)\n",
    "                    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "                    if plot:\n",
    "                        plt.show()\n",
    "                    plt.close()\n",
    "                    \n",
    "                    print(f\"Saved visualization to: {filepath}\")\n",
    "                    break\n",
    "\n",
    "def create_coco_datasets(train_dataset, val_dataset, test_dataset):\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            train_future = executor.submit(get_coco_api_from_dataset, train_dataset)\n",
    "            val_future = executor.submit(get_coco_api_from_dataset, val_dataset)\n",
    "            test_future = executor.submit(get_coco_api_from_dataset, test_dataset)\n",
    "            train_coco_ds = train_future.result()\n",
    "            val_coco_ds = val_future.result()\n",
    "            test_coco_ds = test_future.result()\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds\n",
    "\n",
    "# def main(train_coco_ds, val_coco_ds, best_trial):\n",
    "def main(best_trial):\n",
    "    set_seed(710)\n",
    "\n",
    "    print(best_trial.config)\n",
    "    print()\n",
    "\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 1, \"trainable_layers\": 0, \"improvement_threshold\": 0.01, \"variance_threshold\": 1e-4}, \n",
    "        {\"step\": 1, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 2, \"trainable_layers\": 1, \"improvement_threshold\": 0.008, \"variance_threshold\": 5e-5}, \n",
    "        {\"step\": 2, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 4, \"trainable_layers\": 2, \"improvement_threshold\": 0.005, \"variance_threshold\": 2.5e-5}, \n",
    "        {\"step\": 3, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 8, \"trainable_layers\": 3, \"improvement_threshold\": 0.003, \"variance_threshold\": 1e-5}, \n",
    "        {\"step\": 4, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 16, \"trainable_layers\": 4, \"improvement_threshold\": 0.002, \"variance_threshold\": 5e-6}, \n",
    "        {\"step\": 5, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 32, \"trainable_layers\": 5, \"improvement_threshold\": 0.001, \"variance_threshold\": 2.5e-6}, # bs 256\n",
    "    ]\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f'C:/Users/exx/Documents/GitHub/SSD_VGG_PyTorch/runs/RetinaNet/{current_datetime}')\n",
    "    checkpoint_dir = Path(f'./checkpoints/{current_datetime}')\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    checkpoints = []\n",
    "\n",
    "    dataset = MAVdroneDataset(\n",
    "        csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "        root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "        transforms=get_transform(train=True)\n",
    "    )\n",
    "    dataset_val = MAVdroneDataset(\n",
    "        csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "        root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "        transforms=get_transform(train=False)\n",
    "    )\n",
    "    \n",
    "    dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    train_dataset_eval = torch.utils.data.Subset(dataset_val, train_indices)\n",
    "    train_data_loader_eval = torch.utils.data.DataLoader(train_dataset_eval, batch_size=1, shuffle=False,\n",
    "                                                        collate_fn=utils.collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "    dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "    data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False,\n",
    "                                                  collate_fn=utils.collate_fn, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    train_coco_ds, val_coco_ds, test_coco_ds = create_coco_datasets(train_dataset=train_dataset_eval, \n",
    "                                                                    val_dataset=dataset_val, \n",
    "                                                                    test_dataset=dataset_val)\n",
    "\n",
    "    model = get_retinanet_model(\n",
    "            depth=50,\n",
    "            num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "            score_thresh=best_trial.config[\"score_thresh\"],\n",
    "            nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=200, \n",
    "            alpha=best_trial.config[\"alpha\"], \n",
    "            gamma_loss=best_trial.config[\"gamma_loss\"],\n",
    "            dropout_prob=best_trial.config[\"dropout\"],\n",
    "            beta_loss=best_trial.config[\"beta_loss\"],\n",
    "            lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "            class_weights=best_trial.config[\"class_weights\"]\n",
    "        )\n",
    "\n",
    "    model.to(device)\n",
    "        \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=best_trial.config[\"lr\"],\n",
    "                                    momentum=best_trial.config[\"momentum\"],\n",
    "                                    weight_decay=best_trial.config[\"weight_decay\"],\n",
    "                                    nesterov=True)\n",
    "\n",
    "    start_epoch, step_index = 0, 0\n",
    "\n",
    "    while step_index < len(training_steps):\n",
    "        ts = training_steps[step_index]\n",
    "        batch_size = ts[\"batch_size\"]\n",
    "        print_freq = ts[\"print_freq\"]\n",
    "        accumulation_steps = ts[\"accumulation_steps\"]\n",
    "        backbone_layers = ts[\"trainable_layers\"]\n",
    "        improvement_threshold = ts[\"improvement_threshold\"]\n",
    "        variance_threshold = ts[\"variance_threshold\"]\n",
    "        scaled_lr = best_trial.config[\"lr\"] * math.sqrt((batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps)\n",
    "\n",
    "        # unfreeze backbone and batch norm layers\n",
    "        adjust_trainable_layers(model, backbone_layers)\n",
    "\n",
    "        step_optimizer = optimizer\n",
    "        params_new = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params_new,\n",
    "            lr=scaled_lr,\n",
    "            momentum=step_optimizer.param_groups[0]['momentum'],\n",
    "            weight_decay=step_optimizer.param_groups[0]['weight_decay'],\n",
    "            nesterov=step_optimizer.param_groups[0]['nesterov']\n",
    "        )\n",
    "        old_state = step_optimizer.state_dict()[\"state\"]\n",
    "        for group in optimizer.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                pid = id(p)\n",
    "                if pid in old_state:\n",
    "                    optimizer.state[p] = old_state[pid]\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, sampler=best_trial.config[\"train_sampler\"],\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}\\n')\n",
    "        \n",
    "        # Plateau detection variables\n",
    "        window_loss = []\n",
    "        window_f1 = []\n",
    "        window_size = 5\n",
    "        minimum_epochs = 15  # Do not check plateau until these many epochs\n",
    "        step_epoch_counter = 0\n",
    "\n",
    "        # Early stopping variables\n",
    "        alpha = 0.1\n",
    "        patience = 5 if ts < 3 else 8\n",
    "\n",
    "        ema_loss = None\n",
    "        ema_f1 = None\n",
    "        non_improving_counter = 0\n",
    "\n",
    "        while True:\n",
    "            print(f'Epoch {start_epoch}, Step: {ts[\"step\"]}, Memory: {torch.cuda.memory_allocated(device)} bytes')\n",
    "            print()\n",
    "            \n",
    "            train_metric_logger, val_metric_logger = train_one_epoch(\n",
    "                model, optimizer, data_loader, device, start_epoch,\n",
    "                print_freq, accumulation_steps, data_loader_val, step_epoch_counter\n",
    "            )\n",
    "            print()\n",
    "            train_coco_evaluator, val_coco_evaluator = evaluate(\n",
    "                model, data_loader_val, val_coco_ds, device, \n",
    "                train_data_loader_eval, train_coco_ds\n",
    "            )\n",
    "            print()\n",
    "\n",
    "            train_class_metrics = extract_per_class_metrics(train_coco_evaluator, train_coco_ds)\n",
    "            val_class_metrics = extract_per_class_metrics(val_coco_evaluator, val_coco_ds)\n",
    "            train_class_metrics = {label_dict[k]: v for k, v in train_class_metrics.items()}\n",
    "            val_class_metrics = {label_dict[k]: v for k, v in val_class_metrics.items()}\n",
    "\n",
    "            print(\"Training Class Metrics:\")\n",
    "            for name, m in train_class_metrics.items():\n",
    "                print(f\"Class: {name}, Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}\")\n",
    "            print(\"\\nValidation Class Metrics:\")\n",
    "            for name, m in val_class_metrics.items():\n",
    "                print(f\"Class: {name}, Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}\")\n",
    "            print()\n",
    "\n",
    "            \n",
    "            if start_epoch % 5 == 0:  # Visualize every 3 epochs\n",
    "                visualize_predictions(model, data_loader_val, device, start_epoch, num_samples=3, \n",
    "                                      label_dict=label_dict, bbox_colors=bbox_colors, plot=False,\n",
    "                                      output_dir=f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/prediction_visualizations/{current_datetime}')\n",
    "\n",
    "            current_loss = val_metric_logger.loss.avg\n",
    "            current_f1 = calculate_f1_score(val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                    val_coco_evaluator.coco_eval['bbox'].stats[8])\n",
    "            window_loss.append(current_loss)\n",
    "            window_f1.append(current_f1)\n",
    "            if len(window_loss) > window_size:\n",
    "                window_loss.pop(0)\n",
    "            if len(window_f1) > window_size:\n",
    "                window_f1.pop(0)\n",
    "\n",
    "            # add epoch metrics to list\n",
    "            checkpoint = {\n",
    "                \"epoch\": start_epoch,\n",
    "                \"current_step\": ts[\"step\"],\n",
    "                \"train_loss\": train_metric_logger.loss.avg,\n",
    "                \"val_loss\": current_loss,\n",
    "                \"train_bbox_loss\": train_metric_logger.bbox_regression.avg,\n",
    "                \"val_bbox_loss\": val_metric_logger.bbox_regression.avg,\n",
    "                \"train_class_loss\": train_metric_logger.classification.avg,\n",
    "                \"val_class_loss\": val_metric_logger.classification.avg,\n",
    "                \"train_mAP\": train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                \"train_mAR\": train_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                \"val_mAP\": val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                \"val_mAR\": val_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                train_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                \"val_f1\": current_f1\n",
    "            }\n",
    "            checkpoints.append(checkpoint)\n",
    "            \n",
    "            # save model and optimizer state to model_path\n",
    "            model_path = checkpoint_dir / f\"{current_datetime}_{start_epoch}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': start_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, model_path)\n",
    "\n",
    "            train_val_ratio = {\n",
    "                'loss': checkpoint[\"train_loss\"] / (checkpoint[\"val_loss\"] if checkpoint[\"val_loss\"] != 0 else 1.0),\n",
    "                'mAP': checkpoint[\"train_mAP\"] / (checkpoint[\"val_mAP\"] if checkpoint[\"val_mAP\"] != 0 else 1.0),\n",
    "                'mAR': checkpoint[\"train_mAR\"] / (checkpoint[\"val_mAR\"] if checkpoint[\"val_mAR\"] != 0 else 1.0),\n",
    "                \"f1\": checkpoint[\"train_f1\"] / (checkpoint[\"val_f1\"] if checkpoint[\"val_f1\"] != 0 else 1.0)\n",
    "            }\n",
    "\n",
    "            writer.add_scalar('Loss/Train', float(checkpoint[\"train_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Loss/Val', float(checkpoint[\"val_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Box Loss/Train', float(checkpoint[\"train_bbox_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Box Loss/Val', float(checkpoint[\"val_bbox_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Class Loss/Train', float(checkpoint[\"train_class_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Class Loss/Val', float(checkpoint[\"val_class_loss\"]), start_epoch)\n",
    "            writer.add_scalar('mAP/Train', float(checkpoint[\"train_mAP\"]), start_epoch)\n",
    "            writer.add_scalar('mAP/Val', float(checkpoint[\"val_mAP\"]), start_epoch)\n",
    "            writer.add_scalar('mAR/Train', float(checkpoint[\"train_mAR\"]), start_epoch)\n",
    "            writer.add_scalar('mAR/Val', float(checkpoint[\"val_mAR\"]), start_epoch)\n",
    "            writer.add_scalar('F1/Train', float(checkpoint[\"train_f1\"]), start_epoch)\n",
    "            writer.add_scalar('F1/Val', float(checkpoint[\"val_f1\"]), start_epoch)\n",
    "            writer.add_scalar('Ratios/loss', train_val_ratio['loss'], start_epoch)\n",
    "            writer.add_scalar('Ratios/mAP', train_val_ratio['mAP'], start_epoch)\n",
    "            writer.add_scalar('Ratios/mAR', train_val_ratio['mAR'], start_epoch)\n",
    "            writer.add_scalar('Ratios/f1', train_val_ratio['f1'], start_epoch)\n",
    "\n",
    "            print(f\"Epoch {start_epoch}: Current Loss = {current_loss:.4f},\", end=\" \")\n",
    "\n",
    "            # Only check plateau after minimum epochs and full loss window are stored\n",
    "            if step_epoch_counter >= minimum_epochs and len(window_loss) == window_size:\n",
    "                if ema_loss is None:\n",
    "                    ema_loss = current_loss\n",
    "                    ema_f1 = current_f1\n",
    "                    relative_improvement = 1.0\n",
    "                    relative_f1_improvement = 1.0\n",
    "                else:\n",
    "                    # Update EMAs\n",
    "                    prev_ema = ema_loss\n",
    "                    prev_f1_ema = ema_f1\n",
    "                    ema_loss = alpha * current_loss + (1 - alpha) * prev_ema\n",
    "                    ema_f1 = alpha * current_f1 + (1 - alpha) * prev_f1_ema\n",
    "                    \n",
    "                    # Calculate improvements\n",
    "                    relative_improvement = (prev_ema - ema_loss) / prev_ema\n",
    "                    relative_f1_improvement = (ema_f1 - prev_f1_ema) / prev_f1_ema\n",
    "                    \n",
    "                    # Check both metrics for improvement\n",
    "                    if (relative_improvement < improvement_threshold and \n",
    "                        relative_f1_improvement < improvement_threshold):\n",
    "                        non_improving_counter += 1\n",
    "                    else:\n",
    "                        non_improving_counter = 0\n",
    "\n",
    "                loss_variance = np.var(window_loss)\n",
    "                f1_variance = np.var(window_f1)\n",
    "                \n",
    "                # Check both metrics for plateau\n",
    "                should_break = ((non_improving_counter >= patience) or \n",
    "                            (loss_variance < variance_threshold and f1_variance < variance_threshold))\n",
    "                \n",
    "                print(f\"EMA Loss = {ema_loss:.4f}, Loss Improvement = {relative_improvement:.4f},\", end=\" \")\n",
    "                print(f\"EMA F1 = {ema_f1:.4f}, F1 Improvement = {relative_f1_improvement:.4f},\", end=\" \")\n",
    "                print(f\"Loss Var = {loss_variance:.6f}, F1 Var = {f1_variance:.6f}, \", end=\"\")\n",
    "                print(f\"Non-improvement Count = {non_improving_counter}\")\n",
    "            else:\n",
    "                should_break = False\n",
    "                print(\"\")  \n",
    "\n",
    "            start_epoch += 1\n",
    "            step_epoch_counter += 1\n",
    "\n",
    "            # Clear intermediate tensors to free up memory\n",
    "            del train_metric_logger, val_metric_logger, train_coco_evaluator, val_coco_evaluator\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            if should_break:\n",
    "                print(\"Plateau reached; moving to next training step.\\n\")\n",
    "                break\n",
    "\n",
    "        step_index += 1\n",
    "\n",
    "    print('All Training Steps Complete!')\n",
    "    writer.close()\n",
    "    return checkpoints, test_coco_ds\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    checkpoints, test_coco_ds = main(best_trial)\n",
    "    # checkpoints = main(train_coco_ds, val_coco_ds, best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best train epoch is dictionary in checkpoints with highest validation f1\n",
    "best_train_epoch = max(checkpoints, key = lambda x: x['val_f1'])\n",
    "\n",
    "# initialize model with best trial config\n",
    "model = get_retinanet_model(depth=50,\n",
    "                            num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                            score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                            nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                            detections_per_img=200,\n",
    "                            fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "                            bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "                            topk_candidates=200, \n",
    "                            alpha=best_trial.config[\"alpha\"], \n",
    "                            gamma_loss=best_trial.config[\"gamma_loss\"], \n",
    "                            dropout_prob=best_trial.config[\"dropout\"],\n",
    "                            beta_loss=best_trial.config[\"beta_loss\"],\n",
    "                            lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "                            class_weights=None)\n",
    "\n",
    "# load model weights from best config's best_train_epoch\n",
    "model.load_state_dict(best_train_epoch[\"model_state_dict\"])\n",
    "\n",
    "# save model weights to .pth file\n",
    "torch.save(model.state_dict(), 'RetinaNet_ResNet50_FPN_DuckNet_' + str(datetime.now().strftime(\"%m%d%Y\")) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy checkpoints and remove model and optimizer state dicts\n",
    "checkpoints_copy = checkpoints.copy()\n",
    "\n",
    "# save checkpoints list to text file\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/tuned_model_checkpoints.txt', 'w') as f:\n",
    "    for item in checkpoints_copy:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Model Inference on Test Dataset**</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/', \n",
    "                                transforms = get_transform(train = False))\n",
    "\n",
    "# subset test dataset using test_indices\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size = 1, shuffle = False,\n",
    "                                               collate_fn = utils.collate_fn, num_workers = 0,\n",
    "                                               pin_memory = True)\n",
    "\n",
    "test_coco_ds = get_coco_api_from_dataset(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance = evaluate(model, data_loader_test, test_coco_ds, device='cpu', train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "print(f'Best trial test set mAP: {test_performance.coco_eval[\"bbox\"].stats[0]}') \n",
    "print(f'Best trial test set mAR: {test_performance.coco_eval[\"bbox\"].stats[8]}')\n",
    "print(f'Best trial test set f1 score: {calculate_f1_score(test_performance.coco_eval[\"bbox\"].stats[0], test_performance.coco_eval[\"bbox\"].stats[8])}')\n",
    "\n",
    "# Get per-class metrics\n",
    "test_class_metrics = extract_per_class_metrics(test_performance, test_coco_ds)\n",
    "\n",
    "test_class_metrics = {label_dict[k]: v for k, v in test_class_metrics.items()}\n",
    "\n",
    "print(\"Test Set Class Metrics:\")\n",
    "for class_name, metrics in test_class_metrics.items():\n",
    "    print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Plot Model Predictions for Images in Test Dataset**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n",
    "visualize_predictions(model, data_loader_test, device='cpu', epoch=best_train_epoch[\"epoch\"], num_samples=15, \n",
    "                      label_dict=label_dict, bbox_colors=bbox_colors, plot=True, output_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/test_set_predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Evaluate the Full Dataset**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def evaluate_full_dataset(model, device='cuda:0', output_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/full_dataset_evaluation'):\n",
    "    \"\"\"Evaluate model on full dataset and save comprehensive results\"\"\"\n",
    "    \n",
    "    model = model.to(device='cuda:0', dtype=torch.float32)\n",
    "\n",
    "    full_dataset = MAVdroneDataset(\n",
    "        csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "        root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "        transforms=get_transform(train=False)\n",
    "    )\n",
    "\n",
    "    full_coco_ds = get_coco_api_from_dataset(full_dataset)\n",
    "\n",
    "    full_data_loader = torch.utils.data.DataLoader(\n",
    "        full_dataset, batch_size=1, shuffle=False,\n",
    "        collate_fn=utils.collate_fn, num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    performance = evaluate(model, full_data_loader, full_coco_ds, device=device)\n",
    "    \n",
    "    # Create output directory  \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get evaluation results and image IDs from the COCO evaluator\n",
    "    eval_imgs = performance.coco_eval['bbox'].evalImgs\n",
    "    eval_imgs = [e for e in eval_imgs if e is not None]\n",
    "    img_ids = sorted(full_coco_ds.getImgIds())\n",
    "    \n",
    "    print(f\"Total valid evaluation entries: {len(eval_imgs)}\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    # Run inference and collect predictions\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(full_data_loader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            # Process each image in the batch\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # Get image ID and file name with fallback using the original dataset (dataset_test)\n",
    "                img_id = target.get('image_id', i)\n",
    "                if isinstance(img_id, torch.Tensor):\n",
    "                    img_id = img_id.item()\n",
    "                try:\n",
    "                    # Use dataset_test.indices if available to find corresponding original index\n",
    "                    if img_id < len(dataset_test.indices):\n",
    "                        original_idx = dataset_test.indices[img_id]\n",
    "                    else:\n",
    "                        original_idx = img_id\n",
    "                    if hasattr(dataset_test.dataset, 'unique_image_names') and original_idx < len(dataset_test.dataset.unique_image_names):\n",
    "                        image_name = dataset_test.dataset.unique_image_names[original_idx]\n",
    "                    else:\n",
    "                        image_name = f\"image_{img_id}.jpg\"\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting image name for ID {img_id}: {str(e)}\")\n",
    "                    image_name = f\"image_{img_id}.jpg\"\n",
    "                \n",
    "                # Process predictions for this image\n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "                \n",
    "                for k in range(len(boxes)):\n",
    "                    if scores[k] < 0.1:  # confidence threshold\n",
    "                        continue\n",
    "                    x1, y1, x2, y2 = boxes[k]\n",
    "                    label_id = int(labels[k])\n",
    "                    score = float(scores[k])\n",
    "                    \n",
    "                    all_predictions.append({\n",
    "                        'image_id': img_id,\n",
    "                        'image_name': image_name,\n",
    "                        'class_id': label_id,\n",
    "                        'class_name': label_dict[label_id] if label_id in label_dict else f\"unknown_{label_id}\",\n",
    "                        'confidence': score,\n",
    "                        'x1': float(x1),\n",
    "                        'y1': float(y1),\n",
    "                        'x2': float(x2),\n",
    "                        'y2': float(y2)\n",
    "                    })\n",
    "                    \n",
    "            # Print progress\n",
    "            if (i + 1) % 100 == 0 or i == 0:\n",
    "                print(f\"Processed {i+1}/{len(full_data_loader)} images\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    pred_df = pd.DataFrame(all_predictions)\n",
    "    print(f\"Found {len(pred_df)} predictions above confidence threshold\")\n",
    "    pred_df.to_csv(f'{output_dir}/all_predictions.csv', index=False)\n",
    "    \n",
    "    # Calculate overall metrics using the COCO evaluator's results\n",
    "    mAP = performance.coco_eval[\"bbox\"].stats[0]\n",
    "    mAR = performance.coco_eval[\"bbox\"].stats[8]\n",
    "    f1 = calculate_f1_score(mAP, mAR)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['mAP', 'mAR', 'F1'],\n",
    "        'Value': [mAP, mAR, f1]\n",
    "    })\n",
    "    metrics_df.to_csv(f'{output_dir}/overall_metrics.csv', index=False)\n",
    "    \n",
    "    # Extract per-class metrics\n",
    "    class_metrics = extract_per_class_metrics(performance, full_coco_ds)\n",
    "    class_metrics_dict = {label_dict[k]: v for k, v in class_metrics.items()}\n",
    "    \n",
    "    class_rows = []\n",
    "    for class_name, metrics in class_metrics_dict.items():\n",
    "        class_rows.append({\n",
    "            'class_name': class_name,\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'f1_score': calculate_f1_score(metrics['precision'], metrics['recall'])\n",
    "        })\n",
    "    \n",
    "    class_metrics_df = pd.DataFrame(class_rows)\n",
    "    class_metrics_df.to_csv(f'{output_dir}/class_metrics.csv', index=False)\n",
    "    \n",
    "    # Prepare confusion matrix mapping (exclude background, e.g., id 0)\n",
    "    class_ids = sorted([cid for cid in label_dict.keys() if cid != 0])\n",
    "    n_classes = len(class_ids)\n",
    "    class_to_idx = {cid: idx for idx, cid in enumerate(class_ids)}\n",
    "    \n",
    "    # Create extended confusion matrix: extra row (false negatives) and column (false positives)\n",
    "    extended_cm = np.zeros((n_classes+1, n_classes+1), dtype=int)\n",
    "    iou_threshold = 0.5\n",
    "    \n",
    "    # Loop over all images in the dataset\n",
    "    for img_id in img_ids:\n",
    "        gt_ann_ids = full_coco_ds.getAnnIds(imgIds=img_id)\n",
    "        gt_anns = full_coco_ds.loadAnns(gt_ann_ids)\n",
    "        \n",
    "        gt_boxes = []\n",
    "        gt_classes = []\n",
    "        for ann in gt_anns:\n",
    "            if ann['category_id'] in class_to_idx:\n",
    "                gt_boxes.append(ann['bbox'])  # [x, y, w, h]\n",
    "                gt_classes.append(ann['category_id'])\n",
    "        \n",
    "        df_img = pred_df[pred_df['image_id'] == img_id]\n",
    "        pred_boxes = df_img[['x1', 'y1', 'x2', 'y2']].values  # XYXY format\n",
    "        pred_classes = df_img['class_id'].values\n",
    "        \n",
    "        used = [False] * len(gt_boxes)\n",
    "        for p_box, p_class in zip(pred_boxes, pred_classes):\n",
    "            best_iou = 0\n",
    "            best_idx = -1\n",
    "            for idx, (gt_box, gt_class) in enumerate(zip(gt_boxes, gt_classes)):\n",
    "                if used[idx]:\n",
    "                    continue\n",
    "                # Convert GT box from [x,y,w,h] to [x1,y1,x2,y2]\n",
    "                gt_box_xyxy = [gt_box[0], gt_box[1], gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]]\n",
    "                x_left = max(p_box[0], gt_box_xyxy[0])\n",
    "                y_top = max(p_box[1], gt_box_xyxy[1])\n",
    "                x_right = min(p_box[2], gt_box_xyxy[2])\n",
    "                y_bottom = min(p_box[3], gt_box_xyxy[3])\n",
    "                if x_right < x_left or y_bottom < y_top:\n",
    "                    iou = 0\n",
    "                else:\n",
    "                    inter_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "                    p_area = (p_box[2] - p_box[0]) * (p_box[3] - p_box[1])\n",
    "                    gt_area = (gt_box_xyxy[2] - gt_box_xyxy[0]) * (gt_box_xyxy[3] - gt_box_xyxy[1])\n",
    "                    union = p_area + gt_area - inter_area\n",
    "                    iou = inter_area / union if union > 0 else 0\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_idx = idx\n",
    "            if best_iou >= iou_threshold and best_idx >= 0:\n",
    "                used[best_idx] = True\n",
    "                gt_class = gt_classes[best_idx]\n",
    "                if gt_class in class_to_idx and p_class in class_to_idx:\n",
    "                    extended_cm[class_to_idx[gt_class], class_to_idx[p_class]] += 1\n",
    "            else:\n",
    "                # Count false positive in extra column\n",
    "                if p_class in class_to_idx:\n",
    "                    extended_cm[n_classes, class_to_idx[p_class]] += 1\n",
    "        \n",
    "        # Count false negatives for unmatched ground truth boxes (extra row)\n",
    "        for idx, flag in enumerate(used):\n",
    "            if not flag:\n",
    "                gt_class = gt_classes[idx]\n",
    "                if gt_class in class_to_idx:\n",
    "                    extended_cm[class_to_idx[gt_class], n_classes] += 1\n",
    "    \n",
    "    row_labels = [label_dict[cid] for cid in class_ids] + [\"False Negative\"]\n",
    "    col_labels = [label_dict[cid] for cid in class_ids] + [\"False Positive\"]\n",
    "    \n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.heatmap(extended_cm, annot=True, fmt='d', cmap='Blues', xticklabels=col_labels, yticklabels=row_labels)\n",
    "    plt.title(\"Multi-Class Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/extended_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Full dataset evaluation complete. Results saved to {output_dir}\")\n",
    "    return metrics_df, class_metrics_df, pred_df\n",
    "\n",
    "evaluate_full_dataset(model, 'cuda:0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
