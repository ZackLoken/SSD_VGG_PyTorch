{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Reading and Cleaning Annotation Data for Custom PyTorch Object Detection**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.cache_size = 0  # disable cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # restrict cuda to gpu 0\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # set CUDA kernel to synchronous\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion(); # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load annotation data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading JSON as dictionary\n",
    "def read_json(filename: str) -> dict:\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Reading {filename} file encountered an error: {e}\")\n",
    "    return data\n",
    "\n",
    "# Function to create a DataFrame from a list of records\n",
    "def create_dataframe(data: list) -> pd.DataFrame:\n",
    "    # Normalize the column levels and create a DataFrame\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Main function to iterate over files in directory and add to df\n",
    "def main():\n",
    "    # Assign directory and empty list for collecting records\n",
    "    directory = \"C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Annotations/\"  # annotation directory\n",
    "    records = []\n",
    "    \n",
    "    # Iterate over files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            # Read the JSON file as python dictionary \n",
    "            data = read_json(filename=f)\n",
    "        \n",
    "            # Create the dataframe for the array items in annotations key \n",
    "            df = create_dataframe(data=data['annotations'])\n",
    "            df.insert(loc=0, column='img_name', value=f'{f[-30:-5]}.JPG')\n",
    "        \n",
    "            df.rename(columns={\n",
    "                \"img_name\": \"img_name\",\n",
    "                \"name\": \"label\",\n",
    "                \"bounding_box.h\": \"bbox_height\",\n",
    "                \"bounding_box.w\": \"bbox_width\",\n",
    "                \"bounding_box.x\": \"bbox_x_topLeft\",\n",
    "                \"bounding_box.y\": \"bbox_y_topLeft\",\n",
    "                \"polygon.paths\": \"polygon_path\"\n",
    "            }, inplace=True)\n",
    "            \n",
    "            # Append the records to the list\n",
    "            records.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping non-file: {filename}\")\n",
    "\n",
    "    # Concatenate all records into a single DataFrame\n",
    "    annos_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "    # Convert x, y, h, w to xmin, ymin, xmax, ymax\n",
    "    annos_df['xmin'] = annos_df['bbox_x_topLeft']\n",
    "    annos_df['ymin'] = annos_df['bbox_y_topLeft']\n",
    "    annos_df['xmax'] = annos_df['bbox_x_topLeft'] + annos_df['bbox_width']\n",
    "    annos_df['ymax'] = annos_df['bbox_y_topLeft'] + annos_df['bbox_height']\n",
    "  \n",
    "    # Drop unnecessary columns \n",
    "    annos_df = annos_df.drop(columns=['bbox_height', 'bbox_width', 'bbox_x_topLeft', \n",
    "                                      'bbox_y_topLeft', 'id', 'slot_names', 'polygon_path'])\n",
    "        \n",
    "    return annos_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-process annotation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique image names\n",
    "unique_img_names = df['img_name'].unique()\n",
    "\n",
    "invalid_img_names = []\n",
    "for img_name in unique_img_names:\n",
    "    img_path = f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Images/{img_name}'\n",
    "    img = Image.open(img_path)\n",
    "    if img.size == (5184, 3888):\n",
    "        invalid_img_names.append(img_name)\n",
    "\n",
    "# remove invalid images from df\n",
    "df = df[~df['img_name'].isin(invalid_img_names)]\n",
    "\n",
    "img_classes_to_remove = ['WTDE', 'TURT', 'NUTR', 'ANHI', 'CAGO', \n",
    "                         'DCCO', 'GWFG', 'GBHE', 'COGA', 'PBGR'] # remove images with these classes\n",
    "\n",
    "for class_label in img_classes_to_remove:\n",
    "    # Get all image names with the class\n",
    "    images_with_class = df[df['label'] == class_label]['img_name'].unique()\n",
    "\n",
    "    # Remove all rows for img\n",
    "    df = df[~df['img_name'].isin(images_with_class)]\n",
    "\n",
    "# remove images containing only hens\n",
    "hen_images_no_other_class = df[(df['label'] == 'Hen') & (~df['img_name'].isin(df[df['label'] != 'Hen']['img_name']))]['img_name'].unique()\n",
    "df = df[~df['img_name'].isin(hen_images_no_other_class)]\n",
    "\n",
    "# Separate classes with less than 50 instances\n",
    "class_counts = df['label'].value_counts()\n",
    "other_classes = class_counts[class_counts < 50].index.tolist()\n",
    "positive_classes = class_counts[class_counts >= 50].index.tolist()\n",
    "\n",
    "# print class counts for each label\n",
    "print(\"Number of instances per class in cleaned dataset:\")\n",
    "for label in df['label'].unique():\n",
    "    print(f'{label}: {len(df[df[\"label\"] == label])}')\n",
    "\n",
    "# print other and positive classes\n",
    "print()\n",
    "print(f'Other classes: {other_classes}')\n",
    "print(f'Positive classes: {positive_classes}')\n",
    "\n",
    "# remove images with other classes\n",
    "for class_label in other_classes:\n",
    "    # Get all image names with the class\n",
    "    images_with_class = df[df['label'] == class_label]['img_name'].unique()\n",
    "\n",
    "    # Remove all rows for img\n",
    "    df = df[~df['img_name'].isin(images_with_class)]\n",
    "\n",
    "# confirm the only classes in df are positive classes\n",
    "assert len(df['label'].unique()) == len(positive_classes)\n",
    "\n",
    "# encode labels as int (reserve 0 for 'background')\n",
    "df['target'] = pd.Categorical(df['label']).codes + 1\n",
    "\n",
    "# filter out images with invalid bounding boxes\n",
    "df = df.groupby('img_name').filter(lambda x: ((x['xmin'] < x['xmax']) & (x['ymin'] < x['ymax'])).all())\n",
    "\n",
    "# Create a dictionary using df['label'] as the keys and df['target'] as the values\n",
    "label_dict = dict(zip(df['target'], df['label']))\n",
    "\n",
    "# Drop the original 'label' column from df\n",
    "df = df.drop(['label'], axis=1)\n",
    "\n",
    "# Rename 'target' column to 'label'\n",
    "df.rename(columns={'target': 'label'}, inplace=True)\n",
    "\n",
    "# Save df as csv in directory\n",
    "df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter images after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store unique img_names in filtered df as array\n",
    "img_names = df['img_name'].unique().tolist()\n",
    "\n",
    "# Create a new directory called 'filtered_images'\n",
    "new_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images'\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "else:\n",
    "    for file in os.listdir(new_dir):\n",
    "        os.remove(os.path.join(new_dir, file))\n",
    "\n",
    "# Copy images in img_names to new directory\n",
    "for img in img_names:\n",
    "    shutil.copy2(f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Images/{img}', new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Transform and Augment Image and Annotation Data for Custom PyTorch Object Detection**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "import torchvision.transforms.v2 as T\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAVdroneDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset Loader for Waterfowl Drone Imagery\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transforms):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            root_dir (string): Directory containing all images.\n",
    "            transforms (callable): Transformation to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.unique_image_names = self.df['img_name'].unique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_name = self.unique_image_names[idx]\n",
    "\n",
    "        # Isolate first row to prevent multiple instances of the same image\n",
    "        row = self.df[self.df['img_name'] == image_name].iloc[0]\n",
    "\n",
    "        image_path = os.path.join(self.root_dir, row['img_name'])\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # Convert to Tensor\n",
    "\n",
    "        # Bounding boxes and labels\n",
    "        boxes = self.df[self.df['img_name'] == image_name][['xmin', 'ymin', 'xmax', 'ymax']].values \n",
    "        labels = self.df[self.df['img_name'] == image_name]['label'].values\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)  # (n_objects)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Calculate area\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # Assume no crowd annotations\n",
    "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': tv_tensors.BoundingBoxes(boxes, format=tv_tensors.BoundingBoxFormat.XYXY, canvas_size=(image.shape[1], image.shape[2])),\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train: bool): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train (bool): Whether the transform is for training or validation/testing.\n",
    "    \"\"\"\n",
    "    transforms_list = []\n",
    "    transforms_list.append(T.ToImage())\n",
    "    if train:\n",
    "        transforms_list.append(T.RandomApply([T.RandomRotation(degrees=[-5, 5], fill=defaultdict(lambda: 0, {tv_tensors.Image: (0, 0, 0)}))], p=0.5)) # black fill\n",
    "        transforms_list.append(T.RandomHorizontalFlip(0.5))\n",
    "        # transforms_list.append(T.RandomIoUCrop(min_scale = 0.8, max_scale = 1.1)) # values less than one \"zoom-in\" the image\n",
    "        transforms_list.append(T.RandomApply([T.ColorJitter(brightness=0.05, contrast=0.15, saturation=0.05, hue=0)], p=0.5))\n",
    "    transforms_list.append(T.Resize(size=(810,), max_size=1440, interpolation=torchvision.transforms.InterpolationMode.BICUBIC))\n",
    "    transforms_list.append(T.ClampBoundingBoxes()) # Clamp bounding boxes to image boundaries\n",
    "    transforms_list.append(T.SanitizeBoundingBoxes(min_area=5, min_size=1))\n",
    "    transforms_list.append(T.ToDtype(dtype = torch.float32, scale = True)) # Converts image to [0, 1] range\n",
    "    transforms_list.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])) \n",
    "    return T.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions for plotting image and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes are values in label_dict\n",
    "classes = list(label_dict.values())\n",
    "\n",
    "# reverse label dictionary for mapping predictions to classes\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "# distinct colors \n",
    "bbox_colors = [\n",
    "    \"#FF0000\",  # Red\n",
    "    \"#00FF00\",  # Green\n",
    "    \"#0000FF\",  # Blue\n",
    "    \"#FFFF00\",  # Yellow\n",
    "    \"#FF00FF\",  # Magenta\n",
    "    \"#00FFFF\",  # Cyan\n",
    "    \"#FFC0CB\",  # Pink\n",
    "    \"#FFA500\",  # Orange\n",
    "    \"#800080\",  # Purple\n",
    "    \"#FFFFFF\",  # White\n",
    "    \"#FFD700\",  # Gold\n",
    "    \"#000000\"   # Black\n",
    "]\n",
    "\n",
    "# label color map for plotting color-coded boxes by class\n",
    "label_color_map = {k: bbox_colors[i] for i, k in enumerate(label_dict.keys())}\n",
    "\n",
    "# function for reshaping boxes \n",
    "def get_box(boxes):\n",
    "    boxes = np.array(boxes)\n",
    "    boxes = boxes.astype('float').reshape(-1, 4)\n",
    "    if boxes.shape[0] == 1 : return boxes\n",
    "    return np.squeeze(boxes)\n",
    "\n",
    "\n",
    "# function for plotting image\n",
    "def img_show(image, ax = None, figsize = (6, 9)):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.imshow(image)\n",
    "    return ax\n",
    " \n",
    "\n",
    "def plot_bbox(ax, boxes, labels):\n",
    "    # add box to the image and use label_color_map to color-code by bounding box class if exists else 'black'\n",
    "    ax.add_patch(plt.Rectangle((boxes[:, 0], boxes[:, 1]), boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1],\n",
    "                    fill = False,\n",
    "                    color = label_color_map[labels.item()] if labels.item() in label_color_map else 'black', \n",
    "                    linewidth = 1.5))\n",
    "    # add label text to bounding box using label_dict if label exists else labels\n",
    "    ax.text(boxes[:, 2], boxes[:, 3], \n",
    "            (label_dict[labels.item()] if labels.item() in label_dict else labels.item()),\n",
    "            fontsize = 8,\n",
    "            bbox = dict(facecolor = 'white', alpha = 0.8, pad = 0, edgecolor = 'none'),\n",
    "            color = 'black')\n",
    "\n",
    "\n",
    "# function for plotting all boxes and labels on the image using get_polygon, img_show, and plot_mask functions\n",
    "def plot_detections(image, boxes, labels, ax = None):\n",
    "    ax = img_show(image.permute(1, 2, 0), ax = ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = get_box(boxes[i])\n",
    "        plot_bbox(ax, box, labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot sample batch to confirm data loads and transforms correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample batch of data to custom PyTorch Dataset and Transform\n",
    "sample_dataset = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv', \n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images', \n",
    "                                transforms = get_transform(train = True))\n",
    "\n",
    "sample_data_loader = torch.utils.data.DataLoader(sample_dataset, batch_size = 8, shuffle=True, \n",
    "                                                collate_fn = utils.collate_fn, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store images and annotation targets from sample batch\n",
    "batch = next(iter(sample_data_loader))\n",
    "images, targets = batch\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "images = [np.clip(image, 0, 1) for image in images]\n",
    "\n",
    "# Plot all samples from batch in a grid of subplots\n",
    "plt.figure(figsize=(16, int(sample_data_loader.batch_size) * 5))\n",
    "for i in range(int(sample_data_loader.batch_size)):\n",
    "    ax = plt.subplot(int(sample_data_loader.batch_size), 2, 1 + i)\n",
    "    plot_detections(images[i], targets[i]['boxes'], targets[i]['labels'], ax=ax)\n",
    "    # Query the dataset to get the image name for the given image_id\n",
    "    image_id = targets[i]['image_id'].item()  # Convert tensor to integer\n",
    "    image_name = sample_dataset.unique_image_names[image_id]\n",
    "    plt.title(image_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use stratified sampling to split multi-label dataset into train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set random number generator for reproducible data splits\n",
    "rng = np.random.default_rng(np.random.MT19937(np.random.SeedSequence(420)))\n",
    "\n",
    "# Group annotations by image\n",
    "image_groups = df.groupby('img_name')\n",
    "\n",
    "# Create a dictionary to store the class distribution for each image\n",
    "image_class_distribution = {}\n",
    "\n",
    "# Populate the dictionary with class distributions\n",
    "for image_name, group in image_groups:\n",
    "    labels = group['label'].tolist()\n",
    "    image_class_distribution[image_name] = labels\n",
    "\n",
    "# Create a list of all image names and their corresponding labels\n",
    "all_images = list(image_class_distribution.keys())\n",
    "all_labels = [image_class_distribution[image] for image in all_images]\n",
    "\n",
    "# Use the most frequent label for each image for stratification\n",
    "representative_labels = [max(set(labels), key=labels.count) for labels in all_labels]\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.05\n",
    "\n",
    "# Perform stratified split using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=int(1/test_ratio), shuffle=True, random_state=420)\n",
    "\n",
    "train_val_indices, test_indices = next(skf.split(all_images, representative_labels))\n",
    "\n",
    "# Further split train+val into train and validation sets\n",
    "train_val_images = [all_images[idx] for idx in train_val_indices]\n",
    "train_val_labels = [representative_labels[idx] for idx in train_val_indices]\n",
    "\n",
    "skf_val = StratifiedKFold(n_splits=int(1/(val_ratio/(train_ratio + val_ratio))), shuffle=True, random_state=420)\n",
    "train_indices, val_indices = next(skf_val.split(train_val_images, train_val_labels))\n",
    "\n",
    "# Map image names to unique indices\n",
    "image_to_unique_index = {image: idx for idx, image in enumerate(df['img_name'].unique())}\n",
    "\n",
    "# Create lists of unique indices for each split\n",
    "train_indices = [image_to_unique_index[train_val_images[idx]] for idx in train_indices]\n",
    "val_indices = [image_to_unique_index[train_val_images[idx]] for idx in val_indices]\n",
    "test_indices = [image_to_unique_index[all_images[idx]] for idx in test_indices]\n",
    "\n",
    "# Function to get class distribution\n",
    "def get_class_distribution(images, image_class_distribution):\n",
    "    class_counts = defaultdict(int)\n",
    "    for image in images:\n",
    "        for label in image_class_distribution[image]:\n",
    "            class_counts[label] += 1\n",
    "    return class_counts\n",
    "\n",
    "# Get train, val, and test images\n",
    "train_images = [all_images[idx] for idx in train_indices]\n",
    "val_images = [all_images[idx] for idx in val_indices]\n",
    "test_images = [all_images[idx] for idx in test_indices]\n",
    "\n",
    "train_class_distribution = get_class_distribution(train_images, image_class_distribution)\n",
    "val_class_distribution = get_class_distribution(val_images, image_class_distribution)\n",
    "test_class_distribution = get_class_distribution(test_images, image_class_distribution)\n",
    "\n",
    "class_indices = {label: [] for label in df['label'].unique()}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    class_indices[row['label']].append(idx)\n",
    "\n",
    "train_class_distribution = {k: v / len(class_indices[k]) for k, v in train_class_distribution.items()}\n",
    "val_class_distribution = {k: v / len(class_indices[k]) for k, v in val_class_distribution.items()}\n",
    "test_class_distribution = {k: v / len(class_indices[k]) for k, v in test_class_distribution.items()}\n",
    "\n",
    "print(\"Train class distribution:\", dict(sorted(train_class_distribution.items())))\n",
    "print(\"Validation class distribution:\", dict(sorted(val_class_distribution.items())))\n",
    "print(\"Test class distribution:\", dict(sorted(test_class_distribution.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create weighted random sampler to handle class imbalances during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate class weights dynamically\n",
    "def calculate_class_weights(labels, hen_label_int, background_label_int):\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Remove the \"Hen\" class from the counts\n",
    "    hen_count = class_counts.pop(hen_label_int, None)\n",
    "    \n",
    "    # Identify the count for the second most-frequent class\n",
    "    second_most_frequent_class_count = max(class_counts.values())\n",
    "    \n",
    "    # Calculate the weight for the \"Hen\" class\n",
    "    hen_weight = second_most_frequent_class_count / hen_count if hen_count else 1.0\n",
    "    \n",
    "    # Assign weights to all classes\n",
    "    class_weights = {label: sum(class_counts.values()) / count for label, count in class_counts.items()}\n",
    "    \n",
    "    # Normalize weights to range [1, 2]\n",
    "    min_weight = min(class_weights.values())\n",
    "    max_weight = max(class_weights.values())\n",
    "    class_weights = {label: 1 + (weight - min_weight) / (max_weight - min_weight) for label, weight in class_weights.items()}\n",
    "    \n",
    "    # Add weight for the \"Hen\" class\n",
    "    class_weights[hen_label_int] = hen_weight\n",
    "\n",
    "    # Add weight = 5% for the background class\n",
    "    class_weights[background_label_int] = 0.05\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Store train labels for each image\n",
    "train_labels = [label for image in train_images for label in image_class_distribution[image]]\n",
    "\n",
    "# Calculate class weights dynamically\n",
    "hen_label_int = [key for key, value in label_dict.items() if value == 'Hen'][0]  # Get the integer label for \"Hen\"\n",
    "background_label_int = 0  # Assuming background is class 0\n",
    "class_weights = calculate_class_weights(train_labels, hen_label_int, background_label_int)\n",
    "\n",
    "# Convert class weights to a list in the correct order\n",
    "unique_labels = sorted(set(train_labels))\n",
    "train_class_weights = [class_weights[label] for label in unique_labels]\n",
    "\n",
    "# Add weight for the background class\n",
    "train_class_weights = [class_weights[background_label_int]] + train_class_weights  # Background is class 0\n",
    "train_class_weights = torch.tensor(train_class_weights, dtype=torch.float32)\n",
    "\n",
    "# print class counts and class weight for each class\n",
    "print(\"Train class instances and weights: \")\n",
    "for label in unique_labels:\n",
    "    print(f\"{label_dict[label]}: count = {train_labels.count(label)}, weight = {train_class_weights[label]}\")\n",
    "\n",
    "\n",
    "# Calculate sample weights for each image in the training dataset\n",
    "train_sample_weights = []\n",
    "for image_name in train_images:\n",
    "    labels = image_class_distribution[image_name]\n",
    "    sample_weight = sum(train_class_weights[label] for label in labels) / len(labels)\n",
    "    train_sample_weights.append(sample_weight)\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Custom RetinaNet with ResNet FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead, RetinaNetRegressionHead\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from typing import Optional, Callable, List\n",
    "from torchvision.ops import sigmoid_focal_loss, FrozenBatchNorm2d\n",
    "\n",
    "def _sum(x: List[torch.Tensor]) -> torch.Tensor:\n",
    "    res = x[0]\n",
    "    for i in x[1:]:\n",
    "        res = res + i\n",
    "    return res\n",
    "\n",
    "class CustomRetinaNetClassificationHead(RetinaNetClassificationHead):\n",
    "    def __init__(self, in_channels, num_anchors, num_classes, alpha=0.25, gamma_loss=2.0, prior_probability=0.01, norm_layer: Optional[Callable[..., nn.Module]] = None, dropout_prob=0.25, class_weights=None):\n",
    "        super().__init__(in_channels, num_anchors, num_classes, prior_probability, norm_layer)\n",
    "        self.alpha = alpha\n",
    "        self.gamma_loss = gamma_loss\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, targets, head_outputs, matched_idxs):\n",
    "        losses = []\n",
    "\n",
    "        cls_logits = head_outputs[\"cls_logits\"]\n",
    "\n",
    "        for i, (targets_per_image, cls_logits_per_image, matched_idxs_per_image) in enumerate(zip(targets, cls_logits, matched_idxs)):\n",
    "            # determine only the foreground\n",
    "            foreground_idxs_per_image = matched_idxs_per_image >= 0\n",
    "            num_foreground = foreground_idxs_per_image.sum()\n",
    "\n",
    "            # create the target classification\n",
    "            gt_classes_target = torch.zeros_like(cls_logits_per_image)\n",
    "            gt_classes_target[\n",
    "                foreground_idxs_per_image,\n",
    "                targets_per_image[\"labels\"][matched_idxs_per_image[foreground_idxs_per_image]],\n",
    "            ] = 1.0\n",
    "\n",
    "            # find indices for which anchors should be ignored\n",
    "            valid_idxs_per_image = matched_idxs_per_image != self.BETWEEN_THRESHOLDS\n",
    "\n",
    "            # get the class weights for the valid indices\n",
    "            if self.class_weights is not None:\n",
    "                valid_labels = targets_per_image[\"labels\"][matched_idxs_per_image[valid_idxs_per_image]]\n",
    "                weights = self.class_weights.to(valid_labels.device)[valid_labels]\n",
    "            else:\n",
    "                weights = torch.ones_like(valid_idxs_per_image, dtype=torch.float32)\n",
    "\n",
    "            # compute the classification loss with custom alpha, gamma_loss, and class weights\n",
    "            losses.append(\n",
    "                (sigmoid_focal_loss(\n",
    "                    cls_logits_per_image[valid_idxs_per_image],\n",
    "                    gt_classes_target[valid_idxs_per_image],\n",
    "                    alpha=self.alpha,\n",
    "                    gamma=self.gamma_loss,\n",
    "                    reduction=\"none\",\n",
    "                ) * weights.unsqueeze(1)).sum() / max(1, num_foreground)\n",
    "            )\n",
    "\n",
    "        return _sum(losses) / len(targets)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_cls_logits = []\n",
    "        for features in x:\n",
    "            cls_logits = self.conv(features)\n",
    "            cls_logits = self.dropout(cls_logits)  # Apply dropout\n",
    "            cls_logits = self.cls_logits(cls_logits)\n",
    "\n",
    "            # Permute classification output from (N, A * K, H, W) to (N, HWA, K).\n",
    "            N, _, H, W = cls_logits.shape\n",
    "            cls_logits = cls_logits.view(N, -1, self.num_classes, H, W)\n",
    "            cls_logits = cls_logits.permute(0, 3, 4, 1, 2)\n",
    "            cls_logits = cls_logits.reshape(N, -1, self.num_classes)  # Size=(N, HWA, K)\n",
    "\n",
    "            all_cls_logits.append(cls_logits)\n",
    "\n",
    "\n",
    "        return torch.cat(all_cls_logits, dim=1)\n",
    "\n",
    "class CustomRetinaNetRegressionHead(RetinaNetRegressionHead):\n",
    "    def __init__(self, in_channels, num_anchors, norm_layer: Optional[Callable[..., nn.Module]] = None, dropout_prob=0.25):\n",
    "        super().__init__(in_channels, num_anchors, norm_layer)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        all_bbox_regression = []\n",
    "        for features in x:\n",
    "            bbox_regression = self.conv(features)\n",
    "            bbox_regression = self.dropout(bbox_regression)  # Apply dropout\n",
    "            bbox_regression = self.bbox_reg(bbox_regression)\n",
    "\n",
    "            # Permute bbox regression output from (N, 4 * A, H, W) to (N, HWA, 4).\n",
    "            N, _, H, W = bbox_regression.shape\n",
    "            bbox_regression = bbox_regression.view(N, -1, 4, H, W)\n",
    "            bbox_regression = bbox_regression.permute(0, 3, 4, 1, 2)\n",
    "            bbox_regression = bbox_regression.reshape(N, -1, 4)  # Size=(N, HWA, 4)\n",
    "\n",
    "            all_bbox_regression.append(bbox_regression)\n",
    "\n",
    "        return torch.cat(all_bbox_regression, dim=1)\n",
    "\n",
    "def get_retinanet_model(depth, num_classes=12, min_size=810, max_size=1440, image_mean=[0, 0, 0], image_std=[1, 1, 1], score_thresh=0.1, nms_thresh=0.4, \n",
    "                        detections_per_img=200, fg_iou_thresh=0.4, bg_iou_thresh=0.2, topk_candidates=400, alpha=0.75,\n",
    "                        gamma_loss=3.0, trainable_backbone_layers=4, dropout_prob=0.25, class_weights=None):\n",
    "    # Create the backbone with FPN\n",
    "    if depth == 18:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet18', \n",
    "                                       weights=torchvision.models.ResNet18_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 34:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet34', \n",
    "                                       weights=torchvision.models.ResNet34_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 50:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet50', \n",
    "                                       weights=torchvision.models.ResNet50_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 101:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet101', \n",
    "                                       weights=torchvision.models.ResNet101_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 152:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet152', \n",
    "                                       weights=torchvision.models.ResNet152_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model depth\")\n",
    "\n",
    "    # Create the RetinaNet model with the custom backbone\n",
    "    model = RetinaNet(backbone, \n",
    "                      num_classes=num_classes,\n",
    "                      min_size=min_size, # same size as resize in transform to keep aspect ratio\n",
    "                      max_size=max_size,\n",
    "                      image_mean=image_mean,\n",
    "                      image_std=image_std,\n",
    "                      score_thresh=score_thresh, \n",
    "                      nms_thresh=nms_thresh, \n",
    "                      detections_per_img=detections_per_img,\n",
    "                      fg_iou_thresh=fg_iou_thresh,\n",
    "                      bg_iou_thresh=bg_iou_thresh,\n",
    "                      topk_candidates=topk_candidates\n",
    "                      )\n",
    "\n",
    "    # Replace the classification head with the custom one\n",
    "    in_channels = model.head.classification_head.cls_logits.in_channels\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "    model.head.classification_head = CustomRetinaNetClassificationHead(in_channels, \n",
    "                                                                       num_anchors, \n",
    "                                                                       num_classes, \n",
    "                                                                       alpha=alpha, \n",
    "                                                                       gamma_loss=gamma_loss, \n",
    "                                                                       dropout_prob=dropout_prob,\n",
    "                                                                       class_weights=class_weights)\n",
    "\n",
    "    # Replace the regression head with the custom one\n",
    "    model.head.regression_head = CustomRetinaNetRegressionHead(in_channels, num_anchors, dropout_prob=dropout_prob)\n",
    "    \n",
    "    model.anchor_generator = AnchorGenerator(sizes=((16, 32, 53), (32, 65, 107), (64, 130, 214), (128, 261, 428), (256, 522, 856)), \n",
    "                                             aspect_ratios=((0.9, 1.7, 2.6), (0.9, 1.7, 2.6), (0.9, 1.7, 2.6), (0.9, 1.7, 2.6), (0.9, 1.7, 2.6)))\n",
    "    \n",
    "    # replace FrozenBatchNorm2d in the last two backbone layers with trainable batchNorm2d\n",
    "    for name, module in model.backbone.body.named_modules():\n",
    "        layer_num = int(name.split('.')[0][-1])  # Extract the layer number (e.g., 'layer3' -> 3)\n",
    "        if layer_num > (4 - trainable_backbone_layers):  # Unfreeze the last `trainable_backbone_layers` layers\n",
    "            if isinstance(module, FrozenBatchNorm2d):\n",
    "                # Extract the number of features, mean, and variance from the FrozenBatchNorm2d\n",
    "                num_features = module.weight.shape[0]  # Get the number of channels (features)\n",
    "                running_mean = module.running_mean.clone()  # Extract running mean\n",
    "                running_var = module.running_var.clone()    # Extract running variance\n",
    "\n",
    "                # Create a new BatchNorm2d layer with the same number of features\n",
    "                batch_norm = nn.BatchNorm2d(num_features)\n",
    "\n",
    "                # Initialize BatchNorm2d with the extracted running mean and variance\n",
    "                batch_norm.running_mean = running_mean\n",
    "                batch_norm.running_var = running_var\n",
    "\n",
    "                # Initialize the weights with a normal distribution (mean=1, std=0.02) for stability\n",
    "                nn.init.normal_(batch_norm.weight, mean=1.0, std=0.02)  # Small normal distribution\n",
    "\n",
    "                # Initialize the biases to zero (standard for BatchNorm2d)\n",
    "                nn.init.constant_(batch_norm.bias, 0)\n",
    "\n",
    "                # Replace the FrozenBatchNorm2d layer with BatchNorm2d\n",
    "                parent_module_name = '.'.join(name.split('.')[:-1])  # Get the parent module name\n",
    "                module_name = name.split('.')[-1]  # Extract the last part of the name (the module name)\n",
    "\n",
    "                # Retrieve the parent module and replace the FrozenBatchNorm2d with BatchNorm2d\n",
    "                parent_module = dict(model.backbone.body.named_modules())[parent_module_name]\n",
    "                setattr(parent_module, module_name, batch_norm)\n",
    "\n",
    "                # Set requires_grad to True for the new BatchNorm2d layer\n",
    "                for param in batch_norm.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_retinanet_model(depth=34))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Tune Model Hyperparameters using Ray Tune**</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class for tuning RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "import gc\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import ray.cloudpickle as pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "\n",
    "from engine_gradientAccumulation import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "\n",
    "# Set random seed for reproducible training\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def extract_per_class_metrics(coco_eval, coco_gt):\n",
    "    # Ensure that the evaluation has been performed and results are available\n",
    "    if 'precision' not in coco_eval.coco_eval['bbox'].eval or 'recall' not in coco_eval.coco_eval['bbox'].eval:\n",
    "        raise KeyError(\"Evaluation results are not available. Ensure that the evaluation has been performed.\")\n",
    "\n",
    "    per_class_metrics = {}\n",
    "\n",
    "    # Create a list of category IDs in the order they appear in the evaluation results\n",
    "    cat_ids = list(coco_gt.cats.keys())\n",
    "    cat_id_to_index = {cat_id: idx for idx, cat_id in enumerate(cat_ids)}\n",
    "\n",
    "    for cat_id, idx in cat_id_to_index.items():\n",
    "        try:\n",
    "            precision = coco_eval.coco_eval['bbox'].eval['precision'][:, :, idx, 0, 2]\n",
    "            recall = coco_eval.coco_eval['bbox'].eval['recall'][:, idx, 0, 2]\n",
    "\n",
    "            per_class_metrics[cat_id] = {\n",
    "                'precision': precision.mean(),\n",
    "                'recall': recall.mean()\n",
    "            }\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError for category ID {cat_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return per_class_metrics\n",
    "\n",
    "\n",
    "class RetinaNetTuner:\n",
    "    def __init__(self, num_samples, max_num_epochs, restore_path=\"\"):\n",
    "        self.num_samples = num_samples\n",
    "        self.max_num_epochs = max_num_epochs\n",
    "        self.restore_path = restore_path\n",
    "\n",
    "    def create_coco_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            train_future = executor.submit(get_coco_api_from_dataset, train_dataset)\n",
    "            val_future = executor.submit(get_coco_api_from_dataset, val_dataset)\n",
    "            test_future = executor.submit(get_coco_api_from_dataset, test_dataset)\n",
    "            train_coco_ds = train_future.result()\n",
    "            val_coco_ds = val_future.result()\n",
    "            test_coco_ds = test_future.result()\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds\n",
    "    \n",
    "    def train_lr_finder(self, config):\n",
    "        class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "            def inputs_labels_from_batch(self, batch_data):\n",
    "                inputs = [image.to('cuda:0') for image in batch_data[0]]\n",
    "                labels = [{k: v.to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "                return inputs, labels\n",
    "\n",
    "        dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "        accumulation_steps = 1  ## FIXME: hardcoded for now\n",
    "\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=config[\"batch_size\"],\n",
    "                                                        sampler=config[\"train_sampler\"],\n",
    "                                                        collate_fn=utils.collate_fn,\n",
    "                                                        num_workers=0, pin_memory=True)\n",
    "\n",
    "        model = get_retinanet_model(\n",
    "            depth=50,\n",
    "            num_classes=len(config[\"class_weights\"]),\n",
    "            score_thresh=config[\"score_thresh\"],\n",
    "            nms_thresh=config[\"nms_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=0.4,\n",
    "            bg_iou_thresh=0.2,\n",
    "            topk_candidates=400,\n",
    "            alpha=config[\"alpha\"],\n",
    "            gamma_loss=config[\"gamma_loss\"],\n",
    "            trainable_backbone_layers=4,\n",
    "            dropout_prob=config[\"dropout\"],\n",
    "            class_weights=config[\"class_weights\"]\n",
    "        ).to('cuda:0')\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params, lr=1e-7, momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "        grad_scaler = torch.GradScaler()\n",
    "\n",
    "        class CustomLRFinder(LRFinder):\n",
    "            def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "                super().__init__(model, optimizer, criterion, device)\n",
    "                self.amp_backend = amp_backend\n",
    "                self.amp_config = amp_config\n",
    "                self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "            def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                for _ in range(accumulation_steps):\n",
    "                    inputs, labels = next(train_iter)\n",
    "                    inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                    with torch.autocast(device_type=\"cuda:0\"):\n",
    "                        outputs = self.model(inputs, labels)\n",
    "                        loss = sum(loss for loss in outputs.values())\n",
    "\n",
    "                    loss /= accumulation_steps\n",
    "                    self.grad_scaler.scale(loss).backward()\n",
    "                    total_loss += loss\n",
    "\n",
    "                self.grad_scaler.step(self.optimizer)\n",
    "                self.grad_scaler.update()\n",
    "\n",
    "                return total_loss.item()\n",
    "\n",
    "        lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "        lr_finder.range_test(train_iter, end_lr=1, num_iter=len(data_loader_train), step_mode='exp', accumulation_steps=accumulation_steps) # num_iter = len(dataloader) to sample from full train dataset\n",
    "        suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "        lr_finder.reset()\n",
    "\n",
    "        # return default if torch lr finder fails\n",
    "        try:\n",
    "            if isinstance(suggested_lr, tuple):\n",
    "                axes, suggested_lr_value = suggested_lr\n",
    "                return suggested_lr_value\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during learning rate finding: {e}\")\n",
    "            # Return a default learning rate if an error occurs\n",
    "            return 5e-4\n",
    "\n",
    "    def train_MAVdroneDataset(self, config):\n",
    "\n",
    "        set_seed(420)\n",
    "\n",
    "        dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "        data_loader_val = ray.get(config[\"data_loader_val_ref\"])\n",
    "        train_coco_ds = ray.get(config[\"train_coco_ds_ref\"])\n",
    "        val_coco_ds = ray.get(config[\"val_coco_ds_ref\"])\n",
    "\n",
    "        training_steps = [\n",
    "            {\"step\": 0, \"batch_size\": config[\"batch_size\"], \"print_freq\": 25, \"accumulation_steps\": 1, \"backbone_layers\": 1},\n",
    "            {\"step\": 1, \"batch_size\": config[\"batch_size\"], \"print_freq\": 25, \"accumulation_steps\": 2, \"backbone_layers\": 2},\n",
    "            {\"step\": 2, \"batch_size\": config[\"batch_size\"], \"print_freq\": 25, \"accumulation_steps\": 4, \"backbone_layers\": 3},\n",
    "            {\"step\": 3, \"batch_size\": config[\"batch_size\"], \"print_freq\": 25, \"accumulation_steps\": 8, \"backbone_layers\": 4}\n",
    "        ]\n",
    "\n",
    "        checkpoint = train.get_checkpoint()\n",
    "        if checkpoint:\n",
    "            with checkpoint.as_directory() as checkpoint_dir:\n",
    "                data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                with open(data_path, \"rb\") as fp:\n",
    "                    checkpoint_state = pickle.load(fp)\n",
    "                start_epoch = checkpoint_state[\"epoch\"] + 1\n",
    "                current_step = checkpoint_state[\"current_step\"]\n",
    "                batch_size = checkpoint_state[\"batch_size\"]\n",
    "                accumulation_steps = checkpoint_state[\"accumulation_steps\"]\n",
    "                backbone_layers = checkpoint_state[\"backbone_layers\"]  # Load trainable backbone layers\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            current_step = 0\n",
    "            batch_size = config[\"batch_size\"]\n",
    "            accumulation_steps = training_steps[0][\"accumulation_steps\"]\n",
    "            backbone_layers = training_steps[0][\"backbone_layers\"]  # Initialize trainable backbone layers\n",
    "\n",
    "        step_index = current_step\n",
    "\n",
    "        while step_index < len(training_steps):\n",
    "            step = training_steps[step_index]\n",
    "            batch_size = step['batch_size']\n",
    "            print_freq = step['print_freq']\n",
    "            accumulation_steps = step['accumulation_steps']\n",
    "            backbone_layers = step['backbone_layers']\n",
    "            effective_bs = batch_size * accumulation_steps\n",
    "\n",
    "            # scale the learning rate with batch size increases\n",
    "            base_bs = training_steps[0]['batch_size'] * training_steps[0]['accumulation_steps']\n",
    "            scaled_lr = config[\"lr\"] * (effective_bs / base_bs)\n",
    "\n",
    "            model = get_retinanet_model(depth=50,\n",
    "                                        num_classes=len(config[\"class_weights\"]),\n",
    "                                        score_thresh=config[\"score_thresh\"],\n",
    "                                        nms_thresh=config[\"nms_thresh\"],\n",
    "                                        detections_per_img=200,\n",
    "                                        fg_iou_thresh=0.4,\n",
    "                                        bg_iou_thresh=0.2,\n",
    "                                        topk_candidates=400,\n",
    "                                        alpha=config[\"alpha\"],\n",
    "                                        gamma_loss=config[\"gamma_loss\"],\n",
    "                                        trainable_backbone_layers=backbone_layers,\n",
    "                                        dropout_prob=config[\"dropout\"],\n",
    "                                        class_weights=config[\"class_weights\"])\n",
    "\n",
    "            device = \"cpu\"\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda:0\"\n",
    "            model.to(device)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            params = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.SGD(params, lr=scaled_lr,\n",
    "                                        momentum=config[\"momentum\"],\n",
    "                                        weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "            lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "            if checkpoint:\n",
    "                model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "                optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                                    sampler=config[\"train_sampler\"],\n",
    "                                                    collate_fn=utils.collate_fn,\n",
    "                                                    num_workers=0, pin_memory=True)\n",
    "\n",
    "            print(f'Training step: {step[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}')\n",
    "            print()\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "\n",
    "            while patience_counter < 5:\n",
    "                train_metric_logger, val_metric_logger = train_one_epoch(model, optimizer, data_loader, device,\n",
    "                                                                        start_epoch, print_freq, accumulation_steps,\n",
    "                                                                        data_loader_val)\n",
    "\n",
    "                train_coco_evaluator, val_coco_evaluator = evaluate(model, data_loader_val, val_coco_ds, device, data_loader, train_coco_ds)\n",
    "\n",
    "                # get class metrics\n",
    "                train_class_metrics = extract_per_class_metrics(train_coco_evaluator, train_coco_ds)\n",
    "                val_class_metrics = extract_per_class_metrics(val_coco_evaluator, val_coco_ds)\n",
    "\n",
    "                # map category names using label_dict\n",
    "                train_class_metrics = {label_dict[k]: v for k, v in train_class_metrics.items()}\n",
    "                val_class_metrics = {label_dict[k]: v for k, v in val_class_metrics.items()}\n",
    "\n",
    "                # Print per-class metrics\n",
    "                print(\"Training Class Metrics:\")\n",
    "                for class_name, metrics in train_class_metrics.items():\n",
    "                    print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "                print()\n",
    "\n",
    "                print(\"Validation Class Metrics:\")\n",
    "                for class_name, metrics in val_class_metrics.items():\n",
    "                    print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "                print()\n",
    "\n",
    "                lr_scheduler.step(metrics=val_metric_logger.loss.avg)\n",
    "\n",
    "                checkpoint_data = {\n",
    "                    \"epoch\": start_epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"current_step\": step[\"step\"],\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"accumulation_steps\": accumulation_steps,\n",
    "                    \"backbone_layers\": backbone_layers,  # Save trainable backbone layers\n",
    "                }\n",
    "\n",
    "                with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                    data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                    with open(data_path, \"wb\") as fp:\n",
    "                        pickle.dump(checkpoint_data, fp)\n",
    "                    train.report(\n",
    "                        {\"epoch\": start_epoch,\n",
    "                        \"train_loss\": train_metric_logger.loss.avg,\n",
    "                        \"val_loss\": val_metric_logger.loss.avg,\n",
    "                        \"train_mAP\": train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                        \"val_mAP\": val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                        \"train_mAR\": train_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                        \"val_mAR\": val_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                        \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                        train_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                        \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                    val_coco_evaluator.coco_eval['bbox'].stats[8])},\n",
    "                        checkpoint=train.Checkpoint.from_directory(checkpoint_dir),\n",
    "                    )\n",
    "\n",
    "                if val_metric_logger.loss.avg < best_val_loss:\n",
    "                    best_val_loss = val_metric_logger.loss.avg\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                start_epoch += 1\n",
    "\n",
    "            step_index += 1\n",
    "\n",
    "        print('Tuning Trial Complete!')\n",
    "\n",
    "    def trial_dirname_creator(self, trial):\n",
    "        return f\"{trial.trial_id}\"\n",
    "\n",
    "    def run(self):\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "\n",
    "        dataset = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=True)\n",
    "        )\n",
    "\n",
    "        dataset_val = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_test = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_train = torch.utils.data.Subset(dataset, train_indices)\n",
    "        dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "        dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "            dataset_val, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        data_loader_test = torch.utils.data.DataLoader(\n",
    "            dataset_test, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        train_coco_ds, val_coco_ds, test_coco_ds = self.create_coco_datasets(dataset_train, dataset_val, dataset_test)\n",
    "\n",
    "        dataset_train_ref = ray.put(dataset_train)\n",
    "        data_loader_val_ref = ray.put(data_loader_val)\n",
    "        data_loader_test_ref = ray.put(data_loader_test)\n",
    "        train_coco_ds_ref = ray.put(train_coco_ds)\n",
    "        val_coco_ds_ref = ray.put(val_coco_ds)\n",
    "        test_coco_ds_ref = ray.put(test_coco_ds)\n",
    "\n",
    "        config = {\n",
    "            # \"lr\": tune.sample_from(lambda config: self.train_lr_finder(config)),\n",
    "            \"lr\": tune.loguniform(0.0001, 0.01),\n",
    "            \"momentum\": tune.uniform(0.5, 0.9),\n",
    "            \"weight_decay\": tune.loguniform(0.00001, 0.005),\n",
    "            \"alpha\": tune.uniform(0.3, 0.9),\n",
    "            \"gamma_loss\": tune.uniform(2.0, 4.0),\n",
    "            \"dropout\": tune.uniform(0.2, 0.6),\n",
    "            \"score_thresh\": tune.uniform(0.25, 0.75),\n",
    "            \"nms_thresh\": tune.uniform(0.05, 0.35),\n",
    "            \"dataset_train_ref\": dataset_train_ref,\n",
    "            \"data_loader_val_ref\": data_loader_val_ref,\n",
    "            \"data_loader_test_ref\": data_loader_test_ref,\n",
    "            \"train_coco_ds_ref\": train_coco_ds_ref,\n",
    "            \"val_coco_ds_ref\": val_coco_ds_ref,\n",
    "            \"test_coco_ds_ref\": test_coco_ds_ref,\n",
    "            \"train_sampler\": train_sampler,\n",
    "            \"class_weights\": train_class_weights\n",
    "        }\n",
    "\n",
    "        if tune.Tuner.can_restore(os.path.abspath(self.restore_path)):\n",
    "            tuner = tune.Tuner.restore(\n",
    "                os.path.abspath(self.restore_path),\n",
    "                trainable=self.train_MAVdroneDataset,\n",
    "                param_space=config,\n",
    "                resume_unfinished=True,\n",
    "                resume_errored=False\n",
    "            )\n",
    "            print(f\"Tuner Restored from {self.restore_path}\")\n",
    "        else:\n",
    "            algo = TuneBOHB(\n",
    "                points_to_evaluate=[\n",
    "                     {\"lr\": 0.0018174272508271936,\n",
    "                     \"momentum\": 0.641007483166926,\n",
    "                     \"weight_decay\": 1.7737466760404307e-05,\n",
    "                     \"alpha\": 0.4893875487532765,\n",
    "                     \"gamma_loss\": 3.736332958570757,\n",
    "                     \"dropout\": 0.24357127281473448,\n",
    "                     \"score_thresh\": 0.3394643439588512,\n",
    "                     \"nms_thresh\": 0.22026127980940127\n",
    "                     }\n",
    "                ],\n",
    "                seed=420\n",
    "            )\n",
    "\n",
    "            algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "\n",
    "            scheduler = HyperBandForBOHB(\n",
    "                time_attr=\"training_iteration\",\n",
    "                max_t=int(self.max_num_epochs),\n",
    "                reduction_factor=4,\n",
    "                stop_last_trials=False,\n",
    "            )\n",
    "\n",
    "            reporter = tune.JupyterNotebookReporter(overwrite=True,\n",
    "                metric_columns=[\"epoch\", \"train_loss\", \"val_loss\", \"train_mAP\", \"val_mAP\", \"train_mAR\", \"val_mAR\", \"train_f1\", \"val_f1\"],\n",
    "                parameter_columns=[\"lr\", \"momentum\", \"weight_decay\", \"alpha\", \"gamma_loss\", \"dropout\", \"score_thresh\", \"nms_thresh\"],\n",
    "                print_intermediate_tables=True,\n",
    "                sort_by_metric=True\n",
    "            )\n",
    "\n",
    "            tuner = tune.Tuner(\n",
    "                tune.with_resources(\n",
    "                    self.train_MAVdroneDataset,\n",
    "                    resources={\"cpu\": 24.0, \"gpu\": 1.0}\n",
    "                ),\n",
    "                run_config=train.RunConfig(\n",
    "                    name=f\"BOHB_RetinaNet_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                    failure_config=train.FailureConfig(max_failures=1),\n",
    "                    progress_reporter=reporter,\n",
    "                ),\n",
    "                tune_config=tune.TuneConfig(\n",
    "                    mode=\"min\",\n",
    "                    metric=\"val_loss\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    num_samples=int(self.num_samples),\n",
    "                    trial_dirname_creator=self.trial_dirname_creator\n",
    "                ),\n",
    "                param_space=config\n",
    "            )\n",
    "        results = tuner.fit()\n",
    "\n",
    "        best_trial = results.get_best_result(\"val_f1\", \"max\")\n",
    "\n",
    "        print(\"Best trial config: {}\".format(best_trial.config))\n",
    "        print()\n",
    "        print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "        print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "        print(\"Best trial final training mAP: {}\".format(best_trial.metrics[\"train_mAP\"]))\n",
    "        print(\"Best trial final validation mAP: {}\".format(best_trial.metrics[\"val_mAP\"]))\n",
    "        print(\"Best trial final training mAR: {}\".format(best_trial.metrics[\"train_mAR\"]))\n",
    "        print(\"Best trial final validation mAR: {}\".format(best_trial.metrics[\"val_mAR\"]))\n",
    "        print(\"Best trial final training f1-score: {}\".format(best_trial.metrics[\"train_f1\"]))\n",
    "        print(\"Best trial final validation f1-score: {}\".format(best_trial.metrics[\"val_f1\"]))\n",
    "        \n",
    "        print()\n",
    "\n",
    "        best_checkpoint = best_trial.get_best_checkpoint(metric=\"val_f1\", mode=\"max\")\n",
    "\n",
    "        self.test_best_model(best_trial, best_checkpoint)\n",
    "\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial\n",
    "\n",
    "    def test_best_model(self, best_trial, best_checkpoint):\n",
    "        best_model = get_retinanet_model(depth=50,\n",
    "                                         num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                                         score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                                         nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                                         detections_per_img=200,\n",
    "                                         fg_iou_thresh=0.4,\n",
    "                                         bg_iou_thresh=0.2,\n",
    "                                         topk_candidates=400,\n",
    "                                         alpha=best_trial.config[\"alpha\"],\n",
    "                                         gamma_loss=best_trial.config[\"gamma_loss\"],\n",
    "                                         trainable_backbone_layers=4,\n",
    "                                         dropout_prob=best_trial.config[\"dropout\"],\n",
    "                                         class_weights=None)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        best_model.to(device)\n",
    "\n",
    "        with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                best_checkpoint_data = pickle.load(fp)\n",
    "            best_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "\n",
    "        data_loader_test = ray.get(best_trial.config[\"data_loader_test_ref\"])\n",
    "        test_coco_ds = ray.get(best_trial.config[\"test_coco_ds_ref\"])\n",
    "\n",
    "        test_results = evaluate(best_model, data_loader_test, test_coco_ds, device, train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "        print(f'Best trial test set mAP: {test_results.coco_eval[\"bbox\"].stats[0]}')\n",
    "        print(f'Best trial test set mAR: {test_results.coco_eval[\"bbox\"].stats[8]}')\n",
    "        print(f'Best trial test set f1-score: {calculate_f1_score(test_results.coco_eval[\"bbox\"].stats[0], test_results.coco_eval[\"bbox\"].stats[8])}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    trainer = RetinaNetTuner(num_samples=50, max_num_epochs=50, restore_path=\"C:/Users/exx/ray_results/FALSE\")\n",
    "    train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial = trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Train Model Using Tuned Hyperparameters**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.profiler\n",
    "\n",
    "def main(train_coco_ds, val_coco_ds, best_trial):\n",
    "    # Set seed\n",
    "    set_seed(420)\n",
    "\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": best_trial.config[\"batch_size\"], \"print_freq\": 25, \"accumulation_steps\": 1, \"backbone_layers\": 1},\n",
    "        {\"step\": 1, \"batch_size\": best_trial.config[\"batch_size\"], \"print_freq\": 25, \"accumulation_steps\": 2, \"backbone_layers\": 2},\n",
    "        {\"step\": 2, \"batch_size\": best_trial.config[\"batch_size\"], \"print_freq\": 25, \"accumulation_steps\": 4, \"backbone_layers\": 3},\n",
    "        {\"step\": 3, \"batch_size\": best_trial.config[\"batch_size\"], \"print_freq\": 25, \"accumulation_steps\": 8, \"backbone_layers\": 4}\n",
    "    ]\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    # Initialize the tensorboard writer\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f'C:/Users/exx/Documents/GitHub/SSD_VGG_PyTorch/runs/RetinaNet/{current_datetime}')\n",
    "\n",
    "    # Store one checkpoint dictionary for each epoch in a list of dictionaries. \n",
    "    checkpoints = []\n",
    "\n",
    "    # Initialize the profiler\n",
    "    profiler = torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=1,\n",
    "            warmup=1,\n",
    "            active=3,\n",
    "            repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name = f'C:/Users/exx/Documents/GitHub/SSD_VGG_PyTorch/runs/RetinaNet/{current_datetime}_profiler'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    )\n",
    "\n",
    "    # Load training and validation datasets\n",
    "    dataset = MAVdroneDataset(csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                              root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/', \n",
    "                              transforms=get_transform(train=True))\n",
    "\n",
    "    dataset_val = MAVdroneDataset(csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                                  root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "                                  transforms=get_transform(train=False))\n",
    "    \n",
    "    # Subset using a 80/15/5 split for train, validation, and test datasets\n",
    "    dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "\n",
    "    data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False,\n",
    "                                                  collate_fn=utils.collate_fn, num_workers=0,\n",
    "                                                  pin_memory=True)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    step_index = 0\n",
    "    patience = 5\n",
    "\n",
    "    while step_index < len(training_steps):\n",
    "        step = training_steps[step_index]\n",
    "        batch_size = step['batch_size']\n",
    "        print_freq = step['print_freq']\n",
    "        accumulation_steps = step['accumulation_steps']\n",
    "        backbone_layers = step['backbone_layers']\n",
    "        effective_bs = batch_size * accumulation_steps\n",
    "\n",
    "        # scale the learning rate with batch size increases\n",
    "        base_bs = training_steps[0]['batch_size'] * training_steps[0]['accumulation_steps']\n",
    "        scaled_lr = best_trial.config[\"lr\"] * (effective_bs / base_bs)\n",
    "\n",
    "        # Reinitialize the model with the current thresholds\n",
    "        model = get_retinanet_model(depth=50,\n",
    "                                    num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                                    score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                                    nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                                    detections_per_img=200,\n",
    "                                    fg_iou_thresh=0.4,\n",
    "                                    bg_iou_thresh=0.2,\n",
    "                                    topk_candidates=400, \n",
    "                                    alpha=best_trial.config[\"alpha\"], \n",
    "                                    gamma_loss=best_trial.config[\"gamma_loss\"], \n",
    "                                    trainable_backbone_layers=backbone_layers, \n",
    "                                    dropout_prob=best_trial.config[\"dropout\"],\n",
    "                                    class_weights=best_trial.config[\"class_weights\"])\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Construct an optimizer with the suggested learning rate\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=scaled_lr,\n",
    "                                    momentum=best_trial.config[\"momentum\"], \n",
    "                                    weight_decay=best_trial.config[\"weight_decay\"])\n",
    "        \n",
    "        # And a learning rate scheduler\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor=0.5, patience=5)\n",
    "\n",
    "        # Define training and validation data loaders\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                                #   sampler=best_trial.config[\"train_sampler\"], \n",
    "                                                  shuffle = True, \n",
    "                                                  collate_fn=utils.collate_fn, num_workers=0,\n",
    "                                                  pin_memory=True)\n",
    "        \n",
    "        print(f'Training step: {step[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}')\n",
    "        print()\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        #########################################################\n",
    "        ##               The main training loop                ##\n",
    "        #########################################################\n",
    "        with profiler:\n",
    "            while patience_counter < patience:\n",
    "                # Monitor memory usage at the start of the epoch\n",
    "                print(f\"Epoch {start_epoch} - Memory allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "\n",
    "                train_metric_logger, val_metric_logger = train_one_epoch(model, optimizer, data_loader, device, \n",
    "                                                                         start_epoch, print_freq, accumulation_steps,\n",
    "                                                                         data_loader_val)\n",
    "\n",
    "                # Evaluate on the validation dataset\n",
    "                train_coco_evaluator, val_coco_evaluator = evaluate(model, data_loader_val, val_coco_ds, device,\n",
    "                                                                    data_loader, train_coco_ds)\n",
    "                \n",
    "                # Get class metrics\n",
    "                train_class_metrics = extract_per_class_metrics(train_coco_evaluator, train_coco_ds)\n",
    "                val_class_metrics = extract_per_class_metrics(val_coco_evaluator, val_coco_ds)\n",
    "\n",
    "                # map category names using label_dict\n",
    "                train_class_metrics = {label_dict[k]: v for k, v in train_class_metrics.items()}\n",
    "                val_class_metrics = {label_dict[k]: v for k, v in val_class_metrics.items()}\n",
    "\n",
    "                # Print per-class metrics\n",
    "                print(\"Training Class Metrics:\")\n",
    "                for class_name, metrics in train_class_metrics.items():\n",
    "                    print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "                print()\n",
    "\n",
    "                print(\"Validation Class Metrics:\")\n",
    "                for class_name, metrics in val_class_metrics.items():\n",
    "                    print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "                print()\n",
    "\n",
    "                # Update the learning rate\n",
    "                lr_scheduler.step(metrics=val_metric_logger.loss.avg)\n",
    "\n",
    "                # Store training and validation metrics in checkpoint dictionary. \n",
    "                checkpoint = {\n",
    "                    \"epoch\": start_epoch,\n",
    "                    \"train_loss\": train_metric_logger.loss.avg, # average across entire training epoch\n",
    "                    \"train_bbox_loss\": train_metric_logger.bbox_regression.avg,\n",
    "                    \"train_class_loss\": train_metric_logger.classification.avg,\n",
    "                    \"val_loss\": val_metric_logger.loss.avg,\n",
    "                    \"val_bbox_loss\": val_metric_logger.bbox_regression.avg,\n",
    "                    \"val_class_loss\": val_metric_logger.classification.avg,\n",
    "                    \"train_mAP\": train_coco_evaluator.coco_eval['bbox'].stats[0],# IoU=0.50:0.95\n",
    "                    \"train_mAR\": train_coco_evaluator.coco_eval['bbox'].stats[8],# IoU=0.50:0.95\n",
    "                    \"val_mAP\": val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                    \"val_mAR\": val_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                    \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['bbox'].stats[0], \n",
    "                                                    train_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                    \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                            val_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                }\n",
    "\n",
    "                # Append checkpoint to checkpoints list\n",
    "                checkpoints.append(checkpoint)\n",
    "\n",
    "                # Report training and validation scalars to tensorboard\n",
    "                writer.add_scalar('Loss/Train', np.array(float(checkpoint[\"train_loss\"])), start_epoch) # use tags to group scalars\n",
    "                writer.add_scalar('Loss/Val', np.array(float(checkpoint[\"val_loss\"])), start_epoch)\n",
    "                writer.add_scalar('mAP/Train', np.array(float(checkpoint[\"train_mAP\"])), start_epoch)\n",
    "                writer.add_scalar('mAP/Val', np.array(float(checkpoint[\"val_mAP\"])), start_epoch)\n",
    "                writer.add_scalar('mAR/Train', np.array(float(checkpoint[\"train_mAR\"])), start_epoch)\n",
    "                writer.add_scalar('mAR/Val', np.array(float(checkpoint[\"val_mAR\"])), start_epoch)\n",
    "                writer.add_scalar('F1/Train', np.array(float(checkpoint[\"train_f1\"])), start_epoch)\n",
    "                writer.add_scalar('F1/Val', np.array(float(checkpoint[\"val_f1\"])), start_epoch)\n",
    "\n",
    "                # Clear CUDA cache and collect garbage \n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                # Monitor memory usage at the end of the epoch\n",
    "                print(f\"Epoch {start_epoch} - Max memory allocated: {torch.cuda.max_memory_allocated(device)} bytes\")\n",
    "\n",
    "                if val_metric_logger.loss.avg < best_val_loss:\n",
    "                    best_val_loss = val_metric_logger.loss.avg\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                start_epoch += 1\n",
    "\n",
    "        step_index += 1\n",
    "\n",
    "    print('All Training Steps Complete!')\n",
    "\n",
    "    # Close tensorboard writer\n",
    "    writer.close()\n",
    "\n",
    "    return checkpoints\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    checkpoints = main(train_coco_ds, val_coco_ds, best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best train epoch is dictionary in checkpoints with highest validation f1\n",
    "best_train_epoch = max(checkpoints, key = lambda x: x['val_f1'])\n",
    "\n",
    "# initialize model with best trial config\n",
    "model = get_retinanet_model(depth=50,\n",
    "                            num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                            score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                            nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                            detections_per_img=200,\n",
    "                            fg_iou_thresh=0.4,\n",
    "                            bg_iou_thresh=0.2,\n",
    "                            topk_candidates=400, \n",
    "                            alpha=best_trial.config[\"alpha\"], \n",
    "                            gamma_loss=best_trial.config[\"gamma_loss\"], \n",
    "                            trainable_backbone_layers=4, \n",
    "                            dropout_prob=best_trial.config[\"dropout\"],\n",
    "                            class_weights=None)\n",
    "\n",
    "# load model weights from best config's best_train_epoch\n",
    "model.load_state_dict(best_train_epoch[\"model_state_dict\"])\n",
    "\n",
    "# save model weights to .pth file\n",
    "torch.save(model.state_dict(), 'RetinaNet_ResNet50_FPN_DuckNet_' + str(datetime.now().strftime(\"%m%d%Y\")) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy checkpoints and remove model and optimizer state dicts\n",
    "checkpoints_copy = checkpoints.copy()\n",
    "for c in checkpoints_copy:\n",
    "    del c[\"model_state_dict\"]\n",
    "    del c[\"optimizer_state_dict\"]\n",
    "\n",
    "# save checkpoints list to text file\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/tuned_model_checkpoints.txt', 'w') as f:\n",
    "    for item in checkpoints_copy:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Model Inference on Test Dataset**</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of test indices and image names\n",
    "test_dict = dict(zip(test_indices, test_images))\n",
    "\n",
    "# save test_dict to text file just to be safe\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/test_dict.txt', 'w') as f:\n",
    "    for key, value in test_dict.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/', \n",
    "                                transforms = get_transform(train = False))\n",
    "\n",
    "# subset test dataset using test_indices\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size = 1, shuffle = False,\n",
    "                                               collate_fn = utils.collate_fn, num_workers = 0,\n",
    "                                               pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance = evaluate(model, data_loader_test, test_coco_ds, device=torch.device('cpu'), train_data_loader=None, train_coco_ds=None)\n",
    "print(f'Best trial test set mAP: {test_performance.coco_eval[\"bbox\"].stats[0]}') \n",
    "print(f'Best trial test set mAR: {test_performance.coco_eval[\"bbox\"].stats[8]}')\n",
    "print(f'Best trial test set f1 score: {calculate_f1_score(test_performance.coco_eval[\"bbox\"].stats[0], test_performance.coco_eval[\"bbox\"].stats[8])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate performance metrics on every image in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "metric = MeanAveragePrecision(iou_type=\"bbox\",\n",
    "                              class_metrics=True,\n",
    "                              max_detection_thresholds=[1, 10, 100, 150]\n",
    "                              )\n",
    "\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "for images, targets in data_loader_test:\n",
    "    # use image_id to get image_name from image_names list\n",
    "    image_id = [target['image_id'].item() for target in targets]\n",
    "\n",
    "    # store targets as tensors\n",
    "    targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # filter targets to only include boxes and labels keys\n",
    "    ground_truth = [{k: v for k, v in t.items() if k in ('boxes', 'labels')} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images, targets)\n",
    "\n",
    "    # calculate mAP and mAR from test dataset\n",
    "    metric.update(prediction, ground_truth)\n",
    "    mean_AP = metric.compute()\n",
    "\n",
    "    # append image name to mean_AP\n",
    "    mean_AP['image_name'] = test_dict[image_id[0]]\n",
    "\n",
    "    # Append mean_AP and predictions to results list. \n",
    "    results.append(mean_AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store per-image test dataset metrics as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to create a dataframe of image names and mAP values\n",
    "img_results_df = pd.DataFrame()\n",
    "img_results_df['image_name'] = [result['image_name'] for result in results]\n",
    "img_results_df['mAP'] = [result['map'].item() for result in results]\n",
    "img_results_df['mAP_50'] = [result['map_50'].item() for result in results]\n",
    "img_results_df['mAP_75'] = [result['map_75'].item() for result in results]\n",
    "img_results_df['mAP_small'] = [result['map_small'].item() for result in results]\n",
    "img_results_df['mAP_medium'] = [result['map_medium'].item() for result in results]\n",
    "img_results_df['mAP_large'] = [result['map_large'].item() for result in results]\n",
    "img_results_df['mAR_1'] = [result['mar_1'].item() for result in results]\n",
    "img_results_df['mAR_10'] = [result['mar_10'].item() for result in results]\n",
    "img_results_df['mAR_100'] = [result['mar_100'].item() for result in results]\n",
    "img_results_df['mAR_small'] = [result['mar_small'].item() for result in results]\n",
    "img_results_df['mAR_medium'] = [result['mar_medium'].item() for result in results]\n",
    "img_results_df['mAR_large'] = [result['mar_large'].item() for result in results]\n",
    "\n",
    "# # if value is == -1.0, replace with NaN\n",
    "img_results_df = img_results_df.replace(-1.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric values are running averages in torch metrics, so the last value is the final value.\n",
    "final_metrics = img_results_df.iloc[-1]\n",
    "final_metrics = final_metrics.drop('image_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print per-image metrics for test dataset as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "# create a pretty table object\n",
    "x = PrettyTable()\n",
    "\n",
    "cols = ['Metric', 'Value']  \n",
    "\n",
    "# add column headers\n",
    "x.field_names = cols\n",
    "\n",
    "# values for column one in table are column names from final_metrics, column two are the column values. \n",
    "for i in range(len(final_metrics)):\n",
    "    x.add_row([final_metrics.index[i], f'{final_metrics[i]*100:.2f}%'])\n",
    "\n",
    "# print table\n",
    "print(x)\n",
    "\n",
    "# save table as txt file\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/testDataset_image_summary_table.txt', 'w') as f:\n",
    "    print(x, file = f)\n",
    "\n",
    "# save results_df to csv\n",
    "img_results_df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/per_image_results_test_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store per-class test dataset metrics as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_res_df = pd.DataFrame()\n",
    "\n",
    "# store 'map_per_class' and 'mar_100_per_class' from results in df\n",
    "class_res_df['image_name'] = [result['image_name'] for result in results]\n",
    "class_res_df['classes'] = [result['classes'] for result in results]\n",
    "class_res_df['map_per_class'] = [result['map_per_class'] for result in results]\n",
    "class_res_df['mar_100_per_class'] = [result['mar_100_per_class'] for result in results]\n",
    "\n",
    "# convert tensors to numpy arrays\n",
    "class_res_df['classes'] = class_res_df['classes'].apply(lambda x: x.numpy())\n",
    "class_res_df['map_per_class'] = class_res_df['map_per_class'].apply(lambda x: x.numpy())\n",
    "class_res_df['mar_100_per_class'] = class_res_df['mar_100_per_class'].apply(lambda x: x.numpy())\n",
    "\n",
    "# replace integer labels in classes column with labels using label_dict\n",
    "class_res_df['classes'] = class_res_df['classes'].apply(lambda x: [label_dict.get(i) for i in x])\n",
    "\n",
    "# replace -1.0 values in map_per_class and mar_100_per_class with NaN\n",
    "class_res_df['map_per_class'] = class_res_df['map_per_class'].apply(lambda x: np.where(x == -1.0, np.nan, x))\n",
    "class_res_df['mar_100_per_class'] = class_res_df['mar_100_per_class'].apply(lambda x: np.where(x == -1.0, np.nan, x))\n",
    "\n",
    "# if map_per_class or mar_100_per_class is NaN, delete value from list. Also delete corresponding class label.\n",
    "class_res_df['classes'] = class_res_df.apply(lambda x: [i for i, j in zip(x['classes'], x['map_per_class']) if not np.isnan(j)], axis = 1)\n",
    "class_res_df['map_per_class'] = class_res_df['map_per_class'].apply(lambda x: [i for i in x if not np.isnan(i)])\n",
    "class_res_df['mar_100_per_class'] = class_res_df['mar_100_per_class'].apply(lambda x: [i for i in x if not np.isnan(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric values are running averages in TorchMetrics. Store map and mar from last image in dataset\n",
    "classes = class_res_df['classes'].iloc[-1]\n",
    "class_map = class_res_df['map_per_class'].iloc[-1]\n",
    "class_mar_100 = class_res_df['mar_100_per_class'].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print per-class metrics for every image in test dataset as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = 'value' and all unique classes\n",
    "cols = ['Class', 'mAP', 'mAR_100']\n",
    "\n",
    "# create a pretty table object\n",
    "x = PrettyTable()\n",
    "\n",
    "# add column headers\n",
    "x.field_names = cols\n",
    "\n",
    "# classes go in first column, class_map in second column, and class_mar_100 in third column\n",
    "for i in range(len(classes)):\n",
    "    x.add_row([classes[i], f'{class_map[i]*100:.2f}%', f'{class_mar_100[i]*100:.2f}%'])\n",
    "\n",
    "# print table\n",
    "print(x)\n",
    "\n",
    "# save table as txt file\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/testDataset_class_summary_table.txt', 'w') as f:\n",
    "    print(x, file = f)\n",
    "\n",
    "# save results_df to csv\n",
    "class_res_df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/per_class_results_test_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load test data into one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entire test dataset into one batch\n",
    "data_loader_test_singleBatch = torch.utils.data.DataLoader(dataset_test, batch_size = len(dataset_test), shuffle = False,\n",
    "                                                collate_fn = utils.collate_fn, num_workers = 0)\n",
    "\n",
    "# run predictions on all images in the test dataset\n",
    "images, targets = next(iter(data_loader_test_singleBatch))\n",
    "\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "# convert boxes in targets to tensors\n",
    "targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(images, targets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-process model predictions for plotting on original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image in the batch, remove all predicted boxes with scores below 0.5\n",
    "for i in range(len(predictions)):\n",
    "    predictions[i]['boxes'] = predictions[i]['boxes'][predictions[i]['scores'] > 0.5]\n",
    "    predictions[i]['labels'] = predictions[i]['labels'][predictions[i]['scores'] > 0.5]\n",
    "    predictions[i]['scores'] = predictions[i]['scores'][predictions[i]['scores'] > 0.5]\n",
    "\n",
    "# resize boxes to original image shape\n",
    "for i in range(len(images)):\n",
    "    tran_w, tran_h = images[i].shape[1], images[i].shape[2]\n",
    "    \n",
    "    images[i] = Image.open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/' + test_images[i])\n",
    "\n",
    "    orig_w, orig_h = images[i].size\n",
    "\n",
    "    predictions[i]['boxes'] = predictions[i]['boxes'] * torch.tensor([orig_w/tran_w, \n",
    "                                                                      orig_h/tran_h, \n",
    "                                                                      orig_w/tran_w,\n",
    "                                                                      orig_h/tran_h]).view(1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Plot Model Predictions for Images in Test Dataset**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bbox_predicted(ax, boxes, labels, scores): # modify plot_bbox to add confidence scores\n",
    "    # add box to the image and use label_color_map to color-code by bounding box class if exists else 'black'\n",
    "    ax.add_patch(plt.Rectangle((boxes[:, 0], boxes[:, 1]), boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1],\n",
    "                    fill = False,\n",
    "                    color = label_color_map[labels.item()] if labels.item() in label_color_map else 'black', \n",
    "                    linewidth = 1.5))\n",
    "    \n",
    "    # add label and score to the bounding box. concatenate label and score to one string. \n",
    "    # use label_dict to replace class numbers with class names\n",
    "    ax.text(boxes[:, 0], boxes[:, 1] - 100,\n",
    "        s = f\"{label_dict[labels.item()]} {scores.item():.2f}\",\n",
    "        color = 'black',\n",
    "        fontsize = 6,\n",
    "        verticalalignment = 'top',\n",
    "        bbox = {'color': label_color_map[labels.item()] if labels.item() in label_color_map else 'black', 'pad': 0})\n",
    "    return ax\n",
    "\n",
    "\n",
    "# function for plotting all predictions on images\n",
    "def plot_predictions(image, boxes, labels, scores, ax = None):\n",
    "    ax = img_show(image, ax = ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = get_box(boxes[i])\n",
    "        plot_bbox_predicted(ax, box, labels[i], scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 32 samples from batch in a grid of subplots.\n",
    "plt.figure(figsize = (24, 36))\n",
    "for i in range(0, 32):\n",
    "    ax = plt.subplot(8, 4, 1 + i)\n",
    "    plot_predictions(images[i], predictions[i]['boxes'], predictions[i]['labels'], predictions[i]['scores'], ax = ax)\n",
    "    plt.axis('off')\n",
    "    plt.title(test_images[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run inference on full dataset to get model estimates of abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/', \n",
    "                                transforms = get_transform(train = False))\n",
    "\n",
    "data_loader_all = torch.utils.data.DataLoader(dataset_all, batch_size = 1, shuffle = False,\n",
    "                                            collate_fn = utils.collate_fn, num_workers = 0,\n",
    "                                            pin_memory = True)\n",
    "\n",
    "# get model predictions for every image in data_loader_all\n",
    "model_predictions_all = []\n",
    "\n",
    "for images, targets in data_loader_all:\n",
    "    # use image_id to get image_name from image_names list\n",
    "    image_id = [target['image_id'].item() for target in targets]\n",
    "\n",
    "    # convert boxes in targets to tensors\n",
    "    targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images, targets)\n",
    "\n",
    "    # append image name to prediction\n",
    "    prediction['image_name'] = test_dict[image_id[0]]\n",
    "\n",
    "    # Append mean_AP and predictions to results list. \n",
    "    model_predictions_all.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model_predictions_all to a dataframe\n",
    "model_predictions_df = pd.DataFrame(model_predictions_all)\n",
    "\n",
    "# save csv for comparison with ground truth\n",
    "model_predictions_df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/model_predictions_full_dataset.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bohb_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
