{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Reading and Cleaning Annotation Data for Custom PyTorch Object Detection**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.cache_size = 0  # disable cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # restrict cuda to gpu 0\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion(); # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load annotation data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading JSON as dictionary\n",
    "def read_json(filename: str) -> dict:\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Reading {filename} file encountered an error: {e}\")\n",
    "    return data\n",
    "\n",
    "# Function to create a DataFrame from a list of records\n",
    "def create_dataframe(data: list) -> pd.DataFrame:\n",
    "    # Normalize the column levels and create a DataFrame\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Main function to iterate over files in directory and add to df\n",
    "def main():\n",
    "    # Assign directory and empty list for collecting records\n",
    "    directory = \"C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Annotations/\"  # annotation directory\n",
    "    records = []\n",
    "    \n",
    "    # Iterate over files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            # Read the JSON file as python dictionary \n",
    "            data = read_json(filename=f)\n",
    "        \n",
    "            # Create the dataframe for the array items in annotations key \n",
    "            df = create_dataframe(data=data['annotations'])\n",
    "            df.insert(loc=0, column='img_name', value=f'{f[-30:-5]}.JPG')\n",
    "        \n",
    "            df.rename(columns={\n",
    "                \"img_name\": \"img_name\",\n",
    "                \"name\": \"label\",\n",
    "                \"bounding_box.h\": \"bbox_height\",\n",
    "                \"bounding_box.w\": \"bbox_width\",\n",
    "                \"bounding_box.x\": \"bbox_x_topLeft\",\n",
    "                \"bounding_box.y\": \"bbox_y_topLeft\",\n",
    "                \"polygon.paths\": \"polygon_path\"\n",
    "            }, inplace=True)\n",
    "            \n",
    "            # Append the records to the list\n",
    "            records.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping non-file: {filename}\")\n",
    "\n",
    "    # Concatenate all records into a single DataFrame\n",
    "    annos_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "    # Convert x, y, h, w to xmin, ymin, xmax, ymax\n",
    "    annos_df['xmin'] = annos_df['bbox_x_topLeft']\n",
    "    annos_df['ymin'] = annos_df['bbox_y_topLeft']\n",
    "    annos_df['xmax'] = annos_df['bbox_x_topLeft'] + annos_df['bbox_width']\n",
    "    annos_df['ymax'] = annos_df['bbox_y_topLeft'] + annos_df['bbox_height']\n",
    "  \n",
    "    # Drop unnecessary columns \n",
    "    annos_df = annos_df.drop(columns=['bbox_height', 'bbox_width', 'bbox_x_topLeft', \n",
    "                                      'bbox_y_topLeft', 'id', 'slot_names', 'polygon_path'])\n",
    "        \n",
    "    return annos_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-process annotation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique image names\n",
    "unique_img_names = df['img_name'].unique()\n",
    "\n",
    "invalid_img_names = []\n",
    "for img_name in unique_img_names:\n",
    "    img_path = f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Images/{img_name}'\n",
    "    img = Image.open(img_path)\n",
    "    if img.size == (5184, 3888):\n",
    "        invalid_img_names.append(img_name)\n",
    "\n",
    "# load curated images list from file\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/curated_images.txt', 'r') as f:\n",
    "    curated_images = f.read().splitlines()\n",
    "\n",
    "# remove any invalid images from curated images list\n",
    "curated_images = [img_name for img_name in curated_images if img_name not in invalid_img_names]\n",
    "\n",
    "# filter df to include only curated images\n",
    "df = df[df['img_name'].isin(curated_images)]\n",
    "\n",
    "img_classes_to_remove = ['WTDE', 'TURT', 'NUTR', 'ANHI', 'CAGO', \n",
    "                         'DCCO', 'GWFG', 'GBHE', 'COGA', 'PBGR'] # remove images with these classes\n",
    "\n",
    "for class_label in img_classes_to_remove:\n",
    "    # Get all image names with the class\n",
    "    images_with_class = df[df['label'] == class_label]['img_name'].unique()\n",
    "\n",
    "    # Remove all rows for img\n",
    "    df = df[~df['img_name'].isin(images_with_class)]\n",
    "\n",
    "# remove images containing only hens\n",
    "hen_images_no_other_class = df[(df['label'] == 'Hen') & (~df['img_name'].isin(df[df['label'] != 'Hen']['img_name']))]['img_name'].unique()\n",
    "df = df[~df['img_name'].isin(hen_images_no_other_class)]\n",
    "\n",
    "# Separate classes with less than 100 instances\n",
    "class_counts = df['label'].value_counts()\n",
    "other_classes = class_counts[class_counts < 100].index.tolist()\n",
    "positive_classes = class_counts[class_counts >= 100].index.tolist()\n",
    "\n",
    "# print class counts for each label\n",
    "print(\"Number of instances per class in cleaned dataset:\")\n",
    "for label in df['label'].unique():\n",
    "    print(f'{label}: {len(df[df[\"label\"] == label])}')\n",
    "\n",
    "# print other and positive classes\n",
    "print()\n",
    "print(f'Other classes: {other_classes}')\n",
    "print(f'Positive classes: {positive_classes}')\n",
    "\n",
    "# remove images with other classes\n",
    "for class_label in other_classes:\n",
    "    # Get all image names with the class\n",
    "    images_with_class = df[df['label'] == class_label]['img_name'].unique()\n",
    "\n",
    "    # Remove all rows for img\n",
    "    df = df[~df['img_name'].isin(images_with_class)]\n",
    "\n",
    "# encode labels as int (reserve 0 for 'background')\n",
    "df['target'] = pd.Categorical(df['label']).codes + 1\n",
    "\n",
    "# filter out images with invalid bounding boxes\n",
    "df = df.groupby('img_name').filter(lambda x: ((x['xmin'] < x['xmax']) & (x['ymin'] < x['ymax'])).all())\n",
    "\n",
    "# Create a dictionary using df['label'] as the keys and df['target'] as the values\n",
    "label_dict = dict(zip(df['target'], df['label']))\n",
    "\n",
    "# Drop the original 'label' column from df\n",
    "df = df.drop(['label'], axis=1)\n",
    "\n",
    "# Rename 'target' column to 'label'\n",
    "df.rename(columns={'target': 'label'}, inplace=True)\n",
    "\n",
    "# Save df as csv in directory\n",
    "df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter images after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store unique img_names in filtered df as array\n",
    "img_names = df['img_name'].unique().tolist()\n",
    "\n",
    "# Create a new directory called 'filtered_images'\n",
    "filtered_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images'\n",
    "if not os.path.exists(filtered_dir):\n",
    "    os.makedirs(filtered_dir)\n",
    "else:\n",
    "    for file in os.listdir(filtered_dir):\n",
    "        os.remove(os.path.join(filtered_dir, file))\n",
    "\n",
    "# Copy images in img_names to new directory\n",
    "for img in img_names:\n",
    "    shutil.copy2(f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Images/{img}', filtered_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Transform and Augment Image and Annotation Data for Custom PyTorch Object Detection**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "import torchvision.transforms.v2 as T\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAVdroneDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset Loader for Waterfowl Drone Imagery\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transforms):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            root_dir (string): Directory containing all images.\n",
    "            transforms (callable): Transformation to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.unique_image_names = self.df['img_name'].unique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_name = self.unique_image_names[idx]\n",
    "\n",
    "        # Isolate first row to prevent multiple instances of the same image\n",
    "        row = self.df[self.df['img_name'] == image_name].iloc[0]\n",
    "\n",
    "        image_path = os.path.join(self.root_dir, row['img_name'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # Convert to Tensor\n",
    "\n",
    "        # Bounding boxes and labels\n",
    "        boxes = self.df[self.df['img_name'] == image_name][['xmin', 'ymin', 'xmax', 'ymax']].values \n",
    "        labels = self.df[self.df['img_name'] == image_name]['label'].values\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)  # (n_objects)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Calculate area\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # Assume no crowd annotations\n",
    "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': tv_tensors.BoundingBoxes(boxes, format=tv_tensors.BoundingBoxFormat.XYXY, canvas_size=(image.shape[1], image.shape[2])),\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train: bool):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train (bool): Whether the transform is for training or validation/testing.\n",
    "    \"\"\"\n",
    "    transforms_list = [T.ToImage()]\n",
    "    \n",
    "    if train:\n",
    "        transforms_list.extend([\n",
    "        T.RandomIoUCrop(min_scale=0.75, max_scale=1.5), # zoom in < 1, zoom out > 1\n",
    "        T.RandomApply([T.ColorJitter(brightness=0.15, contrast=0.2, saturation=0.15, hue=0.01)], p=0.3),\n",
    "        T.RandomApply([T.GaussianBlur(kernel_size=3, sigma = (0.5, 1.0))], p=0.3),\n",
    "        T.RandomAdjustSharpness(sharpness_factor=1.25, p=0.3),\n",
    "        T.RandomHorizontalFlip(0.5),\n",
    "        T.ClampBoundingBoxes(), # Clamp bounding boxes to image boundaries\n",
    "        T.SanitizeBoundingBoxes(min_size=1, min_area=1) # Sanitize bounding boxes\n",
    "    ])\n",
    "    \n",
    "    transforms_list.extend([\n",
    "        T.Resize(\n",
    "            size=(810,),\n",
    "            max_size=1440,\n",
    "            interpolation=torchvision.transforms.InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        T.ToDtype(\n",
    "            dtype=torch.float32,\n",
    "            scale=True\n",
    "        ),\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    return T.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions for plotting image and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes are values in label_dict\n",
    "classes = list(label_dict.values())\n",
    "\n",
    "# reverse label dictionary for mapping predictions to classes\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "# distinct colors \n",
    "bbox_colors = [\n",
    "    \"#FF0000\",  # Red\n",
    "    \"#00FF00\",  # Green\n",
    "    \"#FFFF00\",  # Yellow\n",
    "    \"#FF00FF\",  # Magenta\n",
    "    \"#00FFFF\",  # Cyan\n",
    "    \"#FFC0CB\",  # Pink\n",
    "    \"#FFA500\",  # Orange\n",
    "    \"#800080\",  # Purple\n",
    "    \"#FFFFFF\",  # White\n",
    "    \"#FFD700\",  # Gold\n",
    "]\n",
    "\n",
    "# label color map for plotting color-coded boxes by class\n",
    "label_color_map = {k: bbox_colors[i] for i, k in enumerate(label_dict.keys())}\n",
    "\n",
    "# function for reshaping boxes \n",
    "def get_box(boxes):\n",
    "    boxes = np.array(boxes)\n",
    "    boxes = boxes.astype('float').reshape(-1, 4)\n",
    "    if boxes.shape[0] == 1 : return boxes\n",
    "    return np.squeeze(boxes)\n",
    "\n",
    "\n",
    "# function for plotting image\n",
    "def img_show(image, ax = None, figsize = (6, 9)):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.imshow(image)\n",
    "    return ax\n",
    " \n",
    "\n",
    "def plot_bbox(ax, boxes, labels):\n",
    "    # add box to the image and use label_color_map to color-code by bounding box class if exists else 'black'\n",
    "    ax.add_patch(plt.Rectangle((boxes[:, 0], boxes[:, 1]), boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1],\n",
    "                    fill = False,\n",
    "                    color = label_color_map[labels.item()] if labels.item() in label_color_map else 'black', \n",
    "                    linewidth = 1.25))\n",
    "    # add label text to bounding box using label_dict if label exists else labels\n",
    "    ax.text(boxes[:, 2], boxes[:, 3], \n",
    "            (label_dict[labels.item()] if labels.item() in label_dict else labels.item()),\n",
    "            fontsize = 8,\n",
    "            bbox = dict(facecolor = 'white', alpha = 0.8, pad = 0, edgecolor = 'none'),\n",
    "            color = 'black')\n",
    "\n",
    "\n",
    "# function for plotting all boxes and labels on the image using get_polygon, img_show, and plot_mask functions\n",
    "def plot_detections(image, boxes, labels, ax = None):\n",
    "    ax = img_show(image.permute(1, 2, 0), ax = ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = get_box(boxes[i])\n",
    "        plot_bbox(ax, box, labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot sample batch to confirm data loads and transforms correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample batch of data to custom PyTorch Dataset and Transform\n",
    "sample_dataset = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv', \n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images', \n",
    "                                transforms = get_transform(train = True))\n",
    "\n",
    "sample_data_loader = torch.utils.data.DataLoader(sample_dataset, batch_size = 8, shuffle=True, \n",
    "                                                collate_fn = utils.collate_fn, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store images and annotation targets from sample batch\n",
    "batch = next(iter(sample_data_loader))\n",
    "images, targets = batch\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "images = [np.clip(image, 0, 1) for image in images]\n",
    "\n",
    "# Plot all samples from batch in a grid of subplots\n",
    "plt.figure(figsize=(16, int(sample_data_loader.batch_size) * 5))\n",
    "for i in range(int(sample_data_loader.batch_size)):\n",
    "    ax = plt.subplot(int(sample_data_loader.batch_size), 2, 1 + i)\n",
    "    plot_detections(images[i], targets[i]['boxes'], targets[i]['labels'], ax=ax)\n",
    "    image_id = targets[i]['image_id'].item()  # Convert tensor to integer\n",
    "    image_name = sample_dataset.unique_image_names[image_id]\n",
    "    plt.title(image_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use stratified sampling to split multi-label dataset into train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set random number generator for reproducible data splits\n",
    "rng = np.random.default_rng(np.random.MT19937(np.random.SeedSequence(6666)))\n",
    "\n",
    "# Group annotations by image\n",
    "image_groups = df.groupby('img_name')\n",
    "\n",
    "# Create a dictionary to store the class distribution for each image\n",
    "image_class_distribution = {}\n",
    "\n",
    "# Populate the dictionary with class distributions\n",
    "for image_name, group in image_groups:\n",
    "    labels = group['label'].tolist()\n",
    "    image_class_distribution[image_name] = labels\n",
    "\n",
    "# Create a list of all image names and their corresponding labels\n",
    "all_images = list(image_class_distribution.keys())\n",
    "all_labels = [image_class_distribution[image] for image in all_images]\n",
    "\n",
    "# Use the most frequent label for each image for stratification\n",
    "representative_labels = [max(set(labels), key=labels.count) for labels in all_labels]\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.05\n",
    "\n",
    "# Perform stratified split using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=int(1/test_ratio), shuffle=True, random_state=6666)\n",
    "\n",
    "train_val_indices, test_indices = next(skf.split(all_images, representative_labels))\n",
    "\n",
    "# Further split train+val into train and validation sets\n",
    "train_val_images = [all_images[idx] for idx in train_val_indices]\n",
    "train_val_labels = [representative_labels[idx] for idx in train_val_indices]\n",
    "\n",
    "skf_val = StratifiedKFold(n_splits=int(1/(val_ratio/(train_ratio + val_ratio))), shuffle=True, random_state=6666)\n",
    "train_indices, val_indices = next(skf_val.split(train_val_images, train_val_labels))\n",
    "\n",
    "# Map image names to unique indices\n",
    "image_to_unique_index = {image: idx for idx, image in enumerate(df['img_name'].unique())}\n",
    "\n",
    "# Create lists of unique indices for each split\n",
    "train_indices = [image_to_unique_index[train_val_images[idx]] for idx in train_indices]\n",
    "val_indices = [image_to_unique_index[train_val_images[idx]] for idx in val_indices]\n",
    "test_indices = [image_to_unique_index[all_images[idx]] for idx in test_indices]\n",
    "\n",
    "# Function to get class distribution\n",
    "def get_class_distribution(images, image_class_distribution):\n",
    "    class_counts = defaultdict(int)\n",
    "    for image in images:\n",
    "        for label in image_class_distribution[image]:\n",
    "            class_counts[label] += 1\n",
    "    return class_counts\n",
    "\n",
    "# Get train, val, and test images\n",
    "train_images = [all_images[idx] for idx in train_indices]\n",
    "val_images = [all_images[idx] for idx in val_indices]\n",
    "test_images = [all_images[idx] for idx in test_indices]\n",
    "\n",
    "train_class_distribution = get_class_distribution(train_images, image_class_distribution)\n",
    "val_class_distribution = get_class_distribution(val_images, image_class_distribution)\n",
    "test_class_distribution = get_class_distribution(test_images, image_class_distribution)\n",
    "\n",
    "class_indices = {label: [] for label in df['label'].unique()}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    class_indices[row['label']].append(idx)\n",
    "\n",
    "train_class_distribution = {k: v / len(class_indices[k]) for k, v in train_class_distribution.items()}\n",
    "val_class_distribution = {k: v / len(class_indices[k]) for k, v in val_class_distribution.items()}\n",
    "test_class_distribution = {k: v / len(class_indices[k]) for k, v in test_class_distribution.items()}\n",
    "\n",
    "print(\"Train class distribution:\", dict(sorted(train_class_distribution.items())))\n",
    "print(\"Validation class distribution:\", dict(sorted(val_class_distribution.items())))\n",
    "print(\"Test class distribution:\", dict(sorted(test_class_distribution.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create weighted random sampler to handle class imbalances during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(labels, hen_label_int, background_label_int):\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Remove the \"Hen\" class from the counts\n",
    "    hen_count = class_counts.pop(hen_label_int, None)\n",
    "    \n",
    "    # Identify the count for the second most-frequent class\n",
    "    second_most_frequent_class_count = max(class_counts.values())\n",
    "    \n",
    "    # Calculate the weight for the \"Hen\" class\n",
    "    hen_weight = second_most_frequent_class_count / hen_count if hen_count else 1.0\n",
    "\n",
    "    # Assign weights to all classes (non-Hen)\n",
    "    class_weights = {label: sum(class_counts.values()) / count for label, count in class_counts.items()}\n",
    "    \n",
    "    # Add weight for the \"Hen\" class and background before normalization\n",
    "    class_weights[hen_label_int] = hen_weight\n",
    "    class_weights[background_label_int] = 0.1  \n",
    "\n",
    "    # Normalize all weights (including background) by dividing by the maximum weight\n",
    "    max_weight = max(class_weights.values())\n",
    "    class_weights = {label: weight / max_weight for label, weight in class_weights.items()}\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Store train labels for each image\n",
    "train_labels = [label for image in train_images for label in image_class_distribution[image]]\n",
    "\n",
    "# Calculate class weights dynamically\n",
    "hen_label_int = [key for key, value in label_dict.items() if value == 'Hen'][0]  # Get the integer label for \"Hen\"\n",
    "background_label_int = 0  # Assuming background is class 0\n",
    "class_weights = calculate_class_weights(train_labels, hen_label_int, background_label_int)\n",
    "\n",
    "# Ensure the background label is included for printing\n",
    "all_labels = sorted(set([background_label_int] + train_labels))\n",
    "\n",
    "# Convert class weights to a list in the correct order (with background as the first element)\n",
    "train_class_weights = [class_weights[label] for label in all_labels]\n",
    "train_class_weights = torch.tensor(train_class_weights, dtype=torch.float32)\n",
    "\n",
    "# Print class counts and weights (including background)\n",
    "print(\"Train class instances and weights: \")\n",
    "# Print background label explicitly\n",
    "print(f\"Background: weight = {class_weights[background_label_int]}\")\n",
    "for label in all_labels:\n",
    "    if label == background_label_int:\n",
    "        continue\n",
    "    print(f\"{label_dict[label]}: count = {train_labels.count(label)}, weight = {class_weights[label]}\")\n",
    "print()\n",
    "\n",
    "# Calculate sample weights for each image in the training dataset\n",
    "train_sample_weights = []\n",
    "for image_name in train_images:\n",
    "    labels = image_class_distribution[image_name]\n",
    "    sample_weight = sum(train_class_weights[all_labels.index(label)] for label in labels) / len(labels)\n",
    "    train_sample_weights.append(sample_weight)\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=train_sample_weights, num_samples=len(train_sample_weights), replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Anchor Sizes and Aspect Ratios of Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# resized_bounding_boxes = []\n",
    "\n",
    "# for images, targets in sample_data_loader:\n",
    "#     for target in targets:\n",
    "#         for box in target['boxes']:\n",
    "#             resized_bounding_boxes.append(box)\n",
    "\n",
    "# # Convert to numpy array\n",
    "# resized_bounding_boxes = np.array(resized_bounding_boxes)\n",
    "\n",
    "# # Print the resized bounding box dimensions\n",
    "# print(resized_bounding_boxes[:5])\n",
    "\n",
    "# # Convert to [width, height] format\n",
    "# widths = resized_bounding_boxes[:, 2] - resized_bounding_boxes[:, 0]\n",
    "# heights = resized_bounding_boxes[:, 3] - resized_bounding_boxes[:, 1]\n",
    "# bounding_boxes_wh = np.stack((widths, heights), axis=1)\n",
    "\n",
    "# # filter out bounding boxes with width or height less than 25\n",
    "# bounding_boxes_wh = bounding_boxes_wh[(bounding_boxes_wh[:, 0] >= 25) & (bounding_boxes_wh[:, 1] >= 25)]\n",
    "\n",
    "# # Perform k-means clustering to find anchor sizes\n",
    "# num_clusters = 5  # Number of anchor sizes\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=6666).fit(bounding_boxes_wh)\n",
    "# anchor_sizes = kmeans.cluster_centers_\n",
    "\n",
    "# # Print the anchor sizes\n",
    "# print(\"Anchor Sizes (width, height):\")\n",
    "# print(anchor_sizes)\n",
    "\n",
    "# # Determine aspect ratios from the anchor sizes\n",
    "# anchor_aspect_ratios = anchor_sizes[:, 0] / anchor_sizes[:, 1]\n",
    "\n",
    "# # Print the aspect ratios\n",
    "# print(\"Anchor Aspect Ratios:\")\n",
    "# print(anchor_aspect_ratios)\n",
    "\n",
    "# sorted_widths = np.sort(bounding_boxes_wh[:, 0])\n",
    "# sorted_heights = np.sort(bounding_boxes_wh[:, 1])\n",
    "\n",
    "# print(\"Smallest Widths:\")\n",
    "# print(sorted_widths[:5])\n",
    "# print(\"Largest Widths:\")\n",
    "# print(sorted_widths[-5:])\n",
    "# print(\"Smallest Heights:\")\n",
    "# print(sorted_heights[:5])\n",
    "# print(\"Largest Heights:\")\n",
    "# print(sorted_heights[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Custom RetinaNet with ResNet FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead, RetinaNetRegressionHead\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "import torchvision.models.detection._utils as det_utils\n",
    "from torchvision.ops import sigmoid_focal_loss, FrozenBatchNorm2d\n",
    "import torchvision.ops.boxes as box_ops\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "from pt_soft_nms import batched_soft_nms\n",
    "\n",
    "\n",
    "def _sum(x: List[torch.Tensor]) -> torch.Tensor:\n",
    "    res = x[0]\n",
    "    for i in x[1:]:\n",
    "        res = res + i\n",
    "    return res\n",
    "\n",
    "\n",
    "class CustomRetinaNetClassificationHead(RetinaNetClassificationHead):\n",
    "    def __init__(self, in_channels, num_anchors, num_classes, alpha=0.25, gamma_loss=2.0, prior_probability=0.01, \n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None, dropout_prob=0.25, class_weights=None, label_smoothing=0.1):\n",
    "        super().__init__(in_channels, num_anchors, num_classes, prior_probability, norm_layer)\n",
    "        self.alpha = alpha\n",
    "        self.gamma_loss = gamma_loss\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.class_weights = class_weights\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def compute_loss(self, targets, head_outputs, matched_idxs):\n",
    "        losses = []\n",
    "\n",
    "        cls_logits = head_outputs[\"cls_logits\"]\n",
    "\n",
    "        for i, (targets_per_image, cls_logits_per_image, matched_idxs_per_image) in enumerate(zip(targets, cls_logits, matched_idxs)):\n",
    "            # determine only the foreground\n",
    "            foreground_idxs_per_image = matched_idxs_per_image >= 0\n",
    "            num_foreground = foreground_idxs_per_image.sum()\n",
    "\n",
    "            # create the target classification\n",
    "            gt_classes_target = torch.zeros_like(cls_logits_per_image)\n",
    "            gt_classes_target += self.label_smoothing / (self.num_classes - 1) # smoothing for negative classes\n",
    "            gt_classes_target[\n",
    "                foreground_idxs_per_image,\n",
    "                targets_per_image[\"labels\"][matched_idxs_per_image[foreground_idxs_per_image]],\n",
    "            ] = 1.0 - self.label_smoothing # smoothing for positive classes\n",
    "\n",
    "            # find indices for which anchors should be ignored\n",
    "            valid_idxs_per_image = matched_idxs_per_image != self.BETWEEN_THRESHOLDS\n",
    "\n",
    "            # get the class weights for the valid indices\n",
    "            if self.class_weights is not None:\n",
    "                valid_labels = targets_per_image[\"labels\"][matched_idxs_per_image[valid_idxs_per_image]]\n",
    "                weights = self.class_weights.to(valid_labels.device)[valid_labels]\n",
    "            else:\n",
    "                weights = torch.ones(cls_logits_per_image[valid_idxs_per_image].shape[0], dtype=torch.float32, device=cls_logits_per_image.device)\n",
    "\n",
    "            # compute the classification loss with custom alpha, gamma_loss, and class weights\n",
    "            losses.append(\n",
    "                (sigmoid_focal_loss(\n",
    "                    cls_logits_per_image[valid_idxs_per_image],\n",
    "                    gt_classes_target[valid_idxs_per_image],\n",
    "                    alpha=self.alpha,\n",
    "                    gamma=self.gamma_loss,\n",
    "                    reduction=\"none\",\n",
    "                ) * weights.unsqueeze(1)).sum() / max(1, num_foreground)\n",
    "            )\n",
    "\n",
    "        return _sum(losses) / len(targets)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_cls_logits = []\n",
    "        for features in x:\n",
    "            cls_logits = self.conv(features)\n",
    "            cls_logits = self.dropout(cls_logits)  # Apply dropout\n",
    "            cls_logits = self.cls_logits(cls_logits)\n",
    "\n",
    "            # Permute classification output from (N, A * K, H, W) to (N, HWA, K).\n",
    "            N, _, H, W = cls_logits.shape\n",
    "            cls_logits = cls_logits.view(N, -1, self.num_classes, H, W)\n",
    "            cls_logits = cls_logits.permute(0, 3, 4, 1, 2)\n",
    "            cls_logits = cls_logits.reshape(N, -1, self.num_classes)  # Size=(N, HWA, K)\n",
    "\n",
    "            all_cls_logits.append(cls_logits)\n",
    "\n",
    "\n",
    "        return torch.cat(all_cls_logits, dim=1)\n",
    "\n",
    "\n",
    "class CustomRetinaNetRegressionHead(RetinaNetRegressionHead):\n",
    "    def __init__(self, in_channels, num_anchors, norm_layer: Optional[Callable[..., nn.Module]] = None, _loss_type=\"smooth_l1\", beta_loss=0.5, lambda_loss=1.0, dropout_prob=0.25):\n",
    "        super().__init__(in_channels, num_anchors, norm_layer)\n",
    "        self._loss_type = _loss_type\n",
    "        self.beta_loss = beta_loss # beta < 1 helps counter early plateauing\n",
    "        self.lambda_loss = lambda_loss # lambda > 1 places more emphasis on localization loss\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "    \n",
    "    def compute_loss(self, targets, head_outputs, anchors, matched_idxs):\n",
    "        # type: (List[Dict[str, torch.Tensor]], Dict[str, torch.Tensor], List[torch.Tensor], List[torch.Tensor]) -> torch.Tensor\n",
    "        losses = []\n",
    "\n",
    "        bbox_regression = head_outputs[\"bbox_regression\"]\n",
    "\n",
    "        for targets_per_image, bbox_regression_per_image, anchors_per_image, matched_idxs_per_image in zip(\n",
    "            targets, bbox_regression, anchors, matched_idxs\n",
    "        ):\n",
    "            # determine only the foreground indices, ignore the rest\n",
    "            foreground_idxs_per_image = torch.where(matched_idxs_per_image >= 0)[0]\n",
    "            num_foreground = foreground_idxs_per_image.numel()\n",
    "\n",
    "            # select only the foreground boxes\n",
    "            matched_gt_boxes_per_image = targets_per_image[\"boxes\"][matched_idxs_per_image[foreground_idxs_per_image]]\n",
    "            bbox_regression_per_image = bbox_regression_per_image[foreground_idxs_per_image, :]\n",
    "            anchors_per_image = anchors_per_image[foreground_idxs_per_image, :]\n",
    "\n",
    "            # compute the loss\n",
    "            losses.append(\n",
    "                    det_utils._box_loss(\n",
    "                    self._loss_type,\n",
    "                    self.box_coder,\n",
    "                    anchors_per_image,\n",
    "                    matched_gt_boxes_per_image,\n",
    "                    bbox_regression_per_image,\n",
    "                    cnf={'beta': self.beta_loss}, \n",
    "                ) * self.lambda_loss / max(1, num_foreground)\n",
    "            )\n",
    "\n",
    "        return _sum(losses) / max(1, len(targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_bbox_regression = []\n",
    "        for features in x:\n",
    "            bbox_regression = self.conv(features)\n",
    "            bbox_regression = self.dropout(bbox_regression)  # Apply dropout\n",
    "            bbox_regression = self.bbox_reg(bbox_regression)\n",
    "\n",
    "            # Permute bbox regression output from (N, 4 * A, H, W) to (N, HWA, 4).\n",
    "            N, _, H, W = bbox_regression.shape\n",
    "            bbox_regression = bbox_regression.view(N, -1, 4, H, W)\n",
    "            bbox_regression = bbox_regression.permute(0, 3, 4, 1, 2)\n",
    "            bbox_regression = bbox_regression.reshape(N, -1, 4)  # Size=(N, HWA, 4)\n",
    "\n",
    "            all_bbox_regression.append(bbox_regression)\n",
    "\n",
    "        return torch.cat(all_bbox_regression, dim=1)\n",
    "\n",
    "class CustomRetinaNet(RetinaNet):\n",
    "    def __init__(self, backbone, num_classes, min_size, max_size, image_mean, image_std, score_thresh, detections_per_img, \n",
    "                 fg_iou_thresh, bg_iou_thresh, topk_candidates, nms_score, nms_sigma):\n",
    "        super().__init__(backbone, num_classes=num_classes, \n",
    "                         min_size=min_size, \n",
    "                         max_size=max_size, \n",
    "                         image_mean=image_mean, \n",
    "                         image_std=image_std, \n",
    "                         score_thresh=score_thresh, \n",
    "                         nms_thresh=None, \n",
    "                         detections_per_img=detections_per_img, \n",
    "                         fg_iou_thresh=fg_iou_thresh, \n",
    "                         bg_iou_thresh=bg_iou_thresh, \n",
    "                         topk_candidates=topk_candidates)\n",
    "        # Store the new NMS parameters.\n",
    "        self.nms_score = nms_score\n",
    "        self.nms_sigma = nms_sigma\n",
    "\n",
    "    def postprocess_detections(self, head_outputs, anchors, image_shapes):\n",
    "        class_logits = head_outputs[\"cls_logits\"]\n",
    "        box_regression = head_outputs[\"bbox_regression\"]\n",
    "        \n",
    "        num_images = len(image_shapes)\n",
    "        detections: List[Dict[str, torch.Tensor]] = []\n",
    "        \n",
    "        for index in range(num_images):\n",
    "            box_regression_per_image = [br[index] for br in box_regression]\n",
    "            logits_per_image = [cl[index] for cl in class_logits]\n",
    "            anchors_per_image, image_shape = anchors[index], image_shapes[index]\n",
    "            \n",
    "            image_boxes = []\n",
    "            image_scores = []\n",
    "            image_labels = []\n",
    "            \n",
    "            # Process each feature level\n",
    "            for box_regression_per_level, logits_per_level, anchors_per_level in zip(\n",
    "                box_regression_per_image, logits_per_image, anchors_per_image\n",
    "            ):\n",
    "                scores_per_level = torch.sigmoid(logits_per_level)  # (N, num_classes)\n",
    "                keep_idxs = scores_per_level > self.score_thresh\n",
    "                \n",
    "                # Class-specific NMS first\n",
    "                for class_idx in range(scores_per_level.shape[-1]):\n",
    "                    class_scores = scores_per_level[:, class_idx]\n",
    "                    class_keep = keep_idxs[:, class_idx]\n",
    "                    \n",
    "                    if class_keep.sum() == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get boxes and scores for this class\n",
    "                    class_boxes = self.box_coder.decode_single(\n",
    "                        box_regression_per_level[class_keep],\n",
    "                        anchors_per_level[class_keep]\n",
    "                    )\n",
    "                    class_boxes = box_ops.clip_boxes_to_image(class_boxes, image_shape)\n",
    "                    class_scores = class_scores[class_keep]\n",
    "                    \n",
    "                    # Apply class-specific soft-NMS\n",
    "                    class_keep = batched_soft_nms(\n",
    "                        boxes=class_boxes,\n",
    "                        scores=class_scores,\n",
    "                        idxs=torch.zeros_like(class_scores, dtype=torch.long),  # Same class\n",
    "                        sigma=self.nms_sigma,\n",
    "                        score_threshold=self.nms_score,\n",
    "                    )\n",
    "                    \n",
    "                    # Keep top scoring boxes after class-specific NMS\n",
    "                    class_keep = class_keep[class_scores[class_keep].argsort(descending=True)]\n",
    "                    class_keep = class_keep[:self.detections_per_img]\n",
    "                    \n",
    "                    image_boxes.append(class_boxes[class_keep])\n",
    "                    image_scores.append(class_scores[class_keep])\n",
    "                    image_labels.append(torch.full_like(\n",
    "                        class_scores[class_keep], class_idx, dtype=torch.int64\n",
    "                    ))\n",
    "            \n",
    "            if len(image_boxes) > 0:\n",
    "                image_boxes = torch.cat(image_boxes, dim=0)\n",
    "                image_scores = torch.cat(image_scores, dim=0)\n",
    "                image_labels = torch.cat(image_labels, dim=0)\n",
    "                \n",
    "                # Global soft-NMS across all classes with score-weighted IoU\n",
    "                keep = []\n",
    "                remaining_indices = list(range(len(image_boxes)))\n",
    "                \n",
    "                while remaining_indices:\n",
    "                    # Get highest scoring box\n",
    "                    scores = image_scores[remaining_indices]\n",
    "                    current_idx = remaining_indices[scores.argmax()]\n",
    "                    keep.append(current_idx)\n",
    "                    \n",
    "                    if len(remaining_indices) == 1:\n",
    "                        break\n",
    "                        \n",
    "                    # Remove current index\n",
    "                    remaining_indices.remove(current_idx)\n",
    "                    \n",
    "                    # Calculate IoUs with remaining boxes\n",
    "                    current_box = image_boxes[current_idx:current_idx+1]\n",
    "                    remaining_boxes = image_boxes[remaining_indices]\n",
    "                    ious = box_ops.box_iou(current_box, remaining_boxes)[0]\n",
    "                    \n",
    "                    # Weight IoUs by score differences\n",
    "                    score_diffs = torch.abs(image_scores[current_idx] - image_scores[remaining_indices])\n",
    "                    weighted_ious = ious * torch.exp(-score_diffs / self.nms_sigma)\n",
    "                    \n",
    "                    # Update scores using weighted IoUs\n",
    "                    image_scores[remaining_indices] *= torch.exp(\n",
    "                        -weighted_ious ** 2 / self.nms_sigma\n",
    "                    )\n",
    "                    \n",
    "                    # Remove low scoring boxes\n",
    "                    keep_mask = image_scores[remaining_indices] > self.nms_score\n",
    "                    remaining_indices = [remaining_indices[i] for i in range(len(remaining_indices)) if keep_mask[i]]\n",
    "                \n",
    "                keep = torch.tensor(keep, device=image_boxes.device)\n",
    "                \n",
    "                # Sort by score and limit detections\n",
    "                keep = keep[image_scores[keep].argsort(descending=True)]\n",
    "                keep = keep[:self.detections_per_img]\n",
    "                \n",
    "                detections.append({\n",
    "                    \"boxes\": image_boxes[keep],\n",
    "                    \"scores\": image_scores[keep],\n",
    "                    \"labels\": image_labels[keep],\n",
    "                })\n",
    "            else:\n",
    "                detections.append({\n",
    "                    \"boxes\": torch.empty((0, 4), device=box_regression_per_image[0].device),\n",
    "                    \"scores\": torch.empty((0,), device=box_regression_per_image[0].device),\n",
    "                    \"labels\": torch.empty((0,), device=box_regression_per_image[0].device, dtype=torch.int64),\n",
    "                })\n",
    "        \n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retinanet_model(depth, num_classes=10, min_size=810, max_size=1440, image_mean=[0, 0, 0], image_std=[1, 1, 1], score_thresh=0.1,\n",
    "                        detections_per_img=200, fg_iou_thresh=0.6, bg_iou_thresh=0.5, topk_candidates=200, alpha=0.75, gamma_loss=3.0, \n",
    "                        class_weights=None, beta_loss=0.5, lambda_loss=1.5, dropout_prob=0.25, nms_score=0.25, nms_sigma=0.5):\n",
    "    \n",
    "    trainable_backbone_layers = 0  # set constant, adjust later with function\n",
    "\n",
    "    # Create the backbone with FPN\n",
    "    if depth == 18:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet18', \n",
    "                                       weights=torchvision.models.ResNet18_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers)\n",
    "    elif depth == 34:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet34', \n",
    "                                       weights=torchvision.models.ResNet34_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers)\n",
    "    elif depth == 50:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet50', \n",
    "                                       weights=torchvision.models.ResNet50_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers)\n",
    "    elif depth == 101:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet101', \n",
    "                                       weights=torchvision.models.ResNet101_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers)\n",
    "    elif depth == 152:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet152', \n",
    "                                       weights=torchvision.models.ResNet152_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model depth\")\n",
    "\n",
    "    # Create the RetinaNet model with the custom backbone.\n",
    "    model = CustomRetinaNet(backbone, \n",
    "                            num_classes=num_classes,\n",
    "                            min_size=min_size,  # same size as resize in transform to keep aspect ratio\n",
    "                            max_size=max_size,\n",
    "                            image_mean=image_mean,\n",
    "                            image_std=image_std,\n",
    "                            score_thresh=score_thresh, \n",
    "                            detections_per_img=detections_per_img,\n",
    "                            fg_iou_thresh=fg_iou_thresh,\n",
    "                            bg_iou_thresh=bg_iou_thresh,\n",
    "                            topk_candidates=topk_candidates,\n",
    "                            nms_score=nms_score,\n",
    "                            nms_sigma=nms_sigma\n",
    "                           )\n",
    "\n",
    "    # Replace the classification head with the custom one\n",
    "    in_channels = model.head.classification_head.cls_logits.in_channels\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "    model.head.classification_head = CustomRetinaNetClassificationHead(in_channels, \n",
    "                                                                       num_anchors, \n",
    "                                                                       num_classes, \n",
    "                                                                       alpha=alpha, \n",
    "                                                                       gamma_loss=gamma_loss, \n",
    "                                                                       dropout_prob=dropout_prob,\n",
    "                                                                       class_weights=class_weights)\n",
    "\n",
    "    # Replace the regression head with the custom one\n",
    "    model.head.regression_head = CustomRetinaNetRegressionHead(in_channels, \n",
    "                                                               num_anchors, \n",
    "                                                               _loss_type=\"smooth_l1\",\n",
    "                                                               beta_loss=beta_loss,\n",
    "                                                               lambda_loss=lambda_loss,\n",
    "                                                               dropout_prob=dropout_prob)\n",
    "\n",
    "    model.anchor_generator = AnchorGenerator(sizes=((24, 32, 40), (48, 64, 80), (96, 128, 160), (192, 256, 320), (472, 536, 600)), \n",
    "                                             aspect_ratios=((0.75, 1.15, 1.8), (0.75, 1.15, 1.8), (0.75, 1.15, 1.8), (0.75, 1.15, 1.8), (0.75, 1.15, 1.8)))\n",
    "\n",
    "    return model\n",
    "\n",
    "print(get_retinanet_model(depth=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean up Jupyter environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_images, all_labels, ax, background_label_int, batch, class_counts, class_indices, class_label, class_weights, curated_images, df, file, group, hen_images_no_other_class, hen_label_int, i, idx, image_class_distribution, image_groups, image_id, image_name, images, images_with_class, img, img_classes_to_remove, img_name, img_names, img_path, invalid_img_names, label, labels, other_classes, positive_classes, representative_labels, row, sample_data_loader, sample_dataset, sample_weight, targets, test_class_distribution, test_images, test_ratio, train_class_distribution, train_images, train_labels, train_ratio, train_sample_weights, train_val_images, train_val_indices, train_val_labels, unique_img_names, val_class_distribution, val_images, val_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Tune Model Hyperparameters using Ray Tune**</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class for tuning RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import math\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import ray.cloudpickle as pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter, ValDataLoaderIter\n",
    "from engine_gradientAccumulation import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "\n",
    "# Set random seed for reproducible training\n",
    "def set_seed(seed):\n",
    "    import torch, random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def extract_per_class_metrics(coco_eval, coco_gt):\n",
    "    \"\"\"\n",
    "    Extract per-class metrics at different IoU thresholds.\n",
    "    Returns precision and recall for IoU@[0.5:0.95], IoU@0.5, and IoU@0.75\n",
    "    \"\"\"\n",
    "    per_class_metrics = {}\n",
    "\n",
    "    # Create a list of category IDs in the order they appear in the evaluation results\n",
    "    cat_ids = list(coco_gt.cats.keys())\n",
    "    cat_id_to_index = {cat_id: idx for idx, cat_id in enumerate(cat_ids)}\n",
    "\n",
    "    for cat_id, idx in cat_id_to_index.items():\n",
    "        try:\n",
    "            # Get precision array for different IoU thresholds\n",
    "            precision_all = coco_eval.coco_eval['bbox'].eval['precision']  # [T, R, K, A, M]\n",
    "            recall_all = coco_eval.coco_eval['bbox'].eval['recall']      # [T, K, A, M]\n",
    "            \n",
    "            # Extract metrics for each IoU threshold\n",
    "            # For area range 'all' (0) and max detections 100 (2)\n",
    "            precision_50_95 = precision_all[:, :, idx, 0, 2].mean()  # IoU@[0.5:0.95]\n",
    "            precision_50 = precision_all[0, :, idx, 0, 2].mean()     # IoU@0.5\n",
    "            precision_75 = precision_all[5, :, idx, 0, 2].mean()     # IoU@0.75\n",
    "            \n",
    "            recall_50_95 = recall_all[:, idx, 0, 2].mean()          # IoU@[0.5:0.95]\n",
    "            recall_50 = recall_all[0, idx, 0, 2]                    # IoU@0.5\n",
    "            recall_75 = recall_all[5, idx, 0, 2]                    # IoU@0.75\n",
    "\n",
    "            per_class_metrics[cat_id] = {\n",
    "                'precision_50': precision_50,\n",
    "                'recall_50': recall_50,\n",
    "                'f1_50': calculate_f1_score(precision_50, recall_50),\n",
    "                'precision_75': precision_75,\n",
    "                'recall_75': recall_75,\n",
    "                'f1_75': calculate_f1_score(precision_75, recall_75),\n",
    "                'precision_50_95': precision_50_95,\n",
    "                'recall_50_95': recall_50_95,\n",
    "                'f1_50_95': calculate_f1_score(precision_50_95, recall_50_95),\n",
    "            }\n",
    "\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError for category ID {cat_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return per_class_metrics\n",
    "\n",
    "\n",
    "def adjust_trainable_layers(model, trainable_layers):\n",
    "    \"\"\"\n",
    "    Adjust the trainable layers in the RetinaNet backbone (model.backbone.body).\n",
    "    Unfreeze the last `trainable_layers` residual blocks and replace their FrozenBatchNorm2d layers.\n",
    "    When trainable_layers=5, also unfreeze conv1 and replace bn1 with trainable BatchNorm2d.\n",
    "    \"\"\"\n",
    "    def convert_frozen_bn(frozen_bn):\n",
    "        device = frozen_bn.running_mean.device  # Get the device of the frozen BN layer\n",
    "        num_features = frozen_bn.weight.shape[0]\n",
    "        bn = torch.nn.BatchNorm2d(num_features)\n",
    "        bn = bn.to(device)\n",
    "        # Initialize with existing stats.\n",
    "        bn.running_mean = frozen_bn.running_mean.clone()\n",
    "        bn.running_var = frozen_bn.running_var.clone()\n",
    "        torch.nn.init.normal_(bn.weight, mean=1.0, std=0.02)\n",
    "        torch.nn.init.constant_(bn.bias, 0)\n",
    "        return bn\n",
    "\n",
    "    # Collect backbone blocks.\n",
    "    backbone_layers = []\n",
    "    for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        if hasattr(model.backbone.body, layer_name):\n",
    "            backbone_layers.append(getattr(model.backbone.body, layer_name))\n",
    "    \n",
    "    if trainable_layers > 5:\n",
    "        print(f\"Requested trainable_layers ({trainable_layers}) exceeds available layers (5). Using 5 instead.\")\n",
    "        trainable_layers = 5\n",
    "\n",
    "    # Unfreeze the last `trainable_layers` blocks.\n",
    "    for block in backbone_layers[-trainable_layers:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Replace FrozenBatchNorm2d layers in these blocks.\n",
    "    for name, module in model.backbone.body.named_modules():\n",
    "        if isinstance(module, FrozenBatchNorm2d):\n",
    "            if 'layer' in name:\n",
    "                layer_num = int(name.split('.')[0][-1])\n",
    "                if layer_num > (4 - trainable_layers):\n",
    "                    parent_name = '.'.join(name.split('.')[:-1])\n",
    "                    module_name = name.split('.')[-1]\n",
    "                    parent = dict(model.backbone.body.named_modules())[parent_name]\n",
    "                    setattr(parent, module_name, convert_frozen_bn(module))\n",
    "            elif trainable_layers == 5 and name == 'bn1':\n",
    "                model.backbone.body.bn1 = convert_frozen_bn(module)\n",
    "                model.backbone.body.conv1.weight.requires_grad = True\n",
    "\n",
    "\n",
    "class RetinaNetTuner:\n",
    "    def __init__(self, num_samples, restore_path=\"\"):\n",
    "        self.num_samples = num_samples\n",
    "        self.restore_path = restore_path\n",
    "\n",
    "    def create_coco_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            train_future = executor.submit(get_coco_api_from_dataset, train_dataset)\n",
    "            val_future = executor.submit(get_coco_api_from_dataset, val_dataset)\n",
    "            test_future = executor.submit(get_coco_api_from_dataset, test_dataset)\n",
    "            train_coco_ds = train_future.result()\n",
    "            val_coco_ds = val_future.result()\n",
    "            test_coco_ds = test_future.result()\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds\n",
    "\n",
    "    def train_lr_finder(self, config):\n",
    "        class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "            def inputs_labels_from_batch(self, batch_data):\n",
    "                inputs = [image.to('cuda:0') for image in batch_data[0]]\n",
    "                labels = [{k: v.to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "                return inputs, labels\n",
    "\n",
    "        class CustomValDataLoaderIter(ValDataLoaderIter):\n",
    "            def __iter__(self):\n",
    "                self._iterator = iter(self.data_loader)\n",
    "                self.run_counter = 0\n",
    "                return self\n",
    "\n",
    "            def __next__(self):\n",
    "                try:\n",
    "                    self.run_counter += 1\n",
    "                    return super().__next__()\n",
    "                except StopIteration:\n",
    "                    # Reset if exhausted and then return next batch\n",
    "                    self._iterator = iter(self.data_loader)\n",
    "                    self.run_counter = 0\n",
    "                    return super().__next__()\n",
    "\n",
    "            def inputs_labels_from_batch(self, batch_data):\n",
    "                inputs = [image.to(\"cuda:0\") for image in batch_data[0]]\n",
    "                labels = [{k: v.to(\"cuda:0\") for k, v in t.items()} for t in batch_data[1]]\n",
    "                return inputs, labels\n",
    "\n",
    "        dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "        dataset_train = torch.utils.data.Subset(dataset_train, train_indices)\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8,\n",
    "                                                        sampler=self.config[\"train_sampler\"],\n",
    "                                                        collate_fn=utils.collate_fn,\n",
    "                                                        num_workers=0, pin_memory=True)\n",
    "        train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "\n",
    "        data_loader_val = ray.get(config[\"data_loader_val_ref\"])\n",
    "        val_iter = CustomValDataLoaderIter(data_loader_val)\n",
    "\n",
    "        model = get_retinanet_model(\n",
    "            depth=self.config[\"resnet_depth\"],\n",
    "            num_classes=len(self.config[\"class_weights\"]),\n",
    "            score_thresh=self.config[\"score_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=self.config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=self.config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=200,\n",
    "            alpha=self.config[\"alpha\"],\n",
    "            gamma_loss=self.config[\"gamma_loss\"],\n",
    "            class_weights=None,\n",
    "            beta_loss=self.config[\"beta_loss\"],\n",
    "            lambda_loss=self.config[\"lambda_loss\"],\n",
    "            dropout_prob=self.config[\"dropout\"],\n",
    "            nms_score=self.config[\"nms_score\"],\n",
    "            nms_sigma=self.config[\"nms_sigma\"]\n",
    "        ).to('cuda:0')\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params, lr=1e-7, momentum=self.config[\"momentum\"], weight_decay=self.config[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        grad_scaler = torch.GradScaler()\n",
    "\n",
    "        class CustomLRFinder(LRFinder):\n",
    "            def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "                super().__init__(model, optimizer, criterion, device)\n",
    "                self.amp_backend = amp_backend\n",
    "                self.amp_config = amp_config\n",
    "                self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "            def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                for _ in range(accumulation_steps):\n",
    "                    inputs, labels = next(train_iter)\n",
    "                    inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                    with torch.autocast(device_type=\"cuda:0\"):\n",
    "                        outputs = self.model(inputs, labels)\n",
    "                        loss = sum(loss for loss in outputs.values())\n",
    "                    loss /= accumulation_steps\n",
    "                    self.grad_scaler.scale(loss).backward()\n",
    "                    total_loss += loss\n",
    "                self.grad_scaler.step(self.optimizer)\n",
    "                self.grad_scaler.update()\n",
    "                return total_loss.item()\n",
    "\n",
    "            def _validate(self, val_iter, non_blocking_transfer=True):\n",
    "                self.model.train()   # FORCE training mode here!\n",
    "                inputs, labels = next(val_iter)\n",
    "                inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "                with torch.no_grad(), torch.autocast(device_type=\"cuda:0\"):\n",
    "                    outputs = self.model(inputs, labels)\n",
    "                    loss = sum(loss for loss in outputs.values())\n",
    "                return loss.item()\n",
    "\n",
    "        lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "        lr_finder.range_test(train_iter, val_iter, end_lr=1, num_iter=300, step_mode='exp', accumulation_steps=1)\n",
    "        suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "        lr_finder.reset()\n",
    "\n",
    "        # return default if torch lr finder fails\n",
    "        try:\n",
    "            if isinstance(suggested_lr, tuple):\n",
    "                axes, suggested_lr_value = suggested_lr\n",
    "                return suggested_lr_value\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during learning rate finding: {e}\")\n",
    "            # Return a default learning rate if an error occurs\n",
    "            return 5e-4\n",
    "\n",
    "    def train_MAVdroneDataset(self, config):\n",
    "        import pickle, tempfile\n",
    "        from pathlib import Path\n",
    "        set_seed(6666)\n",
    "\n",
    "        dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "        data_loader_train_eval = ray.get(config[\"data_loader_train_eval_ref\"])\n",
    "        data_loader_val = ray.get(config[\"data_loader_val_ref\"])\n",
    "        train_coco_ds = ray.get(config[\"train_coco_ds_ref\"])\n",
    "        val_coco_ds = ray.get(config[\"val_coco_ds_ref\"])\n",
    "\n",
    "        training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 1, \"trainable_layers\": 0, \"improvement_threshold\": 0.01, \"variance_threshold\": 1e-4}, \n",
    "        # {\"step\": 1, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 2, \"trainable_layers\": 1, \"improvement_threshold\": 0.008, \"variance_threshold\": 5e-5}, \n",
    "        # {\"step\": 2, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 4, \"trainable_layers\": 2, \"improvement_threshold\": 0.005, \"variance_threshold\": 2.5e-5}, \n",
    "        # {\"step\": 3, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 8, \"trainable_layers\": 3, \"improvement_threshold\": 0.003, \"variance_threshold\": 1e-5}, \n",
    "        # {\"step\": 4, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 16, \"trainable_layers\": 4, \"improvement_threshold\": 0.002, \"variance_threshold\": 5e-6}, \n",
    "        # {\"step\": 5, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 32, \"trainable_layers\": 5, \"improvement_threshold\": 0.001, \"variance_threshold\": 2.5e-6}, # bs 256\n",
    "    ]\n",
    "\n",
    "        # Instantiate model and optimizer only once\n",
    "        model = get_retinanet_model(\n",
    "            depth=config[\"resnet_depth\"],\n",
    "            num_classes=len(config[\"class_weights\"]),\n",
    "            score_thresh=config[\"score_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=200,\n",
    "            alpha=config[\"alpha\"],\n",
    "            gamma_loss=config[\"gamma_loss\"],\n",
    "            class_weights=None,\n",
    "            beta_loss=config[\"beta_loss\"],\n",
    "            lambda_loss=config[\"lambda_loss\"],\n",
    "            dropout_prob=config[\"dropout\"],\n",
    "            nms_score=config[\"nms_score\"],\n",
    "            nms_sigma=config[\"nms_sigma\"]\n",
    "        )\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=config[\"lr\"],\n",
    "                                    momentum=config[\"momentum\"],\n",
    "                                    weight_decay=config[\"weight_decay\"],\n",
    "                                    nesterov=True)\n",
    "        \n",
    "        # Check for an existing checkpoint and load state if available.\n",
    "        checkpoint = train.get_checkpoint()\n",
    "        if checkpoint:\n",
    "            with checkpoint.as_directory() as checkpoint_dir:\n",
    "                data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                with open(data_path, \"rb\") as fp:\n",
    "                    checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"] + 1\n",
    "            current_step = checkpoint_state[\"current_step\"] \n",
    "            step_epoch_counter = checkpoint_state[\"step_epoch_counter\"] + 1\n",
    "            # Load model state.\n",
    "            model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "            # Replace the checkpoint's optimizer param_groups with the current optimizer's\n",
    "            optimizer_state = checkpoint_state[\"optimizer_state_dict\"]\n",
    "            current_opt_state = optimizer.state_dict()\n",
    "            optimizer_state[\"param_groups\"] = current_opt_state[\"param_groups\"]\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            current_step = 0\n",
    "            step_epoch_counter = 0\n",
    "\n",
    "        while current_step < len(training_steps):\n",
    "            ts = training_steps[current_step]\n",
    "            batch_size = ts[\"batch_size\"]\n",
    "            print_freq = ts[\"print_freq\"]\n",
    "            accumulation_steps = ts[\"accumulation_steps\"]\n",
    "            backbone_layers = ts[\"trainable_layers\"]\n",
    "            improvement_threshold = ts[\"improvement_threshold\"]\n",
    "            variance_threshold = ts[\"variance_threshold\"]\n",
    "\n",
    "            scaled_lr = config[\"lr\"] * math.sqrt((batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps)\n",
    "\n",
    "            # Adjust the trainable layers if needed.\n",
    "            adjust_trainable_layers(model, backbone_layers)\n",
    "\n",
    "            step_optimizer = optimizer\n",
    "            params_new = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.SGD(\n",
    "                params_new,\n",
    "                lr=scaled_lr,\n",
    "                momentum=step_optimizer.param_groups[0]['momentum'],\n",
    "                weight_decay=step_optimizer.param_groups[0]['weight_decay'],\n",
    "                nesterov=step_optimizer.param_groups[0]['nesterov']\n",
    "            )\n",
    "            old_state = step_optimizer.state_dict()[\"state\"]\n",
    "            for group in optimizer.param_groups:\n",
    "                for p in group[\"params\"]:\n",
    "                    pid = id(p)\n",
    "                    if pid in old_state:\n",
    "                        optimizer.state[p] = old_state[pid]\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=batch_size,\n",
    "                sampler=config[\"train_sampler\"],\n",
    "                collate_fn=utils.collate_fn,\n",
    "                num_workers=0, pin_memory=True\n",
    "            )\n",
    "\n",
    "            print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}')\n",
    "            print()\n",
    "\n",
    "            window_loss = []\n",
    "            window_f1 = []\n",
    "            window_size = 5\n",
    "            minimum_epochs = 15\n",
    "            alpha = 0.1\n",
    "            patience = 3 if ts[\"step\"] < 3 else 5\n",
    "            ema_loss = None\n",
    "            ema_f1 = None\n",
    "            non_improving_counter = 0\n",
    "\n",
    "            while True:\n",
    "                print(f\"Epoch {start_epoch}, Step: {ts['step']}, Memory: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "                print()\n",
    "\n",
    "                train_metric_logger, val_metric_logger = train_one_epoch(\n",
    "                    model, optimizer, data_loader, device, start_epoch,\n",
    "                    print_freq, accumulation_steps, data_loader_val, step_epoch_counter\n",
    "                )\n",
    "                print()\n",
    "\n",
    "                train_coco_evaluator, val_coco_evaluator = evaluate(\n",
    "                    model, data_loader_val, val_coco_ds, device, data_loader_train_eval, train_coco_ds\n",
    "                )\n",
    "                print()\n",
    "\n",
    "                train_class_metrics = extract_per_class_metrics(train_coco_evaluator, train_coco_ds)\n",
    "                val_class_metrics = extract_per_class_metrics(val_coco_evaluator, val_coco_ds)\n",
    "                train_class_metrics = {label_dict[k]: v for k, v in train_class_metrics.items()}\n",
    "                val_class_metrics = {label_dict[k]: v for k, v in val_class_metrics.items()}\n",
    "\n",
    "                print(\"Training Class Metrics:\")\n",
    "                for name, m in train_class_metrics.items():\n",
    "                    print(f\"Class: {name}, AP@0.50: {m['precision_50']:.4f}, AR@0.50: {m['recall_50']:.4f}, F1@0.50: {m['f1_50']:.4f}\")\n",
    "                    print(f\"Class: {name}, AP@0.75: {m['precision_75']:.4f}, AR@0.75: {m['recall_75']:.4f}, F1@0.75: {m['f1_75']:.4f}\")\n",
    "                    print(f\"Class: {name}, AP@[0.50:0.95]: {m['precision_50_95']:.4f}, AR@[0.50:0.95]: {m['recall_50_95']:.4f}, F1@[0.50:0.95]: {m['f1_50_95']:.4f}\")\n",
    "                print(\"\\nValidation Class Metrics:\")\n",
    "                for name, m in val_class_metrics.items():\n",
    "                    print(f\"Class: {name}, AP@0.50: {m['precision_50']:.4f}, AR@0.50: {m['recall_50']:.4f}, F1@0.50: {m['f1_50']:.4f}\")\n",
    "                    print(f\"Class: {name}, AP@0.75: {m['precision_75']:.4f}, AR@0.75: {m['recall_75']:.4f}, F1@0.75: {m['f1_75']:.4f}\")\n",
    "                    print(f\"Class: {name}, AP@[0.50:0.95]: {m['precision_50_95']:.4f}, AR@[0.50:0.95]: {m['recall_50_95']:.4f}, F1@[0.50:0.95]: {m['f1_50_95']:.4f}\")\n",
    "                print()\n",
    "\n",
    "                current_loss = val_metric_logger.loss.avg\n",
    "                window_loss.append(current_loss)\n",
    "                if len(window_loss) > window_size:\n",
    "                    window_loss.pop(0)\n",
    "                current_f1 = calculate_f1_score(val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                val_coco_evaluator.coco_eval['bbox'].stats[8]\n",
    "                )\n",
    "                window_f1.append(current_f1)\n",
    "                if len(window_f1) > window_size:\n",
    "                    window_f1.pop(0)\n",
    "                \n",
    "                checkpoint_data = {\n",
    "                    \"epoch\": start_epoch,\n",
    "                    \"current_step\": current_step,\n",
    "                    \"step_epoch_counter\": step_epoch_counter,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                }\n",
    "\n",
    "                with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                    data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                    with open(data_path, \"wb\") as fp:\n",
    "                        pickle.dump(checkpoint_data, fp)\n",
    "                    train.report(\n",
    "                        {\"epoch\": start_epoch,\n",
    "                        \"current_step\": current_step,\n",
    "                        \"train_loss\": train_metric_logger.loss.avg,\n",
    "                        \"val_loss\": current_loss,\n",
    "                        \"train_mAP\": train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                        \"val_mAP\": val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                        \"train_mAR\": train_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                        \"val_mAR\": val_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                        \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                        train_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                        \"val_f1\": current_f1},\n",
    "                        checkpoint=train.Checkpoint.from_directory(checkpoint_dir),\n",
    "                    )\n",
    "\n",
    "                print(f\"Epoch {start_epoch}: Current Loss = {current_loss:.4f},\", end=\" \")\n",
    "\n",
    "                if step_epoch_counter >= minimum_epochs and len(window_loss) == window_size:\n",
    "                    if ema_loss is None:\n",
    "                        ema_loss = current_loss\n",
    "                        ema_f1 = current_f1\n",
    "                        relative_improvement = 1.0\n",
    "                        relative_f1_improvement = 1.0\n",
    "                    else:\n",
    "                        # Update EMAs\n",
    "                        prev_ema = ema_loss\n",
    "                        prev_f1_ema = ema_f1\n",
    "                        ema_loss = alpha * current_loss + (1 - alpha) * prev_ema\n",
    "                        ema_f1 = alpha * current_f1 + (1 - alpha) * prev_f1_ema\n",
    "                        \n",
    "                        # Calculate improvements\n",
    "                        relative_improvement = (prev_ema - ema_loss) / prev_ema\n",
    "                        relative_f1_improvement = (ema_f1 - prev_f1_ema) / prev_f1_ema\n",
    "                        \n",
    "                        # Check both metrics for improvement\n",
    "                        if (relative_improvement < improvement_threshold and \n",
    "                            relative_f1_improvement < improvement_threshold):\n",
    "                            non_improving_counter += 1\n",
    "                        else:\n",
    "                            non_improving_counter = 0\n",
    "\n",
    "                    loss_variance = np.var(window_loss)\n",
    "                    f1_variance = np.var(window_f1)\n",
    "                    \n",
    "                    # Check both metrics for plateau\n",
    "                    should_break = ((non_improving_counter >= patience) or \n",
    "                                (loss_variance < variance_threshold and f1_variance < variance_threshold))\n",
    "                    \n",
    "                    print(f\"EMA Loss = {ema_loss:.4f}, Loss Improvement = {relative_improvement:.4f},\", end=\" \")\n",
    "                    print(f\"EMA F1 = {ema_f1:.4f}, F1 Improvement = {relative_f1_improvement:.4f},\", end=\" \")\n",
    "                    print(f\"Loss Var = {loss_variance:.6f}, F1 Var = {f1_variance:.6f}, \", end=\"\")\n",
    "                    print(f\"Non-improvement Count = {non_improving_counter}\")\n",
    "                else:\n",
    "                    should_break = False\n",
    "                    print(\"\")\n",
    "\n",
    "                if should_break:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    print(\"Plateau reached; moving to next training step.\\n\")\n",
    "                    break\n",
    "\n",
    "                start_epoch += 1\n",
    "                step_epoch_counter += 1\n",
    "\n",
    "            current_step += 1\n",
    "            step_epoch_counter = 0 # reset to zero for next training step\n",
    "\n",
    "        print('Tuning Trial Complete!')\n",
    "\n",
    "    def trial_dirname_creator(self, trial):\n",
    "        return f\"{trial.trial_id}\"\n",
    "\n",
    "    def run(self):\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "\n",
    "        dataset = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=True)\n",
    "        )\n",
    "\n",
    "        dataset_val = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_test = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_train = torch.utils.data.Subset(dataset, train_indices)\n",
    "        dataset_train_eval = torch.utils.data.Subset(dataset_val, train_indices)\n",
    "        dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "        dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "        data_loader_train_eval = torch.utils.data.DataLoader(\n",
    "            dataset_train_eval, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "            dataset_val, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        data_loader_test = torch.utils.data.DataLoader(\n",
    "            dataset_test, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        train_coco_ds, val_coco_ds, test_coco_ds = self.create_coco_datasets(dataset_train, dataset_val, dataset_test)\n",
    "\n",
    "        dataset_train_ref = ray.put(dataset_train)\n",
    "        data_loader_train_eval_ref = ray.put(data_loader_train_eval)\n",
    "        data_loader_val_ref = ray.put(data_loader_val)\n",
    "        data_loader_test_ref = ray.put(data_loader_test)\n",
    "        train_coco_ds_ref = ray.put(train_coco_ds)\n",
    "        val_coco_ds_ref = ray.put(val_coco_ds)\n",
    "        test_coco_ds_ref = ray.put(test_coco_ds)\n",
    "\n",
    "        config = {\n",
    "            # \"lr\": tune.sample_from(lambda config: self.train_lr_finder(config)),\n",
    "            \"lr\": tune.loguniform(0.00001, 0.01),\n",
    "            \"resnet_depth\": 50,\n",
    "            \"momentum\": tune.uniform(0.8, 0.99),\n",
    "            \"weight_decay\": tune.loguniform(0.00001, 0.01),\n",
    "            \"alpha\": tune.uniform(0.1, 0.9),\n",
    "            \"gamma_loss\": tune.uniform(1, 4),\n",
    "            \"dropout\": tune.uniform(0.1, 0.5),\n",
    "            \"score_thresh\": 0.6,\n",
    "            \"fg_iou_thresh\": 0.6,\n",
    "            \"bg_iou_thresh\": 0.4,\n",
    "            \"beta_loss\": 0.5,\n",
    "            \"lambda_loss\": 1.0,\n",
    "            \"nms_score\": 0.25,\n",
    "            \"nms_sigma\": 0.5,\n",
    "            \"dataset_train_ref\": dataset_train_ref,\n",
    "            \"data_loader_train_eval_ref\": data_loader_train_eval_ref,\n",
    "            \"data_loader_val_ref\": data_loader_val_ref,\n",
    "            \"data_loader_test_ref\": data_loader_test_ref,\n",
    "            \"train_coco_ds_ref\": train_coco_ds_ref,\n",
    "            \"val_coco_ds_ref\": val_coco_ds_ref,\n",
    "            \"test_coco_ds_ref\": test_coco_ds_ref,\n",
    "            \"train_sampler\": train_sampler,\n",
    "            \"class_weights\": train_class_weights\n",
    "        }\n",
    "\n",
    "        if tune.Tuner.can_restore(os.path.abspath(self.restore_path)):\n",
    "            tuner = tune.Tuner.restore(\n",
    "                os.path.abspath(self.restore_path),\n",
    "                trainable=self.train_MAVdroneDataset,\n",
    "                param_space=config,\n",
    "                resume_unfinished=True,\n",
    "                resume_errored=False\n",
    "            )\n",
    "            print(f\"Tuner Restored from {self.restore_path}\")\n",
    "        else:\n",
    "            algo = TuneBOHB(\n",
    "                # points_to_evaluate=[\n",
    "                #     {\n",
    "                #         \"lr\": 0.002,\n",
    "                #         \"momentum\": 0.95,\n",
    "                #         \"weight_decay\": 0.005,\n",
    "                #         \"alpha\": 0.37,\n",
    "                #         \"gamma_loss\": 3.5,\n",
    "                #         \"dropout\": 0.4\n",
    "                #     }\n",
    "                # ],\n",
    "                seed=6666\n",
    "            )\n",
    "\n",
    "            algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "\n",
    "            scheduler = HyperBandForBOHB(\n",
    "                time_attr=\"training_iteration\",\n",
    "                reduction_factor=4,\n",
    "                stop_last_trials=False,\n",
    "            )\n",
    "\n",
    "            reporter = tune.JupyterNotebookReporter(overwrite=True,\n",
    "                metric_columns=[\"epoch\", \"current_step\", \"train_loss\", \"val_loss\", \"train_mAP\", \"val_mAP\", \"train_mAR\", \"val_mAR\", \"train_f1\", \"val_f1\"],\n",
    "                parameter_columns=[\"lr\", \"momentum\", \"weight_decay\", \"alpha\", \"gamma_loss\", \"dropout\"],\n",
    "                print_intermediate_tables=True,\n",
    "                sort_by_metric=True\n",
    "            )\n",
    "\n",
    "            tuner = tune.Tuner(\n",
    "                tune.with_resources(\n",
    "                    self.train_MAVdroneDataset,\n",
    "                    resources={\"cpu\": 36.0, \"gpu\": 1.0}\n",
    "                ),\n",
    "                run_config=train.RunConfig(\n",
    "                    name=f\"BOHB_RetinaNet_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                    failure_config=train.FailureConfig(max_failures=1),\n",
    "                    progress_reporter=reporter,\n",
    "                ),\n",
    "                tune_config=tune.TuneConfig(\n",
    "                    mode=\"max\",\n",
    "                    metric=\"val_f1\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    num_samples=int(self.num_samples),\n",
    "                    trial_dirname_creator=self.trial_dirname_creator\n",
    "                ),\n",
    "                param_space=config\n",
    "            )\n",
    "        results = tuner.fit()\n",
    "\n",
    "        best_trial = results.get_best_result(\"val_f1\", \"max\")\n",
    "\n",
    "        print(\"Best trial config: {}\".format(best_trial.config))\n",
    "        print()\n",
    "        print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "        print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "        print(\"Best trial final training mAP: {}\".format(best_trial.metrics[\"train_mAP\"]))\n",
    "        print(\"Best trial final validation mAP: {}\".format(best_trial.metrics[\"val_mAP\"]))\n",
    "        print(\"Best trial final training mAR: {}\".format(best_trial.metrics[\"train_mAR\"]))\n",
    "        print(\"Best trial final validation mAR: {}\".format(best_trial.metrics[\"val_mAR\"]))\n",
    "        print(\"Best trial final training f1-score: {}\".format(best_trial.metrics[\"train_f1\"]))\n",
    "        print(\"Best trial final validation f1-score: {}\".format(best_trial.metrics[\"val_f1\"]))\n",
    "        \n",
    "        print()\n",
    "\n",
    "        best_checkpoint = best_trial.get_best_checkpoint(metric=\"val_f1\", mode=\"max\")\n",
    "\n",
    "        self.test_best_model(best_trial, best_checkpoint)\n",
    "\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial\n",
    "\n",
    "    def test_best_model(self, best_trial, best_checkpoint):\n",
    "        best_model = get_retinanet_model(depth=best_trial.config[\"resnet_depth\"],\n",
    "                                         num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                                         score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                                         detections_per_img=200,\n",
    "                                         fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "                                         bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "                                         topk_candidates=200,\n",
    "                                         alpha=best_trial.config[\"alpha\"],\n",
    "                                         gamma_loss=best_trial.config[\"gamma_loss\"],\n",
    "                                         class_weights=None,\n",
    "                                         beta_loss=best_trial.config[\"beta_loss\"],\n",
    "                                         lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "                                         dropout_prob=best_trial.config[\"dropout\"],\n",
    "                                         nms_score=best_trial.config[\"nms_score\"],\n",
    "                                         nms_sigma=best_trial.config[\"nms_sigma\"])\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        best_model.to(device)\n",
    "\n",
    "        with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                best_checkpoint_data = pickle.load(fp)\n",
    "            best_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "\n",
    "        data_loader_test = ray.get(best_trial.config[\"data_loader_test_ref\"])\n",
    "        test_coco_ds = ray.get(best_trial.config[\"test_coco_ds_ref\"])\n",
    "\n",
    "        test_results = evaluate(best_model, data_loader_test, test_coco_ds, device, train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "        print(f'Best trial test set mAP: {test_results.coco_eval[\"bbox\"].stats[0]}')\n",
    "        print(f'Best trial test set mAR: {test_results.coco_eval[\"bbox\"].stats[8]}')\n",
    "        print(f'Best trial test set f1-score: {calculate_f1_score(test_results.coco_eval[\"bbox\"].stats[0], test_results.coco_eval[\"bbox\"].stats[8])}')\n",
    "\n",
    "        # Get per-class metrics\n",
    "        test_class_metrics = extract_per_class_metrics(test_results, test_coco_ds)\n",
    "        test_class_metrics = {label_dict[k]: v for k, v in test_class_metrics.items()}\n",
    "\n",
    "        print(\"Test Set Class Metrics:\")\n",
    "        for class_name, metrics in test_class_metrics.items():\n",
    "            print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "        print()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     trainer = RetinaNetTuner(num_samples=40, restore_path=\"C:/Users/exx/ray_results/FALSE\")\n",
    "#     train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial = trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Restore search algorithm state and continue tuning if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()\n",
    "# gc.collect()\n",
    "# ray.init()\n",
    "\n",
    "# # Define storage paths and experiment names\n",
    "# storage_path = \"C:/Users/exx/ray_results\"\n",
    "# old_exp_name = \"BOHB_RetinaNet_20250313_121220_resumed\"\n",
    "# prev_experiment_dir = os.path.join(storage_path, old_exp_name)\n",
    "# searcher_state_path = os.path.join(prev_experiment_dir, \"searcher-state-2025-03-30_21-05-08.pkl\")\n",
    "\n",
    "# # Create a new unique experiment name so the previous state isn't overwritten.\n",
    "# new_exp_name = old_exp_name + \"_resumed\"\n",
    "\n",
    "# dataset = MAVdroneDataset(\n",
    "#     csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "#     root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "#     transforms=get_transform(train=True)\n",
    "# )\n",
    "\n",
    "# dataset_val = MAVdroneDataset(\n",
    "#     csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "#     root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "#     transforms=get_transform(train=False)\n",
    "# )\n",
    "\n",
    "# dataset_test = MAVdroneDataset(\n",
    "#     csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "#     root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "#     transforms=get_transform(train=False)\n",
    "# )\n",
    "\n",
    "# dataset_train = torch.utils.data.Subset(dataset, train_indices)\n",
    "# dataset_train_eval = torch.utils.data.Subset(dataset_val, train_indices)\n",
    "# dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "# data_loader_train_eval = torch.utils.data.DataLoader(\n",
    "#     dataset_train_eval, batch_size=1, shuffle=False,\n",
    "#     collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "# )\n",
    "\n",
    "# data_loader_val = torch.utils.data.DataLoader(\n",
    "#     dataset_val, batch_size=1, shuffle=False,\n",
    "#     collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "# )\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, batch_size=1, shuffle=False,\n",
    "#     collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "# )\n",
    "\n",
    "# trainer = RetinaNetTuner(num_samples=10)\n",
    "\n",
    "# dataset_train_ref = ray.put(dataset_train)\n",
    "# data_loader_train_eval_ref = ray.put(data_loader_train_eval)\n",
    "# data_loader_val_ref = ray.put(data_loader_val)\n",
    "# data_loader_test_ref = ray.put(data_loader_test)\n",
    "# train_coco_ds_ref = ray.put(train_coco_ds)\n",
    "# val_coco_ds_ref = ray.put(val_coco_ds)\n",
    "# test_coco_ds_ref = ray.put(test_coco_ds)\n",
    "\n",
    "# # Recreate and restore the search algorithm.\n",
    "# algo = TuneBOHB(seed=6666)\n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "\n",
    "# algo.restore(searcher_state_path)\n",
    "\n",
    "# scheduler = HyperBandForBOHB(\n",
    "#     time_attr=\"training_iteration\",\n",
    "#     reduction_factor=4,\n",
    "#     stop_last_trials=False,\n",
    "# )\n",
    "\n",
    "# def trainable_wrapper(config):\n",
    "#     # add constant parameters to config\n",
    "#     if \"resnet_depth\" not in config:\n",
    "#         config[\"resnet_depth\"] = 50\n",
    "#     if \"score_thresh\" not in config:\n",
    "#         config[\"score_thresh\"] = 0.6\n",
    "#     if \"fg_iou_thresh\" not in config:\n",
    "#         config[\"fg_iou_thresh\"] = 0.6\n",
    "#     if \"bg_iou_thresh\" not in config:\n",
    "#         config[\"bg_iou_thresh\"] = 0.4\n",
    "#     if \"beta_loss\" not in config:\n",
    "#         config[\"beta_loss\"] = 0.5\n",
    "#     if \"lambda_loss\" not in config:\n",
    "#         config[\"lambda_loss\"] = 1.0\n",
    "#     if \"nms_score\" not in config:\n",
    "#         config[\"nms_score\"] = 0.25\n",
    "#     if \"nms_sigma\" not in config:\n",
    "#         config[\"nms_sigma\"] = 0.5\n",
    "\n",
    "#     # Add the dataset references to the config if they're missing\n",
    "#     if \"dataset_train_ref\" not in config:\n",
    "#         config[\"dataset_train_ref\"] = dataset_train_ref\n",
    "#     if \"data_loader_train_eval_ref\" not in config:\n",
    "#         config[\"data_loader_train_eval_ref\"] = data_loader_train_eval_ref\n",
    "#     if \"data_loader_val_ref\" not in config:\n",
    "#         config[\"data_loader_val_ref\"] = data_loader_val_ref\n",
    "#     if \"data_loader_test_ref\" not in config:\n",
    "#         config[\"data_loader_test_ref\"] = data_loader_test_ref\n",
    "#     if \"train_coco_ds_ref\" not in config:\n",
    "#         config[\"train_coco_ds_ref\"] = train_coco_ds_ref\n",
    "#     if \"val_coco_ds_ref\" not in config:\n",
    "#         config[\"val_coco_ds_ref\"] = val_coco_ds_ref\n",
    "#     if \"test_coco_ds_ref\" not in config:\n",
    "#         config[\"test_coco_ds_ref\"] = test_coco_ds_ref\n",
    "#     if \"train_sampler\" not in config:\n",
    "#         config[\"train_sampler\"] = train_sampler\n",
    "#     if \"class_weights\" not in config:\n",
    "#         config[\"class_weights\"] = train_class_weights\n",
    "    \n",
    "#     return trainer.train_MAVdroneDataset(config)\n",
    "\n",
    "# # Use the same wrapped trainable as originally.\n",
    "# wrapped_trainable = tune.with_resources(\n",
    "#     trainable_wrapper,\n",
    "#     resources={\"cpu\": 42.0, \"gpu\": 1.0}\n",
    "# )\n",
    "\n",
    "# reporter = tune.JupyterNotebookReporter(overwrite=True,\n",
    "#                 metric_columns=[\"epoch\", \"current_step\", \"train_loss\", \"val_loss\", \"train_mAP\", \"val_mAP\", \"train_mAR\", \"val_mAR\", \"train_f1\", \"val_f1\"],\n",
    "#                 parameter_columns=[\"lr\", \"momentum\", \"weight_decay\", \"alpha\", \"gamma_loss\", \"dropout\"],\n",
    "#                 print_intermediate_tables=True,\n",
    "#                 sort_by_metric=True\n",
    "#             )\n",
    "\n",
    "# tuner = tune.Tuner(\n",
    "#     wrapped_trainable,\n",
    "#     run_config=train.RunConfig(\n",
    "#         name=new_exp_name,\n",
    "#         storage_path=storage_path,\n",
    "#         failure_config=train.FailureConfig(max_failures=1),\n",
    "#         progress_reporter=reporter\n",
    "#     ),\n",
    "#     tune_config=tune.TuneConfig(\n",
    "#         metric=\"val_f1\",\n",
    "#         mode=\"max\",\n",
    "#         search_alg=algo,\n",
    "#         scheduler=scheduler,\n",
    "#         num_samples=int(trainer.num_samples),\n",
    "#         trial_dirname_creator=trainer.trial_dirname_creator \n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# results = tuner.fit()\n",
    "\n",
    "# best_trial = results.get_best_result(metric=\"val_f1\", mode=\"max\")\n",
    "\n",
    "# print(\"Best trial config: {}\".format(best_trial.config))\n",
    "# print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "# print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "# print(\"Best trial final training mAP: {}\".format(best_trial.metrics[\"train_mAP\"]))\n",
    "# print(\"Best trial final validation mAP: {}\".format(best_trial.metrics[\"val_mAP\"]))\n",
    "# print(\"Best trial final training mAR: {}\".format(best_trial.metrics[\"train_mAR\"]))\n",
    "# print(\"Best trial final validation mAR: {}\".format(best_trial.metrics[\"val_mAR\"]))\n",
    "# print(\"Best trial final training f1-score: {}\".format(best_trial.metrics[\"train_f1\"]))\n",
    "# print(\"Best trial final validation f1-score: {}\".format(best_trial.metrics[\"val_f1\"]))\n",
    "\n",
    "# best_checkpoint = best_trial.get_best_checkpoint(metric=\"val_f1\", mode=\"max\")\n",
    "# trainer.test_best_model(best_trial, best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual definition of hyperparameters, if starting from new session.\n",
    "\n",
    "class BestTrial:\n",
    "    def __init__(self):\n",
    "        # Initialize default config first\n",
    "        self.config = {\n",
    "            \"lr\": 0.001181,\n",
    "            \"resnet_depth\": 50,\n",
    "            \"momentum\": 0.973758,\n",
    "            \"weight_decay\": 1.42512e-05,\n",
    "            \"alpha\": 0.808413,\n",
    "            \"gamma_loss\": 3.46467,\n",
    "            \"dropout\": 0.336394,\n",
    "            \"score_thresh\": 0.5,\n",
    "            \"fg_iou_thresh\": 0.6,\n",
    "            \"bg_iou_thresh\": 0.5,\n",
    "            \"beta_loss\": 0.6,\n",
    "            \"lambda_loss\": 1.2,\n",
    "            \"nms_score\": 0.6,\n",
    "            \"nms_sigma\": 0.3,\n",
    "            \"class_weights\": train_class_weights,\n",
    "            \"train_sampler\": train_sampler,\n",
    "        }\n",
    "\n",
    "        # self.config[\"lr\"] = self.train_lr_finder()\n",
    "\n",
    "    def train_lr_finder(self):\n",
    "        class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "            def inputs_labels_from_batch(self, batch_data):\n",
    "                inputs = [image.to('cuda:0') for image in batch_data[0]]\n",
    "                labels = [{k: v.to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "                return inputs, labels\n",
    "\n",
    "        class CustomValDataLoaderIter(ValDataLoaderIter):\n",
    "            def __iter__(self):\n",
    "                self._iterator = iter(self.data_loader)\n",
    "                self.run_counter = 0\n",
    "                return self\n",
    "\n",
    "            def __next__(self):\n",
    "                try:\n",
    "                    self.run_counter += 1\n",
    "                    return super().__next__()\n",
    "                except StopIteration:\n",
    "                    # Reset if exhausted and then return next batch\n",
    "                    self._iterator = iter(self.data_loader)\n",
    "                    self.run_counter = 0\n",
    "                    return super().__next__()\n",
    "\n",
    "            def inputs_labels_from_batch(self, batch_data):\n",
    "                inputs = [image.to(\"cuda:0\") for image in batch_data[0]]\n",
    "                labels = [{k: v.to(\"cuda:0\") for k, v in t.items()} for t in batch_data[1]]\n",
    "                return inputs, labels\n",
    "\n",
    "        dataset_train = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=True))\n",
    "        dataset_train = torch.utils.data.Subset(dataset_train, train_indices)\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8,\n",
    "                                                        sampler=self.config[\"train_sampler\"],\n",
    "                                                        collate_fn=utils.collate_fn,\n",
    "                                                        num_workers=0, pin_memory=True)\n",
    "        train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "\n",
    "        dataset_val = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False))\n",
    "        dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "            dataset_val, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True)\n",
    "        val_iter = CustomValDataLoaderIter(data_loader_val)\n",
    "\n",
    "        model = get_retinanet_model(\n",
    "            depth=self.config[\"resnet_depth\"],\n",
    "            num_classes=len(self.config[\"class_weights\"]),\n",
    "            score_thresh=self.config[\"score_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=self.config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=self.config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=200,\n",
    "            alpha=self.config[\"alpha\"],\n",
    "            gamma_loss=self.config[\"gamma_loss\"],\n",
    "            class_weights=None,\n",
    "            beta_loss=self.config[\"beta_loss\"],\n",
    "            lambda_loss=self.config[\"lambda_loss\"],\n",
    "            dropout_prob=self.config[\"dropout\"],\n",
    "            nms_score=self.config[\"nms_score\"],\n",
    "            nms_sigma=self.config[\"nms_sigma\"]\n",
    "        ).to('cuda:0')\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params, lr=1e-7, momentum=self.config[\"momentum\"], weight_decay=self.config[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        grad_scaler = torch.GradScaler()\n",
    "\n",
    "        class CustomLRFinder(LRFinder):\n",
    "            def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "                super().__init__(model, optimizer, criterion, device)\n",
    "                self.amp_backend = amp_backend\n",
    "                self.amp_config = amp_config\n",
    "                self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "            def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                for _ in range(accumulation_steps):\n",
    "                    inputs, labels = next(train_iter)\n",
    "                    inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                    with torch.autocast(device_type=\"cuda:0\"):\n",
    "                        outputs = self.model(inputs, labels)\n",
    "                        loss = sum(loss for loss in outputs.values())\n",
    "                    loss /= accumulation_steps\n",
    "                    self.grad_scaler.scale(loss).backward()\n",
    "                    total_loss += loss\n",
    "                self.grad_scaler.step(self.optimizer)\n",
    "                self.grad_scaler.update()\n",
    "                return total_loss.item()\n",
    "\n",
    "            def _validate(self, val_iter, non_blocking_transfer=True):\n",
    "                self.model.train()   # FORCE training mode here!\n",
    "                inputs, labels = next(val_iter)\n",
    "                inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "                with torch.no_grad(), torch.autocast(device_type=\"cuda:0\"):\n",
    "                    outputs = self.model(inputs, labels)\n",
    "                    loss = sum(loss for loss in outputs.values())\n",
    "                return loss.item()\n",
    "\n",
    "        lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "        lr_finder.range_test(train_iter, val_iter, end_lr=1, num_iter=450, step_mode='exp', accumulation_steps=1)\n",
    "        suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "        lr_finder.reset()\n",
    "\n",
    "        # return default if torch lr finder fails\n",
    "        try:\n",
    "            if isinstance(suggested_lr, tuple):\n",
    "                axes, suggested_lr_value = suggested_lr\n",
    "                return suggested_lr_value\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during learning rate finding: {e}\")\n",
    "            # Return a default learning rate if an error occurs\n",
    "            return 5e-4\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_trial = BestTrial()\n",
    "    print(\"Best trial config:\")\n",
    "    for key, value in best_trial.config.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Train Model Using Tuned Hyperparameters**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "ray.shutdown()\n",
    "\n",
    "def visualize_predictions(model, data_loader, device, epoch, num_samples=2, label_dict=None, bbox_colors=None, plot=False,\n",
    "                          output_dir='prediction_visualizations'):\n",
    "    # Define ImageNet normalization parameters\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def denormalize(img_tensor):\n",
    "        img = img_tensor.clone().cpu().numpy().transpose(1, 2, 0)\n",
    "        img = img * imagenet_std + imagenet_mean\n",
    "        return np.clip(img, 0, 1)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get dataset length and generate random indices\n",
    "    dataset_size = len(data_loader.dataset)\n",
    "    random_indices = random.sample(range(dataset_size), min(num_samples, dataset_size))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in random_indices:\n",
    "            # Get the batch index and position within batch\n",
    "            batch_idx = idx // data_loader.batch_size\n",
    "            pos_in_batch = idx % data_loader.batch_size\n",
    "            \n",
    "            for i, (images, targets) in enumerate(data_loader):\n",
    "                if i == batch_idx:\n",
    "                    images = [img.to(device) for img in images]\n",
    "                    outputs = model(images)\n",
    "                    \n",
    "                    b = pos_in_batch\n",
    "                    if b >= len(images):  # Skip if batch is smaller than expected\n",
    "                        continue\n",
    "                        \n",
    "                    img = denormalize(images[b])\n",
    "                    gt_boxes = targets[b]['boxes'].cpu().numpy()\n",
    "                    gt_labels = targets[b]['labels'].cpu().numpy()\n",
    "                    pred_boxes = outputs[b]['boxes'].cpu().numpy()\n",
    "                    pred_labels = outputs[b]['labels'].cpu().numpy()\n",
    "                    pred_scores = outputs[b]['scores'].cpu().numpy()\n",
    "                    \n",
    "                    # Get original image dimensions for high-res output\n",
    "                    height, width = img.shape[0], img.shape[1]\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(width/50, height/100))  # Scale figure appropriately\n",
    "                    \n",
    "                    # Plot ground truth boxes\n",
    "                    ax1.imshow(img)\n",
    "                    for box, label in zip(gt_boxes, gt_labels):\n",
    "                        gt_text = label_dict[label] if label_dict is not None and label in label_dict else str(label)\n",
    "                        gt_color = bbox_colors[label] if bbox_colors is not None and label < len(bbox_colors) else 'black'\n",
    "                        rect = plt.Rectangle((box[0], box[1]),\n",
    "                                          box[2] - box[0],\n",
    "                                          box[3] - box[1],\n",
    "                                          linewidth=1.25, edgecolor=gt_color, facecolor='none')\n",
    "                        ax1.add_patch(rect)\n",
    "                        ax1.text(box[2], box[3],\n",
    "                                f'{gt_text}',\n",
    "                                fontsize=8,\n",
    "                                bbox=dict(facecolor='white', alpha=0.8, pad=0, edgecolor='none'),\n",
    "                                color='black')\n",
    "                    ax1.set_title(f'Ground Truth\\nEpoch {epoch}, Image {idx}')\n",
    "                    \n",
    "                    # Plot predicted boxes\n",
    "                    ax2.imshow(img)\n",
    "                    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "                        pred_text = f\"{label_dict[label] if label_dict is not None and label in label_dict else label}: {score:.2f}\"\n",
    "                        pred_color = bbox_colors[label] if bbox_colors is not None and label < len(bbox_colors) else 'black'\n",
    "                        rect = plt.Rectangle((box[0], box[1]),\n",
    "                                          box[2] - box[0],\n",
    "                                          box[3] - box[1],\n",
    "                                          linewidth=1.25, edgecolor=pred_color, facecolor='none')\n",
    "                        ax2.add_patch(rect)\n",
    "                        ax2.text(box[2], box[3],\n",
    "                                pred_text,\n",
    "                                fontsize=8,\n",
    "                                bbox=dict(facecolor='white', alpha=0.8, pad=0, edgecolor='none'),\n",
    "                                color='black')\n",
    "                    ax2.set_title(f'Predictions\\nEpoch {epoch}, Image {idx}')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    filename = f\"epoch{epoch:03d}_img{idx:05d}.png\"\n",
    "                    filepath = os.path.join(output_dir, filename)\n",
    "                    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "                    if plot:\n",
    "                        plt.show()\n",
    "                    plt.close()\n",
    "                    \n",
    "                    print(f\"Saved visualization to: {filepath}\")\n",
    "                    break\n",
    "\n",
    "# uncomment if running from fresh kernel\n",
    "def create_coco_datasets(train_dataset, val_dataset, test_dataset):\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            train_future = executor.submit(get_coco_api_from_dataset, train_dataset)\n",
    "            val_future = executor.submit(get_coco_api_from_dataset, val_dataset)\n",
    "            test_future = executor.submit(get_coco_api_from_dataset, test_dataset)\n",
    "            train_coco_ds = train_future.result()\n",
    "            val_coco_ds = val_future.result()\n",
    "            test_coco_ds = test_future.result()\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds\n",
    "\n",
    "\n",
    "# def main(train_coco_ds, val_coco_ds, best_trial):\n",
    "def main(best_trial):\n",
    "    set_seed(6666)\n",
    "    print(best_trial.config)\n",
    "    print()\n",
    "\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 1, \"trainable_layers\": 0, \"improvement_threshold\": 0.01, \"variance_threshold\": 1e-4}, \n",
    "        {\"step\": 1, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 2, \"trainable_layers\": 1, \"improvement_threshold\": 0.008, \"variance_threshold\": 5e-5}, \n",
    "        {\"step\": 2, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 4, \"trainable_layers\": 2, \"improvement_threshold\": 0.005, \"variance_threshold\": 2.5e-5}, \n",
    "        {\"step\": 3, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 8, \"trainable_layers\": 3, \"improvement_threshold\": 0.003, \"variance_threshold\": 1e-5}, \n",
    "        {\"step\": 4, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 16, \"trainable_layers\": 4, \"improvement_threshold\": 0.002, \"variance_threshold\": 5e-6}, \n",
    "        {\"step\": 5, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 32, \"trainable_layers\": 5, \"improvement_threshold\": 0.001, \"variance_threshold\": 2.5e-6}, # bs 256\n",
    "    ]\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f'C:/Users/exx/Documents/GitHub/SSD_VGG_PyTorch/runs/RetinaNet/{current_datetime}')\n",
    "    checkpoint_dir = Path(f'./checkpoints/{current_datetime}')\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Dataset setup remains unchanged\n",
    "    dataset = MAVdroneDataset(\n",
    "        csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "        root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "        transforms=get_transform(train=True)\n",
    "    )\n",
    "    dataset_val = MAVdroneDataset(\n",
    "        csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "        root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "        transforms=get_transform(train=False)\n",
    "    )\n",
    "\n",
    "    dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    train_dataset_eval = torch.utils.data.Subset(dataset_val, train_indices)\n",
    "    train_data_loader_eval = torch.utils.data.DataLoader(\n",
    "        train_dataset_eval, batch_size=1, shuffle=False,\n",
    "        collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "    )\n",
    "\n",
    "    dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False,\n",
    "        collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "    )\n",
    "\n",
    "    train_coco_ds, val_coco_ds, test_coco_ds = create_coco_datasets(\n",
    "        train_dataset=train_dataset_eval, \n",
    "        val_dataset=dataset_val, \n",
    "        test_dataset=dataset_val\n",
    "    )\n",
    "\n",
    "    # Instantiate model and initialize optimizer with default parameters\n",
    "    model = get_retinanet_model(\n",
    "        depth=best_trial.config[\"resnet_depth\"],\n",
    "        num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "        score_thresh=best_trial.config[\"score_thresh\"],\n",
    "        detections_per_img=200,\n",
    "        fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "        bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "        topk_candidates=200, \n",
    "        alpha=best_trial.config[\"alpha\"], \n",
    "        gamma_loss=best_trial.config[\"gamma_loss\"],\n",
    "        dropout_prob=best_trial.config[\"dropout\"],\n",
    "        beta_loss=best_trial.config[\"beta_loss\"],\n",
    "        lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "        class_weights=None,\n",
    "        nms_score=best_trial.config[\"nms_score\"],\n",
    "        nms_sigma=best_trial.config[\"nms_sigma\"]\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize with default parameters - we'll update this in each step\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params, \n",
    "        lr=best_trial.config[\"lr\"],\n",
    "        momentum=best_trial.config[\"momentum\"],\n",
    "        weight_decay=best_trial.config[\"weight_decay\"],\n",
    "        nesterov=True\n",
    "    )\n",
    "    \n",
    "    # Training loop state\n",
    "    start_epoch = 0\n",
    "    current_step = 0\n",
    "    step_epoch_counter = 0\n",
    "\n",
    "    # Main training loop\n",
    "    while current_step < len(training_steps):\n",
    "        ts = training_steps[current_step]\n",
    "        batch_size = ts[\"batch_size\"]\n",
    "        print_freq = ts[\"print_freq\"]\n",
    "        accumulation_steps = ts[\"accumulation_steps\"]\n",
    "        backbone_layers = ts[\"trainable_layers\"]\n",
    "        improvement_threshold = ts[\"improvement_threshold\"]\n",
    "        variance_threshold = ts[\"variance_threshold\"]\n",
    "\n",
    "        # Calculate scaled learning rate for this step\n",
    "        scaled_lr = best_trial.config[\"lr\"] * math.sqrt((batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps)\n",
    "\n",
    "        # Adjust the trainable layers for this step\n",
    "        adjust_trainable_layers(model, backbone_layers)\n",
    "\n",
    "        # Create new optimizer for the current step\n",
    "        step_optimizer = optimizer  # Save current optimizer to transfer state\n",
    "        params_new = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params_new,\n",
    "            lr=scaled_lr,\n",
    "            momentum=step_optimizer.param_groups[0]['momentum'],\n",
    "            weight_decay=step_optimizer.param_groups[0]['weight_decay'],\n",
    "            nesterov=step_optimizer.param_groups[0]['nesterov']\n",
    "        )\n",
    "        \n",
    "        # Efficiently transfer optimizer state from previous step\n",
    "        old_state = step_optimizer.state_dict()[\"state\"]\n",
    "        for group in optimizer.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                pid = id(p)\n",
    "                if pid in old_state:\n",
    "                    optimizer.state[p] = old_state[pid]\n",
    "\n",
    "        # Create data loader for this step\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, \n",
    "            sampler=best_trial.config[\"train_sampler\"],\n",
    "            collate_fn=utils.collate_fn, \n",
    "            num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}')\n",
    "        print()\n",
    "        \n",
    "        # Early stopping logic\n",
    "        window_loss = []\n",
    "        window_f1 = []\n",
    "        window_size = 5\n",
    "        minimum_epochs = 15\n",
    "        alpha = 0.1\n",
    "        patience = 3 if ts[\"step\"] < 3 else 5\n",
    "        ema_loss = None\n",
    "        ema_f1 = None\n",
    "        non_improving_counter = 0\n",
    "\n",
    "        # Step training loop\n",
    "        while True:\n",
    "            print(f\"Epoch {start_epoch}, Step: {ts['step']}, Memory: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "            print()\n",
    "\n",
    "            train_metric_logger, val_metric_logger = train_one_epoch(\n",
    "                model, optimizer, data_loader, device, start_epoch,\n",
    "                print_freq, accumulation_steps, data_loader_val, step_epoch_counter\n",
    "            )\n",
    "            print()\n",
    "            \n",
    "            train_coco_evaluator, val_coco_evaluator = evaluate(\n",
    "                model, data_loader_val, val_coco_ds, device, train_data_loader_eval, train_coco_ds\n",
    "            )\n",
    "            print()\n",
    "\n",
    "            # Process metrics\n",
    "            train_class_metrics = extract_per_class_metrics(train_coco_evaluator, train_coco_ds)\n",
    "            val_class_metrics = extract_per_class_metrics(val_coco_evaluator, val_coco_ds)\n",
    "            train_class_metrics = {label_dict[k]: v for k, v in train_class_metrics.items()}\n",
    "            val_class_metrics = {label_dict[k]: v for k, v in val_class_metrics.items()}\n",
    "\n",
    "            # Print per-class metrics\n",
    "            print(\"Training Class Metrics:\")\n",
    "            for name, m in train_class_metrics.items():\n",
    "                print(name)\n",
    "                print(f\"AP@0.50: {m['precision_50']:.4f},\\n AP@0.75: {m['precision_75']:.4f},\\n AP@[0.50:0.95]: {m['precision_50_95']:.4f}\")\n",
    "                print(f\"AR@0.50: {m['recall_50']:.4f},\\n AR@0.75: {m['recall_75']:.4f},\\n AR@[0.50:0.95]: {m['recall_50_95']:.4f}\")\n",
    "                print(f\"F1@0.50: {m['f1_50']:.4f},\\n F1@0.75: {m['f1_75']:.4f},\\n F1@[0.50:0.95]: {m['f1_50_95']:.4f}\")\n",
    "            print()\n",
    "            print(\"\\nValidation Class Metrics:\")\n",
    "            for name, m in val_class_metrics.items():\n",
    "                print(name)\n",
    "                print(f\"AP@0.50: {m['precision_50']:.4f},\\n AP@0.75: {m['precision_75']:.4f},\\n AP@[0.50:0.95]: {m['precision_50_95']:.4f}\")\n",
    "                print(f\"AR@0.50: {m['recall_50']:.4f},\\n AR@0.75: {m['recall_75']:.4f},\\n AR@[0.50:0.95]: {m['recall_50_95']:.4f}\")\n",
    "                print(f\"F1@0.50: {m['f1_50']:.4f},\\n F1@0.75: {m['f1_75']:.4f},\\n F1@[0.50:0.95]: {m['f1_50_95']:.4f}\")\n",
    "            print()\n",
    "\n",
    "            # Visualize predictions periodically\n",
    "            if start_epoch % 5 == 0:\n",
    "                visualize_predictions(model, data_loader_val, device, start_epoch, num_samples=3, \n",
    "                                      label_dict=label_dict, bbox_colors=bbox_colors, plot=False,\n",
    "                                      output_dir=f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/prediction_visualizations/{current_datetime}')\n",
    "\n",
    "            # Calculate current metrics\n",
    "            current_loss = val_metric_logger.loss.avg\n",
    "            current_f1 = calculate_f1_score(val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                           val_coco_evaluator.coco_eval['bbox'].stats[8])\n",
    "            \n",
    "            # Update metric windows for EMA calculation\n",
    "            window_loss.append(current_loss)\n",
    "            window_f1.append(current_f1)\n",
    "            if len(window_loss) > window_size:\n",
    "                window_loss.pop(0)\n",
    "            if len(window_f1) > window_size:\n",
    "                window_f1.pop(0)\n",
    "\n",
    "            # Save checkpoint to disk\n",
    "            checkpoint = {\n",
    "                \"epoch\": start_epoch,\n",
    "                \"current_step\": current_step,\n",
    "                \"step_epoch_counter\": step_epoch_counter,\n",
    "                \"train_loss\": train_metric_logger.loss.avg,\n",
    "                \"val_loss\": current_loss,\n",
    "                \"train_bbox_loss\": train_metric_logger.bbox_regression.avg,\n",
    "                \"val_bbox_loss\": val_metric_logger.bbox_regression.avg,\n",
    "                \"train_class_loss\": train_metric_logger.classification.avg,\n",
    "                \"val_class_loss\": val_metric_logger.classification.avg,\n",
    "                \"train_mAP\": train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                \"train_mAR\": train_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                \"val_mAP\": val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                \"val_mAR\": val_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                            train_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                \"val_f1\": current_f1,\n",
    "                \"config\": best_trial.config\n",
    "            }\n",
    "\n",
    "            # Save checkpoint metrics\n",
    "            checkpoint_metrics_path = checkpoint_dir / f\"{current_datetime}_epoch{start_epoch}_metrics.pth\"\n",
    "            torch.save(checkpoint, checkpoint_metrics_path)\n",
    "\n",
    "            # Save model state\n",
    "            model_path = checkpoint_dir / f\"{current_datetime}_epoch{start_epoch}_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': start_epoch,\n",
    "                'current_step': current_step,\n",
    "                'step_epoch_counter': step_epoch_counter,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, model_path)\n",
    "\n",
    "            # Calculate train/val ratios for monitoring\n",
    "            train_val_ratio = {\n",
    "                'loss': checkpoint[\"train_loss\"] / (checkpoint[\"val_loss\"] if checkpoint[\"val_loss\"] != 0 else 1.0),\n",
    "                'mAP': checkpoint[\"train_mAP\"] / (checkpoint[\"val_mAP\"] if checkpoint[\"val_mAP\"] != 0 else 1.0),\n",
    "                'mAR': checkpoint[\"train_mAR\"] / (checkpoint[\"val_mAR\"] if checkpoint[\"val_mAR\"] != 0 else 1.0),\n",
    "                \"f1\": checkpoint[\"train_f1\"] / (checkpoint[\"val_f1\"] if checkpoint[\"val_f1\"] != 0 else 1.0)\n",
    "            }\n",
    "\n",
    "            # TensorBoard logging\n",
    "            writer.add_scalar('Loss/Train', float(checkpoint[\"train_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Loss/Val', float(checkpoint[\"val_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Box Loss/Train', float(checkpoint[\"train_bbox_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Box Loss/Val', float(checkpoint[\"val_bbox_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Class Loss/Train', float(checkpoint[\"train_class_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Class Loss/Val', float(checkpoint[\"val_class_loss\"]), start_epoch)\n",
    "            writer.add_scalar('mAP/Train', float(checkpoint[\"train_mAP\"]), start_epoch)\n",
    "            writer.add_scalar('mAP/Val', float(checkpoint[\"val_mAP\"]), start_epoch)\n",
    "            writer.add_scalar('mAR/Train', float(checkpoint[\"train_mAR\"]), start_epoch)\n",
    "            writer.add_scalar('mAR/Val', float(checkpoint[\"val_mAR\"]), start_epoch)\n",
    "            writer.add_scalar('F1/Train', float(checkpoint[\"train_f1\"]), start_epoch)\n",
    "            writer.add_scalar('F1/Val', float(checkpoint[\"val_f1\"]), start_epoch)\n",
    "            writer.add_scalar('Ratios/loss', train_val_ratio['loss'], start_epoch)\n",
    "            writer.add_scalar('Ratios/mAP', train_val_ratio['mAP'], start_epoch)\n",
    "            writer.add_scalar('Ratios/mAR', train_val_ratio['mAR'], start_epoch)\n",
    "            writer.add_scalar('Ratios/f1', train_val_ratio['f1'], start_epoch)\n",
    "\n",
    "            # Print current metrics\n",
    "            print(f\"Epoch {start_epoch}: Current Loss = {current_loss:.4f},\", end=\" \")\n",
    "\n",
    "            # Early stopping check\n",
    "            if step_epoch_counter >= minimum_epochs and len(window_loss) == window_size:\n",
    "                if ema_loss is None:\n",
    "                    ema_loss = current_loss\n",
    "                    ema_f1 = current_f1\n",
    "                    relative_improvement = 1.0\n",
    "                    relative_f1_improvement = 1.0\n",
    "                else:\n",
    "                    # Update EMAs\n",
    "                    prev_ema = ema_loss\n",
    "                    prev_f1_ema = ema_f1\n",
    "                    ema_loss = alpha * current_loss + (1 - alpha) * prev_ema\n",
    "                    ema_f1 = alpha * current_f1 + (1 - alpha) * prev_f1_ema\n",
    "                    \n",
    "                    # Calculate improvements\n",
    "                    relative_improvement = (prev_ema - ema_loss) / prev_ema\n",
    "                    relative_f1_improvement = (ema_f1 - prev_f1_ema) / prev_f1_ema\n",
    "                    \n",
    "                    # Check both metrics for improvement\n",
    "                    if (relative_improvement < improvement_threshold and \n",
    "                        relative_f1_improvement < improvement_threshold):\n",
    "                        non_improving_counter += 1\n",
    "                    else:\n",
    "                        non_improving_counter = 0\n",
    "\n",
    "                loss_variance = np.var(window_loss)\n",
    "                f1_variance = np.var(window_f1)\n",
    "                \n",
    "                # Check both metrics for plateau\n",
    "                should_break = ((non_improving_counter >= patience) or \n",
    "                              (loss_variance < variance_threshold and f1_variance < variance_threshold))\n",
    "                \n",
    "                print(f\"EMA Loss = {ema_loss:.4f}, Loss Improvement = {relative_improvement:.4f},\", end=\" \")\n",
    "                print(f\"EMA F1 = {ema_f1:.4f}, F1 Improvement = {relative_f1_improvement:.4f},\", end=\" \")\n",
    "                print(f\"Loss Var = {loss_variance:.6f}, F1 Var = {f1_variance:.6f}, Non-improvement Count = {non_improving_counter}\")\n",
    "            else:\n",
    "                should_break = False\n",
    "                print(\"\")\n",
    "\n",
    "            if start_epoch % 5 == 0:\n",
    "                # Memory cleanup\n",
    "                del train_metric_logger, val_metric_logger, train_coco_evaluator, val_coco_evaluator\n",
    "                gc.collect()\n",
    "            \n",
    "            # Check if we should move to the next step\n",
    "            if should_break:\n",
    "                print(\"Plateau reached; moving to next training step.\\n\")\n",
    "                break\n",
    "                \n",
    "            # Update counters\n",
    "            start_epoch += 1\n",
    "            step_epoch_counter += 1\n",
    "\n",
    "        # Move to next step and reset step epoch counter\n",
    "        current_step += 1\n",
    "        step_epoch_counter = 0\n",
    "\n",
    "    print('All Training Steps Complete!')\n",
    "    writer.close()\n",
    "    # return current_datetime, checkpoint_dir\n",
    "    return current_datetime, checkpoint_dir, test_coco_ds \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     # current_datetime, checkpoint_dir = main(train_coco_ds, val_coco_ds, best_trial)\n",
    "#     current_datetime, checkpoint_dir, test_coco_ds = main(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_datetime = '20250403-211236'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all checkpoint metrics from current run, add to df\n",
    "checkpoint_dir = Path(f'./checkpoints/{current_datetime}')\n",
    "\n",
    "# if file in checkpoint_dir end in '_metrics.pth', load it\n",
    "def load_checkpoints(checkpoint_dir):\n",
    "    checkpoints = []\n",
    "    for file in checkpoint_dir.glob('*_metrics.pth'):\n",
    "        checkpoint = torch.load(file)\n",
    "        checkpoints.append(checkpoint)\n",
    "    return checkpoints\n",
    "\n",
    "checkpoints = load_checkpoints(checkpoint_dir)\n",
    "checkpoints_df = pd.DataFrame(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best train epoch is epoch with max val_f1\n",
    "checkpoints_df = checkpoints_df.sort_values(by='val_f1', ascending=False)\n",
    "best_train_epoch = checkpoints_df.iloc[0]\n",
    "epoch = best_train_epoch['epoch']\n",
    "print(f\"Best train epoch: {epoch}\")\n",
    "\n",
    "# initialize model with best trial config\n",
    "model = get_retinanet_model(depth=best_trial.config[\"resnet_depth\"],\n",
    "                            num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                            score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                            detections_per_img=200,\n",
    "                            fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "                            bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "                            topk_candidates=200, \n",
    "                            alpha=best_trial.config[\"alpha\"], \n",
    "                            gamma_loss=best_trial.config[\"gamma_loss\"], \n",
    "                            dropout_prob=best_trial.config[\"dropout\"],\n",
    "                            beta_loss=best_trial.config[\"beta_loss\"],\n",
    "                            lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "                            class_weights=None,\n",
    "                            nms_score=best_trial.config[\"nms_score\"],\n",
    "                            nms_sigma=best_trial.config[\"nms_sigma\"])\n",
    "\n",
    "# reference best_train_epoch['epoch'] to load model checkpoint from ./checkpoints/{current_datetime}/{current_datetime}_{epoch}.pth\n",
    "checkpoint_path = f'./checkpoints/{current_datetime}/{current_datetime}_epoch{epoch}_model.pth'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# save best model weights to .pth file\n",
    "torch.save(model.state_dict(), f'./checkpoints/{current_datetime}/RetinaNet_ResNet50_FPN_DuckNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy checkpoints and remove model and optimizer state dicts\n",
    "checkpoints_copy = checkpoints.copy()\n",
    "\n",
    "# save checkpoints list to text file in checkpoint_dir\n",
    "checkpoint_list_path = checkpoint_dir / f\"{current_datetime}_checkpoints.txt\"\n",
    "with open(checkpoint_list_path, 'w') as f:\n",
    "    for checkpoint in checkpoints_copy:\n",
    "        checkpoint.pop('model_state_dict', None)\n",
    "        checkpoint.pop('optimizer_state_dict', None)\n",
    "        f.write(f\"{checkpoint}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Model Inference on Test Dataset**</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/', \n",
    "                                transforms = get_transform(train = False))\n",
    "\n",
    "# subset test dataset using test_indices\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size = 1, shuffle = False,\n",
    "                                               collate_fn = utils.collate_fn, num_workers = 0,\n",
    "                                               pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coco_ds = get_coco_api_from_dataset(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda:0')\n",
    "test_performance = evaluate(model, data_loader_test, test_coco_ds, device='cuda:0', train_data_loader=None, train_coco_ds=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AP, AR, and F1 at IoU = 0.5, 0.75, & 0.5:0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "mAP_50_95 = test_performance.coco_eval[\"bbox\"].stats[0]  # AP at IoU=0.50:0.95\n",
    "mAP_50 = test_performance.coco_eval[\"bbox\"].stats[1]     # AP at IoU=0.50\n",
    "mAP_75 = test_performance.coco_eval[\"bbox\"].stats[2]     # AP at IoU=0.75\n",
    "\n",
    "# For recall at specific IoU thresholds, we need to access the raw recall array\n",
    "recall_array = test_performance.coco_eval[\"bbox\"].eval[\"recall\"]  # shape: [T, K, A, M]\n",
    "mAR_50_95 = test_performance.coco_eval[\"bbox\"].stats[8]  # AR at IoU=0.50:0.95\n",
    "mAR_50 = np.mean(recall_array[0, :, 0, -1])  # AR at IoU=0.50 (first threshold)\n",
    "mAR_75 = np.mean(recall_array[5, :, 0, -1])  # AR at IoU=0.75 (sixth threshold)\n",
    "\n",
    "# Print overall results\n",
    "print(\"Overall Detection Performance:\")\n",
    "print(f\"mAP@0.50 = {mAP_50:.4f}\")\n",
    "print(f\"mAR@0.50 = {mAR_50:.4f}\")  \n",
    "print(f\"F1@0.50 = {calculate_f1_score(mAP_50, mAR_50):.4f}\")\n",
    "print()\n",
    "print(f\"mAP@0.75 = {mAP_75:.4f}\")\n",
    "print(f\"mAR@0.75 = {mAR_75:.4f}\")\n",
    "print(f\"F1@0.75 = {calculate_f1_score(mAP_75, mAR_75):.4f}\")\n",
    "print()\n",
    "print(f\"mAP@[0.50:0.95] = {mAP_50_95:.4f}\")\n",
    "print(f\"mAR@[0.50:0.95] = {mAR_50_95:.4f}\")\n",
    "print(f\"F1@[0.50:0.95] = {calculate_f1_score(mAP_50_95, mAR_50_95):.4f}\")\n",
    "print()\n",
    "\n",
    "# Get per-class metrics\n",
    "test_class_metrics = extract_per_class_metrics(test_performance, test_coco_ds)\n",
    "test_class_metrics = {label_dict[k]: v for k, v in test_class_metrics.items()}\n",
    "\n",
    "# Print per-class results\n",
    "print(\"Per-Class Detection Performance:\")\n",
    "for class_name, metrics in test_class_metrics.items():\n",
    "    print(f\"\\nClass: {class_name}\")\n",
    "    print(f\"AP@0.50 = {metrics['precision_50']:.4f}\")\n",
    "    print(f\"AR@0.50 = {metrics['recall_50']:.4f}\")\n",
    "    print(f\"F1@0.50 = {calculate_f1_score(metrics['precision_50'], metrics['recall_50']):.4f}\")\n",
    "    print()\n",
    "    print(f\"AP@0.75 = {metrics['precision_75']:.4f}\")\n",
    "    print(f\"AR@0.75 = {metrics['recall_75']:.4f}\")\n",
    "    print(f\"F1@0.75 = {calculate_f1_score(metrics['precision_75'], metrics['recall_75']):.4f}\")\n",
    "    print()\n",
    "    print(f\"AP@[0.50:0.95] = {metrics['precision_50_95']:.4f}\")\n",
    "    print(f\"AR@[0.50:0.95] = {metrics['recall_50_95']:.4f}\") \n",
    "    print(f\"F1@[0.50:0.95] = {calculate_f1_score(metrics['precision_50_95'], metrics['recall_50_95']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, data_loader_test, device='cuda:0', epoch=best_train_epoch[\"epoch\"], num_samples=10, \n",
    "                      label_dict=label_dict, bbox_colors=bbox_colors, plot=False, output_dir=f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/test_set_predictions/{current_datetime}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Evaluate the Full Dataset**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def evaluate_full_dataset(model, device='cuda:0', score_thresh=0.5, \n",
    "                          output_dir=f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/full_dataset_evaluation/{current_datetime}'):\n",
    "    \"\"\"Evaluate model on full dataset and save comprehensive results\"\"\"\n",
    "    \n",
    "    model = model.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    # full_dataset = MAVdroneDataset(\n",
    "    #     csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "    #     root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "    #     transforms=get_transform(train=False)\n",
    "    # )\n",
    "\n",
    "    # full_coco_ds = get_coco_api_from_dataset(full_dataset)\n",
    "\n",
    "    full_dataset = dataset_test\n",
    "    full_coco_ds = test_coco_ds\n",
    "\n",
    "    full_data_loader = torch.utils.data.DataLoader(\n",
    "        full_dataset, batch_size=1, shuffle=False,\n",
    "        collate_fn=utils.collate_fn, num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    performance = evaluate(model, full_data_loader, full_coco_ds, device=device)\n",
    "    \n",
    "    # Create output directory  \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run inference and collect predictions\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(full_data_loader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for output, target in zip(outputs, targets):\n",
    "                img_id = target.get('image_id', i)\n",
    "                if isinstance(img_id, torch.Tensor):\n",
    "                    img_id = img_id.item()\n",
    "                    \n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "                \n",
    "                for k in range(len(boxes)):\n",
    "                    if scores[k] < 0.1:  # confidence threshold\n",
    "                        continue\n",
    "                    x1, y1, x2, y2 = boxes[k]\n",
    "                    label_id = int(labels[k])\n",
    "                    score = float(scores[k])\n",
    "                    \n",
    "                    all_predictions.append({\n",
    "                        'image_id': img_id,\n",
    "                        'class_id': label_id,\n",
    "                        'class_name': label_dict[label_id] if label_id in label_dict else f\"unknown_{label_id}\",\n",
    "                        'confidence': score,\n",
    "                        'x1': float(x1),\n",
    "                        'y1': float(y1),\n",
    "                        'x2': float(x2),\n",
    "                        'y2': float(y2)\n",
    "                    })\n",
    "                    \n",
    "            if (i + 1) % 100 == 0 or i == 0:\n",
    "                print(f\"Processed {i+1}/{len(full_data_loader)} images\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    pred_df = pd.DataFrame(all_predictions)\n",
    "    print(f\"Found {len(pred_df)} predictions above confidence threshold\")\n",
    "    pred_df.to_csv(f'{output_dir}/all_predictions.csv', index=False)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mAP_50_95 = performance.coco_eval[\"bbox\"].stats[0]  # AP at IoU=0.50:0.95\n",
    "    mAP_50 = performance.coco_eval[\"bbox\"].stats[1]     # AP at IoU=0.50  \n",
    "    mAP_75 = performance.coco_eval[\"bbox\"].stats[2]     # AP at IoU=0.75\n",
    "\n",
    "    recall_array = performance.coco_eval[\"bbox\"].eval[\"recall\"]  # shape: [T, K, A, M]\n",
    "    mAR_50_95 = performance.coco_eval[\"bbox\"].stats[8]  # AR at IoU=0.50:0.95\n",
    "    mAR_50 = np.mean(recall_array[0, :, 0, -1])  # AR at IoU=0.50 \n",
    "    mAR_75 = np.mean(recall_array[5, :, 0, -1])  # AR at IoU=0.75\n",
    "\n",
    "    # Calculate F1 scores\n",
    "    f1_50 = calculate_f1_score(mAP_50, mAR_50) \n",
    "    f1_75 = calculate_f1_score(mAP_75, mAR_75)\n",
    "    f1_50_95 = calculate_f1_score(mAP_50_95, mAR_50_95)\n",
    "\n",
    "    # Save overall metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['mAP@0.50', 'mAR@0.50', 'F1@0.50',\n",
    "                'mAP@0.75', 'mAR@0.75', 'F1@0.75', \n",
    "                'mAP@[0.50:0.95]', 'mAR@[0.50:0.95]', 'F1@[0.50:0.95]'],\n",
    "        'Value': [mAP_50, mAR_50, f1_50,\n",
    "                mAP_75, mAR_75, f1_75,\n",
    "                mAP_50_95, mAR_50_95, f1_50_95]\n",
    "    })\n",
    "    metrics_df.to_csv(f'{output_dir}/overall_metrics.csv', index=False)\n",
    "    \n",
    "    # Get per-class metrics\n",
    "    class_metrics = extract_per_class_metrics(performance, full_coco_ds)\n",
    "    class_metrics_dict = {label_dict[k]: v for k, v in class_metrics.items()}\n",
    "    \n",
    "    class_rows = []\n",
    "    for class_name, metrics in class_metrics_dict.items():\n",
    "        class_rows.append({\n",
    "            'class_name': class_name,\n",
    "            'precision_50': metrics['precision_50'],\n",
    "            'recall_50': metrics['recall_50'],\n",
    "            'f1_50': calculate_f1_score(metrics['precision_50'], metrics['recall_50']),\n",
    "            'precision_75': metrics['precision_75'], \n",
    "            'recall_75': metrics['recall_75'],\n",
    "            'f1_75': calculate_f1_score(metrics['precision_75'], metrics['recall_75']),\n",
    "            'precision_50_95': metrics['precision_50_95'],\n",
    "            'recall_50_95': metrics['recall_50_95'],\n",
    "            'f1_50_95': calculate_f1_score(metrics['precision_50_95'], metrics['recall_50_95'])\n",
    "        })\n",
    "    \n",
    "    class_metrics_df = pd.DataFrame(class_rows)\n",
    "    class_metrics_df.to_csv(f'{output_dir}/class_metrics.csv', index=False)\n",
    "    \n",
    "    print(f\"Full dataset evaluation complete. Results saved to {output_dir}\")\n",
    "\n",
    "    # Create confusion matrices for each IoU threshold\n",
    "    iou_thresholds = [0.5, 0.75]  # Single IoU thresholds\n",
    "    iou_range = np.arange(0.5, 1.0, 0.05)  # Range for 0.5:0.95\n",
    "\n",
    "    n_classes = len(label_dict)\n",
    "\n",
    "    # Initialize confusion matrices with extra row/column for false positives/negatives\n",
    "    confusion_matrices = {\n",
    "        'IoU=0.50': np.zeros((n_classes+1, n_classes+1), dtype=np.int32),\n",
    "        'IoU=0.75': np.zeros((n_classes+1, n_classes+1), dtype=np.int32),\n",
    "        'IoU=0.50:0.95': np.zeros((n_classes+1, n_classes+1))  # Keep float for average\n",
    "    }\n",
    "\n",
    "    # Match predictions to ground truth using different IoU thresholds\n",
    "    with torch.no_grad():\n",
    "        for images, targets in full_data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for output, target in zip(outputs, targets):\n",
    "                gt_boxes = target['boxes'].cpu().numpy()\n",
    "                gt_labels = target['labels'].cpu().numpy()\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "                pred_labels = output['labels'].cpu().numpy()\n",
    "                \n",
    "                # Filter out predictions below threshold\n",
    "                valid_preds = pred_scores >= score_thresh\n",
    "                pred_boxes = pred_boxes[valid_preds]\n",
    "                pred_labels = pred_labels[valid_preds]\n",
    "                pred_scores = pred_scores[valid_preds]\n",
    "                \n",
    "                # Calculate IoU between each pred box and gt box\n",
    "                if len(pred_boxes) > 0 and len(gt_boxes) > 0:\n",
    "                    ious = box_ops.box_iou(\n",
    "                        torch.from_numpy(pred_boxes),\n",
    "                        torch.from_numpy(gt_boxes)\n",
    "                    ).numpy()\n",
    "                    \n",
    "                    # Process single IoU thresholds (0.5 and 0.75)\n",
    "                    for iou_thresh in iou_thresholds:\n",
    "                        iou_key = f'IoU={iou_thresh:.2f}'\n",
    "                        gt_matched = set()\n",
    "                        pred_matched = set()\n",
    "                        \n",
    "                        # Sort predictions by confidence score (high to low)\n",
    "                        pred_order = np.argsort(-pred_scores)\n",
    "                        \n",
    "                        # Match each prediction to a ground truth box\n",
    "                        for pred_idx in pred_order:\n",
    "                            best_iou = -1\n",
    "                            best_gt_idx = -1\n",
    "                            \n",
    "                            # Find the best matching ground truth box\n",
    "                            for gt_idx in range(len(gt_boxes)):\n",
    "                                if gt_idx in gt_matched:\n",
    "                                    continue\n",
    "                                    \n",
    "                                iou = ious[pred_idx, gt_idx]\n",
    "                                if iou >= iou_thresh and iou > best_iou:\n",
    "                                    best_iou = iou\n",
    "                                    best_gt_idx = gt_idx\n",
    "                            \n",
    "                            # If a match is found, update confusion matrix\n",
    "                            if best_gt_idx != -1:\n",
    "                                gt_label_idx = gt_labels[best_gt_idx] - 1  # Adjust index (RetinaNet uses 1-indexed labels)\n",
    "                                pred_label_idx = pred_labels[pred_idx] - 1  # Adjust index\n",
    "                                \n",
    "                                confusion_matrices[iou_key][gt_label_idx, pred_label_idx] += 1\n",
    "                                gt_matched.add(best_gt_idx)\n",
    "                                pred_matched.add(pred_idx)\n",
    "                        \n",
    "                        # Add false positives (predictions with no matching ground truth)\n",
    "                        for pred_idx in range(len(pred_boxes)):\n",
    "                            if pred_idx not in pred_matched:\n",
    "                                pred_label_idx = pred_labels[pred_idx] - 1  # Adjust index\n",
    "                                # Last row is for false positives\n",
    "                                confusion_matrices[iou_key][n_classes, pred_label_idx] += 1\n",
    "                        \n",
    "                        # Add false negatives (ground truths with no matching prediction)\n",
    "                        for gt_idx in range(len(gt_boxes)):\n",
    "                            if gt_idx not in gt_matched:\n",
    "                                gt_label_idx = gt_labels[gt_idx] - 1  # Adjust index\n",
    "                                # Last column is for false negatives\n",
    "                                confusion_matrices[iou_key][gt_label_idx, n_classes] += 1\n",
    "                    \n",
    "                    # Process IoU range 0.5:0.95\n",
    "                    temp_matrices = []\n",
    "                    for iou_thresh in iou_range:\n",
    "                        temp_mat = np.zeros((n_classes+1, n_classes+1))\n",
    "                        gt_matched = set()\n",
    "                        pred_matched = set()\n",
    "                        \n",
    "                        # Sort predictions by confidence score (high to low)\n",
    "                        pred_order = np.argsort(-pred_scores)\n",
    "                        \n",
    "                        # Match each prediction to a ground truth box\n",
    "                        for pred_idx in pred_order:\n",
    "                            best_iou = -1\n",
    "                            best_gt_idx = -1\n",
    "                            \n",
    "                            # Find the best matching ground truth box\n",
    "                            for gt_idx in range(len(gt_boxes)):\n",
    "                                if gt_idx in gt_matched:\n",
    "                                    continue\n",
    "                                    \n",
    "                                iou = ious[pred_idx, gt_idx]\n",
    "                                if iou >= iou_thresh and iou > best_iou:\n",
    "                                    best_iou = iou\n",
    "                                    best_gt_idx = gt_idx\n",
    "                            \n",
    "                            # If a match is found, update confusion matrix\n",
    "                            if best_gt_idx != -1:\n",
    "                                gt_label_idx = gt_labels[best_gt_idx] - 1  # Adjust index\n",
    "                                pred_label_idx = pred_labels[pred_idx] - 1  # Adjust index\n",
    "                                \n",
    "                                temp_mat[gt_label_idx, pred_label_idx] += 1\n",
    "                                gt_matched.add(best_gt_idx)\n",
    "                                pred_matched.add(pred_idx)\n",
    "                        \n",
    "                        # Add false positives (predictions with no matching ground truth)\n",
    "                        for pred_idx in range(len(pred_boxes)):\n",
    "                            if pred_idx not in pred_matched:\n",
    "                                pred_label_idx = pred_labels[pred_idx] - 1  # Adjust index\n",
    "                                # Last row is for false positives\n",
    "                                temp_mat[n_classes, pred_label_idx] += 1\n",
    "                        \n",
    "                        # Add false negatives (ground truths with no matching prediction)\n",
    "                        for gt_idx in range(len(gt_boxes)):\n",
    "                            if gt_idx not in gt_matched:\n",
    "                                gt_label_idx = gt_labels[gt_idx] - 1  # Adjust index\n",
    "                                # Last column is for false negatives\n",
    "                                temp_mat[gt_label_idx, n_classes] += 1\n",
    "                        \n",
    "                        temp_matrices.append(temp_mat)\n",
    "                    \n",
    "                    # Average the matrices for 0.5:0.95\n",
    "                    confusion_matrices['IoU=0.50:0.95'] += np.mean(temp_matrices, axis=0)\n",
    "\n",
    "    # Plot confusion matrices\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    for idx, (iou_key, conf_mat) in enumerate(confusion_matrices.items()):\n",
    "        plt.subplot(1, 3, idx + 1)\n",
    "        \n",
    "        # Create labels list with class names plus FP/FN labels\n",
    "        labels = [label_dict[i+1] for i in range(n_classes)]\n",
    "        labels.append(\"False Pos\")  \n",
    "        labels_with_fn = labels.copy()[:-1]  # Remove False Pos for x-axis\n",
    "        labels_with_fn.append(\"False Neg\") \n",
    "        \n",
    "        sns.heatmap(conf_mat, \n",
    "            annot=True, \n",
    "            fmt='d' if 'IoU=0.50:0.95' not in iou_key else '.1f',  # Integer for single thresholds, float for average\n",
    "            cmap='Blues',\n",
    "            xticklabels=labels_with_fn, \n",
    "            yticklabels=labels)\n",
    "        \n",
    "        if 'IoU=0.50:0.95' in iou_key:\n",
    "            plt.title('Confusion Matrix Averaged Across IoU=[0.50:0.95]')\n",
    "        else:\n",
    "            plt.title(f'Confusion Matrix at {iou_key}')\n",
    "        plt.ylabel('True Class')\n",
    "        plt.xlabel('Predicted Class')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return metrics_df, class_metrics_df, pred_df\n",
    "\n",
    "evaluate_full_dataset(model, device='cuda:0', score_thresh=float(model.score_thresh),\n",
    "                      output_dir=f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/full_dataset_evaluation/{current_datetime}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bohb_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
