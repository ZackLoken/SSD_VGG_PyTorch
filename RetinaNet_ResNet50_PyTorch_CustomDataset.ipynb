{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Reading and Cleaning Annotation Data for Custom PyTorch Object Detection**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "if ipython is not None:\n",
    "    ipython.cache_size = 0  # disable cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # restrict cuda to gpu 0\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # set CUDA kernel to synchronous\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion(); # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load annotation data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading JSON as dictionary\n",
    "def read_json(filename: str) -> dict:\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Reading {filename} file encountered an error: {e}\")\n",
    "    return data\n",
    "\n",
    "# Function to create a DataFrame from a list of records\n",
    "def create_dataframe(data: list) -> pd.DataFrame:\n",
    "    # Normalize the column levels and create a DataFrame\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Main function to iterate over files in directory and add to df\n",
    "def main():\n",
    "    # Assign directory and empty list for collecting records\n",
    "    directory = \"C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Annotations/\"  # annotation directory\n",
    "    records = []\n",
    "    \n",
    "    # Iterate over files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            # Read the JSON file as python dictionary \n",
    "            data = read_json(filename=f)\n",
    "        \n",
    "            # Create the dataframe for the array items in annotations key \n",
    "            df = create_dataframe(data=data['annotations'])\n",
    "            df.insert(loc=0, column='img_name', value=f'{f[-30:-5]}.JPG')\n",
    "        \n",
    "            df.rename(columns={\n",
    "                \"img_name\": \"img_name\",\n",
    "                \"name\": \"label\",\n",
    "                \"bounding_box.h\": \"bbox_height\",\n",
    "                \"bounding_box.w\": \"bbox_width\",\n",
    "                \"bounding_box.x\": \"bbox_x_topLeft\",\n",
    "                \"bounding_box.y\": \"bbox_y_topLeft\",\n",
    "                \"polygon.paths\": \"polygon_path\"\n",
    "            }, inplace=True)\n",
    "            \n",
    "            # Append the records to the list\n",
    "            records.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping non-file: {filename}\")\n",
    "\n",
    "    # Concatenate all records into a single DataFrame\n",
    "    annos_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "    # Convert x, y, h, w to xmin, ymin, xmax, ymax\n",
    "    annos_df['xmin'] = annos_df['bbox_x_topLeft']\n",
    "    annos_df['ymin'] = annos_df['bbox_y_topLeft']\n",
    "    annos_df['xmax'] = annos_df['bbox_x_topLeft'] + annos_df['bbox_width']\n",
    "    annos_df['ymax'] = annos_df['bbox_y_topLeft'] + annos_df['bbox_height']\n",
    "  \n",
    "    # Drop unnecessary columns \n",
    "    annos_df = annos_df.drop(columns=['bbox_height', 'bbox_width', 'bbox_x_topLeft', \n",
    "                                      'bbox_y_topLeft', 'id', 'slot_names', 'polygon_path'])\n",
    "        \n",
    "    return annos_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-process annotation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique image names\n",
    "unique_img_names = df['img_name'].unique()\n",
    "\n",
    "invalid_img_names = []\n",
    "for img_name in unique_img_names:\n",
    "    img_path = f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Images/{img_name}'\n",
    "    img = Image.open(img_path)\n",
    "    if img.size == (5184, 3888):\n",
    "        invalid_img_names.append(img_name)\n",
    "\n",
    "# remove invalid images from df\n",
    "df = df[~df['img_name'].isin(invalid_img_names)]\n",
    "\n",
    "img_classes_to_remove = ['WTDE', 'TURT', 'NUTR', 'ANHI', 'CAGO', \n",
    "                         'DCCO', 'GWFG', 'GBHE', 'COGA', 'PBGR'] # remove images with these classes\n",
    "\n",
    "for class_label in img_classes_to_remove:\n",
    "    # Get all image names with the class\n",
    "    images_with_class = df[df['label'] == class_label]['img_name'].unique()\n",
    "\n",
    "    # Remove all rows for img\n",
    "    df = df[~df['img_name'].isin(images_with_class)]\n",
    "\n",
    "# remove images containing only hens\n",
    "hen_images_no_other_class = df[(df['label'] == 'Hen') & (~df['img_name'].isin(df[df['label'] != 'Hen']['img_name']))]['img_name'].unique()\n",
    "df = df[~df['img_name'].isin(hen_images_no_other_class)]\n",
    "\n",
    "# Separate classes with less than 100 instances\n",
    "class_counts = df['label'].value_counts()\n",
    "other_classes = class_counts[class_counts < 100].index.tolist()\n",
    "positive_classes = class_counts[class_counts >= 100].index.tolist()\n",
    "\n",
    "# print class counts for each label\n",
    "print(\"Number of instances per class in cleaned dataset:\")\n",
    "for label in df['label'].unique():\n",
    "    print(f'{label}: {len(df[df[\"label\"] == label])}')\n",
    "\n",
    "# print other and positive classes\n",
    "print()\n",
    "print(f'Other classes: {other_classes}')\n",
    "print(f'Positive classes: {positive_classes}')\n",
    "\n",
    "# remove images with other classes\n",
    "for class_label in other_classes:\n",
    "    # Get all image names with the class\n",
    "    images_with_class = df[df['label'] == class_label]['img_name'].unique()\n",
    "\n",
    "    # Remove all rows for img\n",
    "    df = df[~df['img_name'].isin(images_with_class)]\n",
    "\n",
    "# confirm the only classes in df are positive classes\n",
    "assert len(df['label'].unique()) == len(positive_classes)\n",
    "\n",
    "# encode labels as int (reserve 0 for 'background')\n",
    "df['target'] = pd.Categorical(df['label']).codes + 1\n",
    "\n",
    "# filter out images with invalid bounding boxes\n",
    "df = df.groupby('img_name').filter(lambda x: ((x['xmin'] < x['xmax']) & (x['ymin'] < x['ymax'])).all())\n",
    "\n",
    "# Create a dictionary using df['label'] as the keys and df['target'] as the values\n",
    "label_dict = dict(zip(df['target'], df['label']))\n",
    "\n",
    "# Drop the original 'label' column from df\n",
    "df = df.drop(['label'], axis=1)\n",
    "\n",
    "# Rename 'target' column to 'label'\n",
    "df.rename(columns={'target': 'label'}, inplace=True)\n",
    "\n",
    "# Save df as csv in directory\n",
    "df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter images after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store unique img_names in filtered df as array\n",
    "img_names = df['img_name'].unique().tolist()\n",
    "\n",
    "# Create a new directory called 'filtered_images'\n",
    "new_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images'\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "else:\n",
    "    for file in os.listdir(new_dir):\n",
    "        os.remove(os.path.join(new_dir, file))\n",
    "\n",
    "# Copy images in img_names to new directory\n",
    "for img in img_names:\n",
    "    shutil.copy2(f'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/Images/{img}', new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Transform and Augment Image and Annotation Data for Custom PyTorch Object Detection**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "import torchvision.transforms.v2 as T\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAVdroneDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset Loader for Waterfowl Drone Imagery\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transforms):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the CSV file with annotations.\n",
    "            root_dir (string): Directory containing all images.\n",
    "            transforms (callable): Transformation to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.unique_image_names = self.df['img_name'].unique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_name = self.unique_image_names[idx]\n",
    "\n",
    "        # Isolate first row to prevent multiple instances of the same image\n",
    "        row = self.df[self.df['img_name'] == image_name].iloc[0]\n",
    "\n",
    "        image_path = os.path.join(self.root_dir, row['img_name'])\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # Convert to Tensor\n",
    "\n",
    "        # Bounding boxes and labels\n",
    "        boxes = self.df[self.df['img_name'] == image_name][['xmin', 'ymin', 'xmax', 'ymax']].values \n",
    "        labels = self.df[self.df['img_name'] == image_name]['label'].values\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)  # (n_objects)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Calculate area\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # Assume no crowd annotations\n",
    "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': tv_tensors.BoundingBoxes(boxes, format=tv_tensors.BoundingBoxFormat.XYXY, canvas_size=(image.shape[1], image.shape[2])),\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_image_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train: bool):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train (bool): Whether the transform is for training or validation/testing.\n",
    "    \"\"\"\n",
    "    transforms_list = [T.ToImage()]\n",
    "    \n",
    "    if train:\n",
    "        transforms_list.append(\n",
    "            T.RandomApply(\n",
    "                [T.RandomRotation(\n",
    "                    degrees=[-15, 15],\n",
    "                    interpolation=torchvision.transforms.InterpolationMode.BILINEAR,\n",
    "                    fill=defaultdict(lambda: 0, {tv_tensors.Image: (0, 0, 0)})\n",
    "                )],\n",
    "                p=0.5\n",
    "            )\n",
    "        )\n",
    "        transforms_list.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms_list.append(T.ClampBoundingBoxes())  # Clamp bounding boxes to image boundaries\n",
    "        transforms_list.append(T.SanitizeBoundingBoxes(min_size=25))\n",
    "    \n",
    "    transforms_list.append(\n",
    "        T.Resize(\n",
    "            size=(810,),\n",
    "            max_size=1440,\n",
    "            interpolation=torchvision.transforms.InterpolationMode.BICUBIC\n",
    "        )\n",
    "    )\n",
    "    transforms_list.append(\n",
    "        T.ToDtype(\n",
    "            dtype=torch.float32,\n",
    "            scale=True\n",
    "        )\n",
    "    )\n",
    "    transforms_list.append(\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return T.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions for plotting image and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes are values in label_dict\n",
    "classes = list(label_dict.values())\n",
    "\n",
    "# reverse label dictionary for mapping predictions to classes\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "# distinct colors \n",
    "bbox_colors = [\n",
    "    \"#FF0000\",  # Red\n",
    "    \"#00FF00\",  # Green\n",
    "    \"#FFFF00\",  # Yellow\n",
    "    \"#FF00FF\",  # Magenta\n",
    "    \"#00FFFF\",  # Cyan\n",
    "    \"#FFC0CB\",  # Pink\n",
    "    \"#FFA500\",  # Orange\n",
    "    \"#800080\",  # Purple\n",
    "    \"#FFFFFF\",  # White\n",
    "    \"#FFD700\",  # Gold\n",
    "]\n",
    "\n",
    "# label color map for plotting color-coded boxes by class\n",
    "label_color_map = {k: bbox_colors[i] for i, k in enumerate(label_dict.keys())}\n",
    "\n",
    "# function for reshaping boxes \n",
    "def get_box(boxes):\n",
    "    boxes = np.array(boxes)\n",
    "    boxes = boxes.astype('float').reshape(-1, 4)\n",
    "    if boxes.shape[0] == 1 : return boxes\n",
    "    return np.squeeze(boxes)\n",
    "\n",
    "\n",
    "# function for plotting image\n",
    "def img_show(image, ax = None, figsize = (6, 9)):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.imshow(image)\n",
    "    return ax\n",
    " \n",
    "\n",
    "def plot_bbox(ax, boxes, labels):\n",
    "    # add box to the image and use label_color_map to color-code by bounding box class if exists else 'black'\n",
    "    ax.add_patch(plt.Rectangle((boxes[:, 0], boxes[:, 1]), boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1],\n",
    "                    fill = False,\n",
    "                    color = label_color_map[labels.item()] if labels.item() in label_color_map else 'black', \n",
    "                    linewidth = 1.25))\n",
    "    # add label text to bounding box using label_dict if label exists else labels\n",
    "    ax.text(boxes[:, 2], boxes[:, 3], \n",
    "            (label_dict[labels.item()] if labels.item() in label_dict else labels.item()),\n",
    "            fontsize = 8,\n",
    "            bbox = dict(facecolor = 'white', alpha = 0.8, pad = 0, edgecolor = 'none'),\n",
    "            color = 'black')\n",
    "\n",
    "\n",
    "# function for plotting all boxes and labels on the image using get_polygon, img_show, and plot_mask functions\n",
    "def plot_detections(image, boxes, labels, ax = None):\n",
    "    ax = img_show(image.permute(1, 2, 0), ax = ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = get_box(boxes[i])\n",
    "        plot_bbox(ax, box, labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot sample batch to confirm data loads and transforms correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample batch of data to custom PyTorch Dataset and Transform\n",
    "sample_dataset = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv', \n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images', \n",
    "                                transforms = get_transform(train = True))\n",
    "\n",
    "sample_data_loader = torch.utils.data.DataLoader(sample_dataset, batch_size = 8, shuffle=True, \n",
    "                                                collate_fn = utils.collate_fn, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store images and annotation targets from sample batch\n",
    "batch = next(iter(sample_data_loader))\n",
    "images, targets = batch\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "images = [np.clip(image, 0, 1) for image in images]\n",
    "\n",
    "# Plot all samples from batch in a grid of subplots\n",
    "plt.figure(figsize=(16, int(sample_data_loader.batch_size) * 5))\n",
    "for i in range(int(sample_data_loader.batch_size)):\n",
    "    ax = plt.subplot(int(sample_data_loader.batch_size), 2, 1 + i)\n",
    "    plot_detections(images[i], targets[i]['boxes'], targets[i]['labels'], ax=ax)\n",
    "    # Query the dataset to get the image name for the given image_id\n",
    "    image_id = targets[i]['image_id'].item()  # Convert tensor to integer\n",
    "    image_name = sample_dataset.unique_image_names[image_id]\n",
    "    plt.title(image_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use stratified sampling to split multi-label dataset into train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set random number generator for reproducible data splits\n",
    "rng = np.random.default_rng(np.random.MT19937(np.random.SeedSequence(710)))\n",
    "\n",
    "# Group annotations by image\n",
    "image_groups = df.groupby('img_name')\n",
    "\n",
    "# Create a dictionary to store the class distribution for each image\n",
    "image_class_distribution = {}\n",
    "\n",
    "# Populate the dictionary with class distributions\n",
    "for image_name, group in image_groups:\n",
    "    labels = group['label'].tolist()\n",
    "    image_class_distribution[image_name] = labels\n",
    "\n",
    "# Create a list of all image names and their corresponding labels\n",
    "all_images = list(image_class_distribution.keys())\n",
    "all_labels = [image_class_distribution[image] for image in all_images]\n",
    "\n",
    "# Use the most frequent label for each image for stratification\n",
    "representative_labels = [max(set(labels), key=labels.count) for labels in all_labels]\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.05\n",
    "\n",
    "# Perform stratified split using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=int(1/test_ratio), shuffle=True, random_state=710)\n",
    "\n",
    "train_val_indices, test_indices = next(skf.split(all_images, representative_labels))\n",
    "\n",
    "# Further split train+val into train and validation sets\n",
    "train_val_images = [all_images[idx] for idx in train_val_indices]\n",
    "train_val_labels = [representative_labels[idx] for idx in train_val_indices]\n",
    "\n",
    "skf_val = StratifiedKFold(n_splits=int(1/(val_ratio/(train_ratio + val_ratio))), shuffle=True, random_state=710)\n",
    "train_indices, val_indices = next(skf_val.split(train_val_images, train_val_labels))\n",
    "\n",
    "# Map image names to unique indices\n",
    "image_to_unique_index = {image: idx for idx, image in enumerate(df['img_name'].unique())}\n",
    "\n",
    "# Create lists of unique indices for each split\n",
    "train_indices = [image_to_unique_index[train_val_images[idx]] for idx in train_indices]\n",
    "val_indices = [image_to_unique_index[train_val_images[idx]] for idx in val_indices]\n",
    "test_indices = [image_to_unique_index[all_images[idx]] for idx in test_indices]\n",
    "\n",
    "# Function to get class distribution\n",
    "def get_class_distribution(images, image_class_distribution):\n",
    "    class_counts = defaultdict(int)\n",
    "    for image in images:\n",
    "        for label in image_class_distribution[image]:\n",
    "            class_counts[label] += 1\n",
    "    return class_counts\n",
    "\n",
    "# Get train, val, and test images\n",
    "train_images = [all_images[idx] for idx in train_indices]\n",
    "val_images = [all_images[idx] for idx in val_indices]\n",
    "test_images = [all_images[idx] for idx in test_indices]\n",
    "\n",
    "train_class_distribution = get_class_distribution(train_images, image_class_distribution)\n",
    "val_class_distribution = get_class_distribution(val_images, image_class_distribution)\n",
    "test_class_distribution = get_class_distribution(test_images, image_class_distribution)\n",
    "\n",
    "class_indices = {label: [] for label in df['label'].unique()}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    class_indices[row['label']].append(idx)\n",
    "\n",
    "train_class_distribution = {k: v / len(class_indices[k]) for k, v in train_class_distribution.items()}\n",
    "val_class_distribution = {k: v / len(class_indices[k]) for k, v in val_class_distribution.items()}\n",
    "test_class_distribution = {k: v / len(class_indices[k]) for k, v in test_class_distribution.items()}\n",
    "\n",
    "print(\"Train class distribution:\", dict(sorted(train_class_distribution.items())))\n",
    "print(\"Validation class distribution:\", dict(sorted(val_class_distribution.items())))\n",
    "print(\"Test class distribution:\", dict(sorted(test_class_distribution.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create weighted random sampler to handle class imbalances during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate class weights dynamically\n",
    "def calculate_class_weights(labels, hen_label_int, background_label_int):\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Remove the \"Hen\" class from the counts\n",
    "    hen_count = class_counts.pop(hen_label_int, None)\n",
    "    \n",
    "    # Identify the count for the second most-frequent class\n",
    "    second_most_frequent_class_count = max(class_counts.values())\n",
    "    \n",
    "    # Calculate the weight for the \"Hen\" class\n",
    "    hen_weight = second_most_frequent_class_count / hen_count if hen_count else 1.0\n",
    "    \n",
    "    # Assign weights to all classes\n",
    "    class_weights = {label: sum(class_counts.values()) / count for label, count in class_counts.items()}\n",
    "    \n",
    "    # Normalize weights to range [1, 2]\n",
    "    min_weight = min(class_weights.values())\n",
    "    max_weight = max(class_weights.values())\n",
    "    class_weights = {label: 1 + (weight - min_weight) / (max_weight - min_weight) for label, weight in class_weights.items()}\n",
    "    \n",
    "    # Add weight for the \"Hen\" class\n",
    "    class_weights[hen_label_int] = hen_weight\n",
    "\n",
    "    # weight = 20% for the background class (penalize for false positives)\n",
    "    class_weights[background_label_int] = 0.2\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Store train labels for each image\n",
    "train_labels = [label for image in train_images for label in image_class_distribution[image]]\n",
    "\n",
    "# Calculate class weights dynamically\n",
    "hen_label_int = [key for key, value in label_dict.items() if value == 'Hen'][0]  # Get the integer label for \"Hen\"\n",
    "background_label_int = 0  # Assuming background is class 0\n",
    "class_weights = calculate_class_weights(train_labels, hen_label_int, background_label_int)\n",
    "\n",
    "# Convert class weights to a list in the correct order\n",
    "unique_labels = sorted(set(train_labels))\n",
    "train_class_weights = [class_weights[label] for label in unique_labels]\n",
    "\n",
    "# Add weight for the background class\n",
    "train_class_weights = [class_weights[background_label_int]] + train_class_weights  # Background is class 0\n",
    "train_class_weights = torch.tensor(train_class_weights, dtype=torch.float32)\n",
    "\n",
    "# print class counts and class weight for each class\n",
    "print(\"Train class instances and weights: \")\n",
    "for label in unique_labels:\n",
    "    print(f\"{label_dict[label]}: count = {train_labels.count(label)}, weight = {train_class_weights[label]}\")\n",
    "\n",
    "\n",
    "# Calculate sample weights for each image in the training dataset\n",
    "train_sample_weights = []\n",
    "for image_name in train_images:\n",
    "    labels = image_class_distribution[image_name]\n",
    "    sample_weight = sum(train_class_weights[label] for label in labels) / len(labels)\n",
    "    train_sample_weights.append(sample_weight)\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(weights=train_sample_weights, num_samples=len(train_sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Anchor Sizes and Aspect Ratios of Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# resized_bounding_boxes = []\n",
    "\n",
    "# for images, targets in sample_data_loader:\n",
    "#     for target in targets:\n",
    "#         for box in target['boxes']:\n",
    "#             resized_bounding_boxes.append(box)\n",
    "\n",
    "# # Convert to numpy array\n",
    "# resized_bounding_boxes = np.array(resized_bounding_boxes)\n",
    "\n",
    "# # Print the resized bounding box dimensions\n",
    "# print(resized_bounding_boxes[:5])\n",
    "\n",
    "# Convert to [width, height] format\n",
    "# widths = resized_bounding_boxes[:, 2] - resized_bounding_boxes[:, 0]\n",
    "# heights = resized_bounding_boxes[:, 3] - resized_bounding_boxes[:, 1]\n",
    "# bounding_boxes_wh = np.stack((widths, heights), axis=1)\n",
    "\n",
    "# # filter out bounding boxes with width or height less than 25\n",
    "# bounding_boxes_wh = bounding_boxes_wh[(bounding_boxes_wh[:, 0] >= 25) & (bounding_boxes_wh[:, 1] >= 25)]\n",
    "\n",
    "# # Perform k-means clustering to find anchor sizes\n",
    "# num_clusters = 5  # Number of anchor sizes\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=710).fit(bounding_boxes_wh)\n",
    "# anchor_sizes = kmeans.cluster_centers_\n",
    "\n",
    "# # Print the anchor sizes\n",
    "# print(\"Anchor Sizes (width, height):\")\n",
    "# print(anchor_sizes)\n",
    "\n",
    "# # Determine aspect ratios from the anchor sizes\n",
    "# anchor_aspect_ratios = anchor_sizes[:, 0] / anchor_sizes[:, 1]\n",
    "\n",
    "# # Print the aspect ratios\n",
    "# print(\"Anchor Aspect Ratios:\")\n",
    "# print(anchor_aspect_ratios)\n",
    "\n",
    "# sorted_widths = np.sort(bounding_boxes_wh[:, 0])\n",
    "# sorted_heights = np.sort(bounding_boxes_wh[:, 1])\n",
    "\n",
    "# print(\"Smallest Widths:\")\n",
    "# print(sorted_widths[:5])\n",
    "# print(\"Largest Widths:\")\n",
    "# print(sorted_widths[-5:])\n",
    "# print(\"Smallest Heights:\")\n",
    "# print(sorted_heights[:5])\n",
    "# print(\"Largest Heights:\")\n",
    "# print(sorted_heights[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Custom RetinaNet with ResNet FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead, RetinaNetRegressionHead\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection._utils import _box_loss\n",
    "from torchvision.ops import sigmoid_focal_loss, FrozenBatchNorm2d\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "\n",
    "def _sum(x: List[torch.Tensor]) -> torch.Tensor:\n",
    "    res = x[0]\n",
    "    for i in x[1:]:\n",
    "        res = res + i\n",
    "    return res\n",
    "\n",
    "\n",
    "class CustomRetinaNetClassificationHead(RetinaNetClassificationHead):\n",
    "    def __init__(self, in_channels, num_anchors, num_classes, alpha=0.25, gamma_loss=2.0, prior_probability=0.01, norm_layer: Optional[Callable[..., nn.Module]] = None, dropout_prob=0.25, class_weights=None):\n",
    "        super().__init__(in_channels, num_anchors, num_classes, prior_probability, norm_layer)\n",
    "        self.alpha = alpha\n",
    "        self.gamma_loss = gamma_loss\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, targets, head_outputs, matched_idxs):\n",
    "        losses = []\n",
    "\n",
    "        cls_logits = head_outputs[\"cls_logits\"]\n",
    "\n",
    "        for i, (targets_per_image, cls_logits_per_image, matched_idxs_per_image) in enumerate(zip(targets, cls_logits, matched_idxs)):\n",
    "            # determine only the foreground\n",
    "            foreground_idxs_per_image = matched_idxs_per_image >= 0\n",
    "            num_foreground = foreground_idxs_per_image.sum()\n",
    "\n",
    "            # create the target classification\n",
    "            gt_classes_target = torch.zeros_like(cls_logits_per_image)\n",
    "            gt_classes_target[\n",
    "                foreground_idxs_per_image,\n",
    "                targets_per_image[\"labels\"][matched_idxs_per_image[foreground_idxs_per_image]],\n",
    "            ] = 1.0\n",
    "\n",
    "            # find indices for which anchors should be ignored\n",
    "            valid_idxs_per_image = matched_idxs_per_image != self.BETWEEN_THRESHOLDS\n",
    "\n",
    "            # get the class weights for the valid indices\n",
    "            if self.class_weights is not None:\n",
    "                valid_labels = targets_per_image[\"labels\"][matched_idxs_per_image[valid_idxs_per_image]]\n",
    "                weights = self.class_weights.to(valid_labels.device)[valid_labels]\n",
    "            else:\n",
    "                weights = torch.ones_like(valid_idxs_per_image, dtype=torch.float32)\n",
    "\n",
    "            # compute the classification loss with custom alpha, gamma_loss, and class weights\n",
    "            losses.append(\n",
    "                (sigmoid_focal_loss(\n",
    "                    cls_logits_per_image[valid_idxs_per_image],\n",
    "                    gt_classes_target[valid_idxs_per_image],\n",
    "                    alpha=self.alpha,\n",
    "                    gamma=self.gamma_loss,\n",
    "                    reduction=\"none\",\n",
    "                ) * weights.unsqueeze(1)).sum() / max(1, num_foreground)\n",
    "            )\n",
    "\n",
    "        return _sum(losses) / len(targets)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_cls_logits = []\n",
    "        for features in x:\n",
    "            cls_logits = self.conv(features)\n",
    "            cls_logits = self.dropout(cls_logits)  # Apply dropout\n",
    "            cls_logits = self.cls_logits(cls_logits)\n",
    "\n",
    "            # Permute classification output from (N, A * K, H, W) to (N, HWA, K).\n",
    "            N, _, H, W = cls_logits.shape\n",
    "            cls_logits = cls_logits.view(N, -1, self.num_classes, H, W)\n",
    "            cls_logits = cls_logits.permute(0, 3, 4, 1, 2)\n",
    "            cls_logits = cls_logits.reshape(N, -1, self.num_classes)  # Size=(N, HWA, K)\n",
    "\n",
    "            all_cls_logits.append(cls_logits)\n",
    "\n",
    "\n",
    "        return torch.cat(all_cls_logits, dim=1)\n",
    "\n",
    "\n",
    "class CustomRetinaNetRegressionHead(RetinaNetRegressionHead):\n",
    "    def __init__(self, in_channels, num_anchors, norm_layer: Optional[Callable[..., nn.Module]] = None, _loss_type=\"smooth_l1\", beta_loss=0.5, lambda_loss=1.5, dropout_prob=0.25):\n",
    "        super().__init__(in_channels, num_anchors, norm_layer)\n",
    "        self._loss_type = _loss_type\n",
    "        self.beta_loss = beta_loss # beta < 1 helps counter early plateauing\n",
    "        self.lambda_loss = lambda_loss # lambda > 1 places more emphasis on localization loss\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "    \n",
    "    def compute_loss(self, targets, head_outputs, anchors, matched_idxs):\n",
    "        # type: (List[Dict[str, torch.Tensor]], Dict[str, torch.Tensor], List[torch.Tensor], List[torch.Tensor]) -> torch.Tensor\n",
    "        losses = []\n",
    "\n",
    "        bbox_regression = head_outputs[\"bbox_regression\"]\n",
    "\n",
    "        for targets_per_image, bbox_regression_per_image, anchors_per_image, matched_idxs_per_image in zip(\n",
    "            targets, bbox_regression, anchors, matched_idxs\n",
    "        ):\n",
    "            # determine only the foreground indices, ignore the rest\n",
    "            foreground_idxs_per_image = torch.where(matched_idxs_per_image >= 0)[0]\n",
    "            num_foreground = foreground_idxs_per_image.numel()\n",
    "\n",
    "            # select only the foreground boxes\n",
    "            matched_gt_boxes_per_image = targets_per_image[\"boxes\"][matched_idxs_per_image[foreground_idxs_per_image]]\n",
    "            bbox_regression_per_image = bbox_regression_per_image[foreground_idxs_per_image, :]\n",
    "            anchors_per_image = anchors_per_image[foreground_idxs_per_image, :]\n",
    "\n",
    "            # compute the loss\n",
    "            losses.append(\n",
    "                _box_loss(\n",
    "                    self._loss_type,\n",
    "                    self.box_coder,\n",
    "                    anchors_per_image,\n",
    "                    matched_gt_boxes_per_image,\n",
    "                    bbox_regression_per_image,\n",
    "                    cnf={'beta': self.beta_loss}, \n",
    "                ) * self.lambda_loss / max(1, num_foreground)\n",
    "            )\n",
    "\n",
    "        return _sum(losses) / max(1, len(targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        all_bbox_regression = []\n",
    "        for features in x:\n",
    "            bbox_regression = self.conv(features)\n",
    "            bbox_regression = self.dropout(bbox_regression)  # Apply dropout\n",
    "            bbox_regression = self.bbox_reg(bbox_regression)\n",
    "\n",
    "            # Permute bbox regression output from (N, 4 * A, H, W) to (N, HWA, 4).\n",
    "            N, _, H, W = bbox_regression.shape\n",
    "            bbox_regression = bbox_regression.view(N, -1, 4, H, W)\n",
    "            bbox_regression = bbox_regression.permute(0, 3, 4, 1, 2)\n",
    "            bbox_regression = bbox_regression.reshape(N, -1, 4)  # Size=(N, HWA, 4)\n",
    "\n",
    "            all_bbox_regression.append(bbox_regression)\n",
    "\n",
    "        return torch.cat(all_bbox_regression, dim=1)\n",
    "    \n",
    "\n",
    "def get_retinanet_model(depth, num_classes=12, min_size=810, max_size=1440, image_mean=[0, 0, 0], image_std=[1, 1, 1], score_thresh=0.1, nms_thresh=0.4, \n",
    "                        detections_per_img=200, fg_iou_thresh=0.4, bg_iou_thresh=0.2, topk_candidates=400, alpha=0.75, gamma_loss=3.0, class_weights=None,\n",
    "                        beta_loss=0.5, lambda_loss=1.5, dropout_prob=0.25):\n",
    "    \n",
    "    trainable_backbone_layers = 0 # set constant, adjust later with function\n",
    "\n",
    "    # Create the backbone with FPN\n",
    "    if depth == 18:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet18', \n",
    "                                       weights=torchvision.models.ResNet18_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 34:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet34', \n",
    "                                       weights=torchvision.models.ResNet34_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 50:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet50', \n",
    "                                       weights=torchvision.models.ResNet50_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 101:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet101', \n",
    "                                       weights=torchvision.models.ResNet101_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 152:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet152', \n",
    "                                       weights=torchvision.models.ResNet152_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model depth\")\n",
    "\n",
    "    # Create the RetinaNet model with the custom backbone\n",
    "    model = RetinaNet(backbone, \n",
    "                      num_classes=num_classes,\n",
    "                      min_size=min_size, # same size as resize in transform to keep aspect ratio\n",
    "                      max_size=max_size,\n",
    "                      image_mean=image_mean,\n",
    "                      image_std=image_std,\n",
    "                      score_thresh=score_thresh, \n",
    "                      nms_thresh=nms_thresh, \n",
    "                      detections_per_img=detections_per_img,\n",
    "                      fg_iou_thresh=fg_iou_thresh,\n",
    "                      bg_iou_thresh=bg_iou_thresh,\n",
    "                      topk_candidates=topk_candidates\n",
    "                      )\n",
    "\n",
    "    # Replace the classification head with the custom one\n",
    "    in_channels = model.head.classification_head.cls_logits.in_channels\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "    model.head.classification_head = CustomRetinaNetClassificationHead(in_channels, \n",
    "                                                                       num_anchors, \n",
    "                                                                       num_classes, \n",
    "                                                                       alpha=alpha, \n",
    "                                                                       gamma_loss=gamma_loss, \n",
    "                                                                       dropout_prob=dropout_prob,\n",
    "                                                                       class_weights=class_weights)\n",
    "\n",
    "    # Replace the regression head with the custom one\n",
    "    model.head.regression_head = CustomRetinaNetRegressionHead(in_channels, \n",
    "                                                               num_anchors, \n",
    "                                                               _loss_type=\"smooth_l1\",\n",
    "                                                               beta_loss=beta_loss,\n",
    "                                                               lambda_loss=lambda_loss,\n",
    "                                                               dropout_prob=dropout_prob)\n",
    "    \n",
    "    model.anchor_generator = AnchorGenerator(sizes=((32, 40, 50), (64, 80, 101), (128, 161, 203), (256, 322, 406), (512, 645, 812)), \n",
    "                                             aspect_ratios=((0.75, 1.25, 1.75), (0.75, 1.25, 1.75), (0.75, 1.25, 1.75), (0.75, 1.25, 1.75), (0.75, 1.25, 1.75)))\n",
    "\n",
    "    return model\n",
    "\n",
    "print(get_retinanet_model(depth=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Tune Model Hyperparameters using Ray Tune**</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class for tuning RetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from datetime import datetime\n",
    "import torch\n",
    "# import torchvision\n",
    "import gc\n",
    "# import tempfile\n",
    "from pathlib import Path\n",
    "import ray.cloudpickle as pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "\n",
    "from engine_gradientAccumulation import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.raise_on_ctx_manager_usage = False\n",
    "torch._dynamo.disable()  # disable dynamo globally\n",
    "\n",
    "# Set random seed for reproducible training\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def extract_per_class_metrics(coco_eval, coco_gt):\n",
    "    per_class_metrics = {}\n",
    "\n",
    "    # Create a list of category IDs in the order they appear in the evaluation results\n",
    "    cat_ids = list(coco_gt.cats.keys())\n",
    "    cat_id_to_index = {cat_id: idx for idx, cat_id in enumerate(cat_ids)}\n",
    "\n",
    "    for cat_id, idx in cat_id_to_index.items():\n",
    "        try:\n",
    "            precision = coco_eval.coco_eval['bbox'].eval['precision'][:, :, idx, 0, 2]\n",
    "            recall = coco_eval.coco_eval['bbox'].eval['recall'][:, idx, 0, 2]\n",
    "\n",
    "            per_class_metrics[cat_id] = {\n",
    "                'precision': precision.mean(),\n",
    "                'recall': recall.mean()\n",
    "            }\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError for category ID {cat_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return per_class_metrics\n",
    "\n",
    "\n",
    "def adjust_trainable_layers(model, trainable_layers):\n",
    "    \"\"\"\n",
    "    Adjust the trainable layers in the RetinaNet backbone (model.backbone.body).\n",
    "    Unfreeze the last `trainable_layers` residual blocks and replace their FrozenBatchNorm2d layers.\n",
    "    When trainable_layers=5, also unfreeze conv1 and replace bn1 with trainable BatchNorm2d.\n",
    "    \"\"\"\n",
    "    def convert_frozen_bn(frozen_bn):\n",
    "        num_features = frozen_bn.weight.shape[0]\n",
    "        bn = torch.nn.BatchNorm2d(num_features)\n",
    "        \n",
    "        # Initialize with existing batch norm parameters\n",
    "        bn.running_mean = frozen_bn.running_mean.clone()\n",
    "        bn.running_var = frozen_bn.running_var.clone()\n",
    "        torch.nn.init.normal_(bn.weight, mean=1.0, std=0.02)\n",
    "        torch.nn.init.constant_(bn.bias, 0)\n",
    "        \n",
    "        return bn\n",
    "\n",
    "    # Collect backbone blocks\n",
    "    backbone_layers = []\n",
    "    for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        if hasattr(model.backbone.body, layer_name):\n",
    "            backbone_layers.append(getattr(model.backbone.body, layer_name))\n",
    "\n",
    "    if trainable_layers > 5:\n",
    "        print(f\"Requested trainable_layers ({trainable_layers}) exceeds available layers (5). Using 5 instead.\")\n",
    "        trainable_layers = 5\n",
    "\n",
    "    # Unfreeze the last `trainable_layers` blocks\n",
    "    for block in backbone_layers[-trainable_layers:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Replace FrozenBatchNorm2d in backbone layers\n",
    "    for name, module in model.backbone.body.named_modules():\n",
    "        if isinstance(module, FrozenBatchNorm2d):\n",
    "            if 'layer' in name:\n",
    "                layer_num = int(name.split('.')[0][-1])\n",
    "                if layer_num > (4 - trainable_layers):\n",
    "                    parent_name = '.'.join(name.split('.')[:-1])\n",
    "                    module_name = name.split('.')[-1]\n",
    "                    parent = dict(model.backbone.body.named_modules())[parent_name]\n",
    "                    setattr(parent, module_name, convert_frozen_bn(module))\n",
    "            elif trainable_layers == 5 and name == 'bn1':\n",
    "                # Replace the initial frozen batch norm layer\n",
    "                model.backbone.body.bn1 = convert_frozen_bn(module)\n",
    "                # Ensure conv1 is trainable\n",
    "                model.backbone.body.conv1.weight.requires_grad = True\n",
    "\n",
    "\n",
    "class RetinaNetTuner:\n",
    "    def __init__(self, num_samples, restore_path=\"\"):\n",
    "        self.num_samples = num_samples\n",
    "        self.restore_path = restore_path\n",
    "\n",
    "    def create_coco_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            train_future = executor.submit(get_coco_api_from_dataset, train_dataset)\n",
    "            val_future = executor.submit(get_coco_api_from_dataset, val_dataset)\n",
    "            test_future = executor.submit(get_coco_api_from_dataset, test_dataset)\n",
    "            train_coco_ds = train_future.result()\n",
    "            val_coco_ds = val_future.result()\n",
    "            test_coco_ds = test_future.result()\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds\n",
    "    \n",
    "    def train_lr_finder(self, config):\n",
    "        class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "            def inputs_labels_from_batch(self, batch_data):\n",
    "                inputs = [image.to('cuda:0') for image in batch_data[0]]\n",
    "                labels = [{k: v.to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "                return inputs, labels\n",
    "\n",
    "        dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "        accumulation_steps = 1  ## FIXME: hardcoded for now\n",
    "\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=config[\"batch_size\"],\n",
    "                                                        sampler=config[\"train_sampler\"],\n",
    "                                                        collate_fn=utils.collate_fn,\n",
    "                                                        num_workers=0, pin_memory=True)\n",
    "\n",
    "        model = get_retinanet_model(\n",
    "            depth=50,\n",
    "            num_classes=len(config[\"class_weights\"]),\n",
    "            score_thresh=config[\"score_thresh\"],\n",
    "            nms_thresh=config[\"nms_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thres=config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=400,\n",
    "            alpha=config[\"alpha\"],\n",
    "            gamma_loss=config[\"gamma_loss\"],\n",
    "            class_weights=config[\"class_weights\"],\n",
    "            beta_loss=config[\"beta_loss\"],\n",
    "            lambda_loss=config[\"lambda_loss\"],\n",
    "            dropout_prob=config[\"dropout\"],\n",
    "        ).to('cuda:0')\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params, lr=1e-7, momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "        grad_scaler = torch.GradScaler()\n",
    "\n",
    "        class CustomLRFinder(LRFinder):\n",
    "            def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "                super().__init__(model, optimizer, criterion, device)\n",
    "                self.amp_backend = amp_backend\n",
    "                self.amp_config = amp_config\n",
    "                self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "            def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                for _ in range(accumulation_steps):\n",
    "                    inputs, labels = next(train_iter)\n",
    "                    inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                    with torch.autocast(device_type=\"cuda:0\"):\n",
    "                        outputs = self.model(inputs, labels)\n",
    "                        loss = sum(loss for loss in outputs.values())\n",
    "\n",
    "                    loss /= accumulation_steps\n",
    "                    self.grad_scaler.scale(loss).backward()\n",
    "                    total_loss += loss\n",
    "\n",
    "                self.grad_scaler.step(self.optimizer)\n",
    "                self.grad_scaler.update()\n",
    "\n",
    "                return total_loss.item()\n",
    "\n",
    "        lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "        lr_finder.range_test(train_iter, end_lr=1, num_iter=len(data_loader_train), step_mode='exp', accumulation_steps=accumulation_steps) # num_iter = len(dataloader) to sample from full train dataset\n",
    "        suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "        lr_finder.reset()\n",
    "\n",
    "        # return default if torch lr finder fails\n",
    "        try:\n",
    "            if isinstance(suggested_lr, tuple):\n",
    "                axes, suggested_lr_value = suggested_lr\n",
    "                return suggested_lr_value\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during learning rate finding: {e}\")\n",
    "            # Return a default learning rate if an error occurs\n",
    "            return 5e-4\n",
    "    \n",
    "    def train_MAVdroneDataset(self, config):\n",
    "        import pickle, tempfile\n",
    "        from pathlib import Path\n",
    "        set_seed(710)\n",
    "\n",
    "        dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "        data_loader_val = ray.get(config[\"data_loader_val_ref\"])\n",
    "        train_coco_ds = ray.get(config[\"train_coco_ds_ref\"])\n",
    "        val_coco_ds = ray.get(config[\"val_coco_ds_ref\"])\n",
    "\n",
    "        training_steps = [\n",
    "            {\"step\": 0, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 1, \"backbone_layers\": 0},\n",
    "            # {\"step\": 1, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 2, \"backbone_layers\": 1},\n",
    "            # {\"step\": 2, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 4, \"backbone_layers\": 2},\n",
    "            # {\"step\": 3, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 8, \"backbone_layers\": 3},\n",
    "            # {\"step\": 4, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 16, \"backbone_layers\": 4},\n",
    "            # {\"step\": 5, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 32, \"backbone_layers\": 5}\n",
    "        ]\n",
    "\n",
    "        # Instantiate model and optimizer only once\n",
    "        model = get_retinanet_model(\n",
    "            depth=50,\n",
    "            num_classes=len(config[\"class_weights\"]),\n",
    "            score_thresh=config[\"score_thresh\"],\n",
    "            nms_thresh=config[\"nms_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=400,\n",
    "            alpha=config[\"alpha\"],\n",
    "            gamma_loss=config[\"gamma_loss\"],\n",
    "            class_weights=config[\"class_weights\"],\n",
    "            beta_loss=config[\"beta_loss\"],\n",
    "            lambda_loss=config[\"lambda_loss\"],\n",
    "            dropout_prob=config[\"dropout\"],\n",
    "        )\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=config[\"lr\"],\n",
    "                                    momentum=config[\"momentum\"],\n",
    "                                    weight_decay=config[\"weight_decay\"],\n",
    "                                    nesterov=True)\n",
    "        \n",
    "        # Check for an existing checkpoint and load state if available.\n",
    "        checkpoint = train.get_checkpoint()\n",
    "        if checkpoint:\n",
    "            with checkpoint.as_directory() as checkpoint_dir:\n",
    "                data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                with open(data_path, \"rb\") as fp:\n",
    "                    checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"] + 1\n",
    "            current_step = checkpoint_state[\"current_step\"]\n",
    "            # Load model and optimizer state.\n",
    "            model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "            if checkpoint_state[\"current_step\"] == current_step:\n",
    "                optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            current_step = 0\n",
    "\n",
    "        while current_step < len(training_steps):\n",
    "            ts = training_steps[current_step]\n",
    "            batch_size = ts[\"batch_size\"]\n",
    "            print_freq = ts[\"print_freq\"]\n",
    "            accumulation_steps = ts[\"accumulation_steps\"]\n",
    "            backbone_layers = ts[\"backbone_layers\"]\n",
    "\n",
    "            scaled_lr = config[\"lr\"] * (batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps\n",
    "\n",
    "            # Adjust the trainable layers if needed.\n",
    "            adjust_trainable_layers(model, backbone_layers)\n",
    "\n",
    "            # Update optimizer parameter groups to include only the updated parameters.\n",
    "            for group in optimizer.param_groups:\n",
    "                group['params'] = [p for p in model.parameters() if p.requires_grad]\n",
    "                group['lr'] = scaled_lr\n",
    "\n",
    "            data_loader = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=batch_size,\n",
    "                sampler=config[\"train_sampler\"],\n",
    "                collate_fn=utils.collate_fn,\n",
    "                num_workers=0, pin_memory=True\n",
    "            )\n",
    "\n",
    "            print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}')\n",
    "            print()\n",
    "\n",
    "            # Plateau detection variables.\n",
    "            window_loss = []\n",
    "            window_size = 5\n",
    "            minimum_epochs = 20  # Do not check plateau until these many epochs have passed.\n",
    "            step_epoch_counter = 0\n",
    "\n",
    "            # Early stopping parameters.\n",
    "            alpha = 0.1\n",
    "            relative_improvement_threshold = 0.01  # 1% improvement required.\n",
    "            patience = 3\n",
    "            variance_threshold = 1e-3\n",
    "\n",
    "            ema_loss = None\n",
    "            non_improving_counter = 0\n",
    "\n",
    "            while True:\n",
    "                step_epoch_counter += 1\n",
    "                print(f\"Epoch {start_epoch}, Step: {ts['step']}, Memory: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "                print()\n",
    "\n",
    "                train_metric_logger, val_metric_logger = train_one_epoch(\n",
    "                    model, optimizer, data_loader, device,\n",
    "                    start_epoch, print_freq, accumulation_steps, data_loader_val\n",
    "                )\n",
    "                print()\n",
    "\n",
    "                train_coco_evaluator, val_coco_evaluator = evaluate(\n",
    "                    model, data_loader_val, val_coco_ds, device, data_loader, train_coco_ds\n",
    "                )\n",
    "                print()\n",
    "\n",
    "                train_class_metrics = extract_per_class_metrics(train_coco_evaluator, train_coco_ds)\n",
    "                val_class_metrics = extract_per_class_metrics(val_coco_evaluator, val_coco_ds)\n",
    "                train_class_metrics = {label_dict[k]: v for k, v in train_class_metrics.items()}\n",
    "                val_class_metrics = {label_dict[k]: v for k, v in val_class_metrics.items()}\n",
    "\n",
    "                print(\"Training Class Metrics:\")\n",
    "                for name, m in train_class_metrics.items():\n",
    "                    print(f\"Class: {name}, Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}\")\n",
    "                print(\"\\nValidation Class Metrics:\")\n",
    "                for name, m in val_class_metrics.items():\n",
    "                    print(f\"Class: {name}, Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}\")\n",
    "                print()\n",
    "\n",
    "                current_loss = val_metric_logger.loss.avg\n",
    "                window_loss.append(current_loss)\n",
    "                if len(window_loss) > window_size:\n",
    "                    window_loss.pop(0)\n",
    "                               \n",
    "                # Filter optimizer state to only include currently trainable parameters\n",
    "                current_model_params = {id(p) for p in model.parameters() if p.requires_grad}\n",
    "                optimizer_state = optimizer.state_dict()\n",
    "                optimizer_state[\"state\"] = {k: v for k, v in optimizer_state[\"state\"].items() if k in current_model_params}\n",
    "                optimizer_state[\"param_groups\"] = [{k: v for k, v in pg.items() if k != \"params\"} for pg in optimizer_state[\"param_groups\"]] \n",
    "                \n",
    "                checkpoint_data = {\n",
    "                    \"epoch\": start_epoch,\n",
    "                    \"current_step\": current_step,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                }\n",
    "\n",
    "                with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                    data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                    with open(data_path, \"wb\") as fp:\n",
    "                        pickle.dump(checkpoint_data, fp)\n",
    "                    train.report(\n",
    "                        {\"epoch\": start_epoch,\n",
    "                        \"current_step\": current_step,\n",
    "                        \"train_loss\": train_metric_logger.loss.avg,\n",
    "                        \"val_loss\": val_metric_logger.loss.avg,\n",
    "                        \"train_mAP\": train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                        \"val_mAP\": val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                        \"train_mAR\": train_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                        \"val_mAR\": val_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                        \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                        train_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                        \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                    val_coco_evaluator.coco_eval['bbox'].stats[8])},\n",
    "                        checkpoint=train.Checkpoint.from_directory(checkpoint_dir),\n",
    "                    )\n",
    "\n",
    "                print(f\"Epoch {start_epoch}: Current Loss = {current_loss:.4f},\", end=\" \")\n",
    "\n",
    "                # Early stopping check, but metrics are reported every epoch.\n",
    "                if step_epoch_counter >= minimum_epochs and len(window_loss) == window_size:\n",
    "                    if ema_loss is None:\n",
    "                        ema_loss = current_loss\n",
    "                        relative_improvement = 1.0\n",
    "                    else:\n",
    "                        prev_ema = ema_loss\n",
    "                        ema_loss = alpha * current_loss + (1 - alpha) * prev_ema\n",
    "                        relative_improvement = (prev_ema - ema_loss) / prev_ema\n",
    "                        if relative_improvement < relative_improvement_threshold:\n",
    "                            non_improving_counter += 1\n",
    "                        else:\n",
    "                            non_improving_counter = 0\n",
    "\n",
    "                    loss_variance = np.var(window_loss)\n",
    "                    print(f\"EMA Loss = {ema_loss:.4f}, Relative Improvement = {relative_improvement:.4f},\", end=\" \")\n",
    "                    print(f\"Variance = {loss_variance:.6f}, Non-improvement Count = {non_improving_counter}\")\n",
    "                    should_break = (non_improving_counter >= patience) or (loss_variance < variance_threshold)\n",
    "                else:\n",
    "                    should_break = False\n",
    "                    print(\"\")\n",
    "\n",
    "                start_epoch += 1\n",
    "\n",
    "                if should_break:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    print(\"Plateau reached; moving to next training step.\\n\")\n",
    "                    break\n",
    "\n",
    "            current_step += 1\n",
    "\n",
    "        print('Tuning Trial Complete!')\n",
    "\n",
    "    def trial_dirname_creator(self, trial):\n",
    "        return f\"{trial.trial_id}\"\n",
    "\n",
    "    def run(self):\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "\n",
    "        dataset = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=True)\n",
    "        )\n",
    "\n",
    "        dataset_val = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_test = MAVdroneDataset(\n",
    "            csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "            root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "            transforms=get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_train = torch.utils.data.Subset(dataset, train_indices)\n",
    "        dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "        dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "            dataset_val, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        data_loader_test = torch.utils.data.DataLoader(\n",
    "            dataset_test, batch_size=1, shuffle=False,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        train_coco_ds, val_coco_ds, test_coco_ds = self.create_coco_datasets(dataset_train, dataset_val, dataset_test)\n",
    "\n",
    "        dataset_train_ref = ray.put(dataset_train)\n",
    "        data_loader_val_ref = ray.put(data_loader_val)\n",
    "        data_loader_test_ref = ray.put(data_loader_test)\n",
    "        train_coco_ds_ref = ray.put(train_coco_ds)\n",
    "        val_coco_ds_ref = ray.put(val_coco_ds)\n",
    "        test_coco_ds_ref = ray.put(test_coco_ds)\n",
    "\n",
    "        config = {\n",
    "            # \"lr\": tune.sample_from(lambda config: self.train_lr_finder(config)),\n",
    "            \"lr\": tune.loguniform(0.00005, 0.005),\n",
    "            \"momentum\": tune.uniform(0.8, 0.99),\n",
    "            \"weight_decay\": tune.loguniform(0.0001, 0.005),\n",
    "            \"alpha\": tune.uniform(0.5, 0.9),\n",
    "            \"gamma_loss\": tune.uniform(2.5, 4.5),\n",
    "            \"dropout\": tune.uniform(0.1, 0.5),\n",
    "            \"score_thresh\": tune.uniform(0.5, 0.8),\n",
    "            \"nms_thresh\": tune.uniform(0.1, 0.3),\n",
    "            \"fg_iou_thresh\": tune.uniform(0.4, 0.7),\n",
    "            \"bg_iou_thresh\": tune.uniform(0.1, 0.4),\n",
    "            \"beta_loss\": tune.uniform(0.25, 0.75),\n",
    "            \"lambda_loss\": tune.uniform(1.5, 2.0),\n",
    "            \"dataset_train_ref\": dataset_train_ref,\n",
    "            \"data_loader_val_ref\": data_loader_val_ref,\n",
    "            \"data_loader_test_ref\": data_loader_test_ref,\n",
    "            \"train_coco_ds_ref\": train_coco_ds_ref,\n",
    "            \"val_coco_ds_ref\": val_coco_ds_ref,\n",
    "            \"test_coco_ds_ref\": test_coco_ds_ref,\n",
    "            \"train_sampler\": train_sampler,\n",
    "            \"class_weights\": train_class_weights\n",
    "        }\n",
    "\n",
    "        if tune.Tuner.can_restore(os.path.abspath(self.restore_path)):\n",
    "            tuner = tune.Tuner.restore(\n",
    "                os.path.abspath(self.restore_path),\n",
    "                trainable=self.train_MAVdroneDataset,\n",
    "                param_space=config,\n",
    "                resume_unfinished=True,\n",
    "                resume_errored=False\n",
    "            )\n",
    "            print(f\"Tuner Restored from {self.restore_path}\")\n",
    "        else:\n",
    "            algo = TuneBOHB(\n",
    "                seed=710\n",
    "            )\n",
    "\n",
    "            algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "\n",
    "            scheduler = HyperBandForBOHB(\n",
    "                time_attr=\"training_iteration\",\n",
    "                reduction_factor=4,\n",
    "                stop_last_trials=False,\n",
    "            )\n",
    "\n",
    "            reporter = tune.JupyterNotebookReporter(overwrite=True,\n",
    "                metric_columns=[\"epoch\", \"current_step\", \"train_loss\", \"val_loss\", \"train_mAP\", \"val_mAP\", \"train_mAR\", \"val_mAR\", \"train_f1\", \"val_f1\"],\n",
    "                parameter_columns=[\"lr\", \"momentum\", \"weight_decay\", \"alpha\", \"gamma_loss\", \"dropout\", \"score_thresh\", \"nms_thresh\", \"fg_iou_thresh\", \"bg_iou_thresh\", \"beta_loss\", \"lambda_loss\"],\n",
    "                print_intermediate_tables=True,\n",
    "                sort_by_metric=True\n",
    "            )\n",
    "\n",
    "            tuner = tune.Tuner(\n",
    "                tune.with_resources(\n",
    "                    self.train_MAVdroneDataset,\n",
    "                    resources={\"cpu\": 36.0, \"gpu\": 1.0}\n",
    "                ),\n",
    "                run_config=train.RunConfig(\n",
    "                    name=f\"BOHB_RetinaNet_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                    failure_config=train.FailureConfig(max_failures=1),\n",
    "                    progress_reporter=reporter,\n",
    "                ),\n",
    "                tune_config=tune.TuneConfig(\n",
    "                    mode=\"min\",\n",
    "                    metric=\"val_loss\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    num_samples=int(self.num_samples),\n",
    "                    trial_dirname_creator=self.trial_dirname_creator\n",
    "                ),\n",
    "                param_space=config\n",
    "            )\n",
    "        results = tuner.fit()\n",
    "\n",
    "        best_trial = results.get_best_result(\"val_f1\", \"max\")\n",
    "\n",
    "        print(\"Best trial config: {}\".format(best_trial.config))\n",
    "        print()\n",
    "        print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "        print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "        print(\"Best trial final training mAP: {}\".format(best_trial.metrics[\"train_mAP\"]))\n",
    "        print(\"Best trial final validation mAP: {}\".format(best_trial.metrics[\"val_mAP\"]))\n",
    "        print(\"Best trial final training mAR: {}\".format(best_trial.metrics[\"train_mAR\"]))\n",
    "        print(\"Best trial final validation mAR: {}\".format(best_trial.metrics[\"val_mAR\"]))\n",
    "        print(\"Best trial final training f1-score: {}\".format(best_trial.metrics[\"train_f1\"]))\n",
    "        print(\"Best trial final validation f1-score: {}\".format(best_trial.metrics[\"val_f1\"]))\n",
    "        \n",
    "        print()\n",
    "\n",
    "        best_checkpoint = best_trial.get_best_checkpoint(metric=\"val_f1\", mode=\"max\")\n",
    "\n",
    "        self.test_best_model(best_trial, best_checkpoint)\n",
    "\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial\n",
    "\n",
    "    def test_best_model(self, best_trial, best_checkpoint):\n",
    "        best_model = get_retinanet_model(depth=50,\n",
    "                                         num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                                         score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                                         nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                                         detections_per_img=200,\n",
    "                                         fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "                                         bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "                                         topk_candidates=400,\n",
    "                                         alpha=best_trial.config[\"alpha\"],\n",
    "                                         gamma_loss=best_trial.config[\"gamma_loss\"],\n",
    "                                         class_weights=None,\n",
    "                                         beta_loss=best_trial.config[\"beta_loss\"],\n",
    "                                         lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "                                         dropout_prob=best_trial.config[\"dropout\"])\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        best_model.to(device)\n",
    "\n",
    "        with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                best_checkpoint_data = pickle.load(fp)\n",
    "            best_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "\n",
    "        data_loader_test = ray.get(best_trial.config[\"data_loader_test_ref\"])\n",
    "        test_coco_ds = ray.get(best_trial.config[\"test_coco_ds_ref\"])\n",
    "\n",
    "        test_results = evaluate(best_model, data_loader_test, test_coco_ds, device, train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "        print(f'Best trial test set mAP: {test_results.coco_eval[\"bbox\"].stats[0]}')\n",
    "        print(f'Best trial test set mAR: {test_results.coco_eval[\"bbox\"].stats[8]}')\n",
    "        print(f'Best trial test set f1-score: {calculate_f1_score(test_results.coco_eval[\"bbox\"].stats[0], test_results.coco_eval[\"bbox\"].stats[8])}')\n",
    "\n",
    "        # Get per-class metrics\n",
    "        test_class_metrics = extract_per_class_metrics(test_results, test_coco_ds)\n",
    "\n",
    "        test_class_metrics = {label_dict[k]: v for k, v in test_class_metrics.items()}\n",
    "\n",
    "        print(\"Test Set Class Metrics:\")\n",
    "        for class_name, metrics in test_class_metrics.items():\n",
    "            print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "        print()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     trainer = RetinaNetTuner(num_samples=50, restore_path=\"C:/Users/exx/ray_results/FALSE\")\n",
    "#     train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestTrial:\n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            \"lr\": 0.002,\n",
    "            \"momentum\": 0.90,\n",
    "            \"weight_decay\": 0.002,\n",
    "            \"alpha\": 0.75,\n",
    "            \"gamma_loss\": 4.0,\n",
    "            \"dropout\": 0.25,\n",
    "            \"score_thresh\": 0.50,\n",
    "            \"nms_thresh\": 0.15,\n",
    "            \"fg_iou_thresh\": 0.50,\n",
    "            \"bg_iou_thresh\": 0.25,\n",
    "            \"beta_loss\": 0.50,\n",
    "            \"lambda_loss\": 2.0,\n",
    "            \"class_weights\": train_class_weights\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_trial = BestTrial()\n",
    "    print(\"Best trial config:\")\n",
    "    for key, value in best_trial.config.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Train Model Using Tuned Hyperparameters**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "\n",
    "def visualize_predictions(model, data_loader, device, epoch, num_samples=2):\n",
    "    \"\"\"\n",
    "    Visualize object detection predictions.\n",
    "    Shows input image, ground truth boxes, and predicted boxes with labels.\n",
    "    Reverses ImageNet normalization before plotting.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Define ImageNet normalization parameters.\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    def denormalize(img_tensor):\n",
    "        \"\"\"Revert ImageNet normalization.\"\"\"\n",
    "        # img_tensor is assumed to be a tensor with shape (C, H, W)\n",
    "        img = img_tensor.clone().cpu().numpy().transpose(1, 2, 0)\n",
    "        img = img * imagenet_std + imagenet_mean\n",
    "        return np.clip(img, 0, 1)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for b in range(len(images)):\n",
    "                # Denoormalize the image\n",
    "                img = denormalize(images[b])\n",
    "                \n",
    "                # Get ground truth boxes and labels\n",
    "                gt_boxes = targets[b]['boxes'].cpu().numpy()\n",
    "                gt_labels = targets[b]['labels'].cpu().numpy()\n",
    "                \n",
    "                # Get predicted boxes, labels, and scores\n",
    "                pred_boxes = outputs[b]['boxes'].cpu().numpy()\n",
    "                pred_labels = outputs[b]['labels'].cpu().numpy()\n",
    "                pred_scores = outputs[b]['scores'].cpu().numpy()\n",
    "                \n",
    "                # Create figure\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "                \n",
    "                # Plot original image with ground truth boxes\n",
    "                ax1.imshow(img)\n",
    "                for j in range(len(gt_boxes)):\n",
    "                    box = get_box(gt_boxes[j])\n",
    "                    plot_bbox(ax1, box, gt_labels[j])\n",
    "                ax1.set_title(f'Ground Truth\\nEpoch {epoch}, Batch {i}')\n",
    "                ax1.axis('off')\n",
    "                \n",
    "                # Plot original image with predicted boxes\n",
    "                ax2.imshow(img)\n",
    "                for j in range(len(pred_boxes)):\n",
    "                    box = get_box(pred_boxes[j])\n",
    "                    plot_bbox(ax2, box, pred_labels[j])\n",
    "                    ax2.text(box[2], box[3], f'{pred_scores[j]:.2f}', fontsize=8, color='red')\n",
    "                ax2.set_title(f'Predictions\\nEpoch {epoch}, Batch {i}')\n",
    "                ax2.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "# def main(train_coco_ds, val_coco_ds, best_trial):\n",
    "def main(best_trial):\n",
    "    set_seed(710)\n",
    "\n",
    "    print(best_trial.config)\n",
    "    print()\n",
    "\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 1, \"trainable_layers\": 0}, # bs 16\n",
    "        {\"step\": 1, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 2, \"trainable_layers\": 1}, # bs 32\n",
    "        {\"step\": 2, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 4, \"trainable_layers\": 2}, # bs 64\n",
    "        {\"step\": 3, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 8, \"trainable_layers\": 3}, # bs 128\n",
    "        {\"step\": 4, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 16, \"trainable_layers\": 4}, # bs 256\n",
    "        {\"step\": 5, \"batch_size\": 16, \"print_freq\": 10, \"accumulation_steps\": 32, \"trainable_layers\": 5}, # bs 512\n",
    "    ]\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f'C:/Users/exx/Documents/GitHub/SSD_VGG_PyTorch/runs/RetinaNet/{current_datetime}')\n",
    "    checkpoints = []\n",
    "\n",
    "    dataset = MAVdroneDataset(\n",
    "        csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "        root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "        transforms=get_transform(train=True)\n",
    "    )\n",
    "    dataset_val = MAVdroneDataset(\n",
    "        csv_file='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "        root_dir='C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/',\n",
    "        transforms=get_transform(train=False)\n",
    "    )\n",
    "    \n",
    "    dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "    data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False,\n",
    "                                                  collate_fn=utils.collate_fn, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    train_coco_ds = get_coco_api_from_dataset(dataset)\n",
    "    val_coco_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "    model = get_retinanet_model(\n",
    "            depth=50,\n",
    "            num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "            score_thresh=best_trial.config[\"score_thresh\"],\n",
    "            nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "            detections_per_img=200,\n",
    "            fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "            bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "            topk_candidates=400, \n",
    "            alpha=best_trial.config[\"alpha\"], \n",
    "            gamma_loss=best_trial.config[\"gamma_loss\"],\n",
    "            dropout_prob=best_trial.config[\"dropout\"],\n",
    "            beta_loss=best_trial.config[\"beta_loss\"],\n",
    "            lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "            class_weights=best_trial.config[\"class_weights\"]\n",
    "        )\n",
    "\n",
    "    model.to(device)\n",
    "        \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=best_trial.config[\"lr\"],\n",
    "                                    momentum=best_trial.config[\"momentum\"],\n",
    "                                    weight_decay=best_trial.config[\"weight_decay\"],\n",
    "                                    nesterov=True)\n",
    "\n",
    "    start_epoch, step_index = 0, 0\n",
    "\n",
    "    while step_index < len(training_steps):\n",
    "        ts = training_steps[step_index]\n",
    "        batch_size, print_freq, accumulation_steps = ts[\"batch_size\"], ts[\"print_freq\"], ts[\"accumulation_steps\"]\n",
    "        scaled_lr = best_trial.config[\"lr\"] * (batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps\n",
    "\n",
    "        # unfreeze backbone and batch norm layers\n",
    "        adjust_trainable_layers(model, ts[\"trainable_layers\"])\n",
    "\n",
    "        # Update optimizer parameter groups to include only the parameters that require gradients\n",
    "        for group in optimizer.param_groups:\n",
    "            group['params'] = [p for p in model.parameters() if p.requires_grad]\n",
    "            group['lr'] = scaled_lr  # update learning rate if necessary\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=True,\n",
    "            collate_fn=utils.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}\\n')\n",
    "        \n",
    "        # Plateau detection variables\n",
    "        window_loss = []\n",
    "        window_size = 5\n",
    "        minimum_epochs = 15  # Do not check plateau until these many epochs\n",
    "        step_epoch_counter = 0\n",
    "\n",
    "        # Early stopping variables\n",
    "        alpha = 0.1\n",
    "        relative_improvement_threshold = 0.01  # 1% improvement required\n",
    "        patience = 3\n",
    "        variance_threshold = 1e-3\n",
    "\n",
    "        ema_loss = None\n",
    "        non_improving_counter = 0\n",
    "\n",
    "        while True:\n",
    "            step_epoch_counter += 1\n",
    "            print(f'Epoch {start_epoch}, Step: {ts[\"step\"]}, Memory: {torch.cuda.memory_allocated(device)} bytes')\n",
    "            print()\n",
    "            \n",
    "            train_metric_logger, val_metric_logger = train_one_epoch(\n",
    "                model, optimizer, data_loader, device, \n",
    "                start_epoch, print_freq, accumulation_steps, data_loader_val\n",
    "            )\n",
    "            print()\n",
    "            train_coco_evaluator, val_coco_evaluator = evaluate(\n",
    "                model, data_loader_val, val_coco_ds, device, data_loader, train_coco_ds\n",
    "            )\n",
    "            print()\n",
    "\n",
    "            train_class_metrics = extract_per_class_metrics(train_coco_evaluator, train_coco_ds)\n",
    "            val_class_metrics = extract_per_class_metrics(val_coco_evaluator, val_coco_ds)\n",
    "            train_class_metrics = {label_dict[k]: v for k, v in train_class_metrics.items()}\n",
    "            val_class_metrics = {label_dict[k]: v for k, v in val_class_metrics.items()}\n",
    "\n",
    "            print(\"Training Class Metrics:\")\n",
    "            for name, m in train_class_metrics.items():\n",
    "                print(f\"Class: {name}, Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}\")\n",
    "            print(\"\\nValidation Class Metrics:\")\n",
    "            for name, m in val_class_metrics.items():\n",
    "                print(f\"Class: {name}, Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}\")\n",
    "            print()\n",
    "\n",
    "            # Add visualization for monitoring validation predictions\n",
    "            if start_epoch % 5 == 0:  # Visualize every 5 epochs\n",
    "                visualize_predictions(model, data_loader_val, device, start_epoch, num_samples=2)\n",
    "\n",
    "            current_loss = val_metric_logger.loss.avg\n",
    "            window_loss.append(current_loss)\n",
    "            if len(window_loss) > window_size:\n",
    "                window_loss.pop(0)\n",
    "\n",
    "            # Filter optimizer state to only include currently trainable parameters\n",
    "            current_model_params = {id(p) for p in model.parameters() if p.requires_grad}\n",
    "            optimizer_state = optimizer.state_dict()\n",
    "            optimizer_state[\"state\"] = {k: v for k, v in optimizer_state[\"state\"].items() if k in current_model_params}\n",
    "            optimizer_state[\"param_groups\"] = [{k: v for k, v in pg.items() if k != \"params\"} for pg in optimizer_state[\"param_groups\"]]    \n",
    "\n",
    "            checkpoint = {\n",
    "                \"epoch\": start_epoch,\n",
    "                \"current_step\": ts[\"step\"],\n",
    "                \"train_loss\": train_metric_logger.loss.avg,\n",
    "                \"val_loss\": val_metric_logger.loss.avg,\n",
    "                \"train_bbox_loss\": train_metric_logger.bbox_regression.avg,\n",
    "                \"val_bbox_loss\": val_metric_logger.bbox_regression.avg,\n",
    "                \"train_class_loss\": train_metric_logger.classification.avg,\n",
    "                \"val_class_loss\": val_metric_logger.classification.avg,\n",
    "                \"train_mAP\": train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                \"train_mAR\": train_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                \"val_mAP\": val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                \"val_mAR\": val_coco_evaluator.coco_eval['bbox'].stats[8],\n",
    "                \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                train_coco_evaluator.coco_eval['bbox'].stats[8]),\n",
    "                \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['bbox'].stats[0],\n",
    "                                                    val_coco_evaluator.coco_eval['bbox'].stats[8])\n",
    "            }\n",
    "            # if last training step, add optimizer and model state to checkpoint. \n",
    "            if step_index == len(training_steps) - 1:\n",
    "                checkpoint[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
    "                checkpoint[\"model_state_dict\"] = model.state_dict()\n",
    "\n",
    "            checkpoints.append(checkpoint)\n",
    "            writer.add_scalar('Loss/Train', float(checkpoint[\"train_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Loss/Val', float(checkpoint[\"val_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Box Loss/Train', float(checkpoint[\"train_bbox_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Box Loss/Val', float(checkpoint[\"val_bbox_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Class Loss/Train', float(checkpoint[\"train_class_loss\"]), start_epoch)\n",
    "            writer.add_scalar('Class Loss/Val', float(checkpoint[\"val_class_loss\"]), start_epoch)\n",
    "            writer.add_scalar('mAP/Train', float(checkpoint[\"train_mAP\"]), start_epoch)\n",
    "            writer.add_scalar('mAP/Val', float(checkpoint[\"val_mAP\"]), start_epoch)\n",
    "            writer.add_scalar('mAR/Train', float(checkpoint[\"train_mAR\"]), start_epoch)\n",
    "            writer.add_scalar('mAR/Val', float(checkpoint[\"val_mAR\"]), start_epoch)\n",
    "            writer.add_scalar('F1/Train', float(checkpoint[\"train_f1\"]), start_epoch)\n",
    "            writer.add_scalar('F1/Val', float(checkpoint[\"val_f1\"]), start_epoch)\n",
    "\n",
    "            print(f\"Epoch {start_epoch}: Current Loss = {current_loss:.4f},\", end=\" \")\n",
    "\n",
    "            # Only check plateau after minimum epochs and full loss window are stored\n",
    "            if step_epoch_counter >= minimum_epochs and len(window_loss) == window_size:\n",
    "                if ema_loss is None:\n",
    "                    ema_loss = current_loss\n",
    "                    relative_improvement = 1.0  # starting value\n",
    "                else:\n",
    "                    prev_ema = ema_loss\n",
    "                    ema_loss = alpha * current_loss + (1 - alpha) * prev_ema\n",
    "                    relative_improvement = (prev_ema - ema_loss) / prev_ema\n",
    "                    if relative_improvement < relative_improvement_threshold:\n",
    "                        non_improving_counter += 1\n",
    "                    else:\n",
    "                        non_improving_counter = 0\n",
    "\n",
    "                loss_variance = np.var(window_loss)\n",
    "                should_break = (non_improving_counter >= patience) or (loss_variance < variance_threshold)\n",
    "                print(f\"EMA Loss = {ema_loss:.4f}, Relative Improvement = {relative_improvement:.4f},\", end=\" \")\n",
    "                print(f\"Variance = {loss_variance:.6f}, Non-improvement Count = {non_improving_counter}\")\n",
    "            else:\n",
    "                should_break = False\n",
    "                print(\"\")  \n",
    "\n",
    "            start_epoch += 1\n",
    "\n",
    "            # Clear intermediate tensors to free up memory\n",
    "            del train_metric_logger, val_metric_logger, train_coco_evaluator, val_coco_evaluator\n",
    "            gc.collect()\n",
    "\n",
    "            if should_break:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                print(\"Plateau reached; moving to next training step.\\n\")\n",
    "                break\n",
    "\n",
    "        step_index += 1\n",
    "\n",
    "    print('All Training Steps Complete!')\n",
    "    writer.close()\n",
    "    return checkpoints\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    checkpoints = main(best_trial)\n",
    "    # checkpoints = main(train_coco_ds, val_coco_ds, best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best train epoch is dictionary in checkpoints with highest validation f1\n",
    "best_train_epoch = max(checkpoints, key = lambda x: x['val_f1'])\n",
    "\n",
    "# initialize model with best trial config\n",
    "model = get_retinanet_model(depth=50,\n",
    "                            num_classes=len(best_trial.config[\"class_weights\"]),\n",
    "                            score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                            nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                            detections_per_img=200,\n",
    "                            fg_iou_thresh=best_trial.config[\"fg_iou_thresh\"],\n",
    "                            bg_iou_thresh=best_trial.config[\"bg_iou_thresh\"],\n",
    "                            topk_candidates=400, \n",
    "                            alpha=best_trial.config[\"alpha\"], \n",
    "                            gamma_loss=best_trial.config[\"gamma_loss\"], \n",
    "                            dropout_prob=best_trial.config[\"dropout\"],\n",
    "                            beta_loss=best_trial.config[\"beta_loss\"],\n",
    "                            lambda_loss=best_trial.config[\"lambda_loss\"],\n",
    "                            class_weights=None)\n",
    "\n",
    "# load model weights from best config's best_train_epoch\n",
    "model.load_state_dict(best_train_epoch[\"model_state_dict\"])\n",
    "\n",
    "# save model weights to .pth file\n",
    "torch.save(model.state_dict(), 'RetinaNet_ResNet50_FPN_DuckNet_' + str(datetime.now().strftime(\"%m%d%Y\")) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy checkpoints and remove model and optimizer state dicts\n",
    "checkpoints_copy = checkpoints.copy()\n",
    "for c in checkpoints_copy:\n",
    "    del c[\"model_state_dict\"]\n",
    "    del c[\"optimizer_state_dict\"]\n",
    "\n",
    "# save checkpoints list to text file\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/tuned_model_checkpoints.txt', 'w') as f:\n",
    "    for item in checkpoints_copy:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Model Inference on Test Dataset**</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of test indices and image names\n",
    "test_dict = dict(zip(test_indices, test_images))\n",
    "\n",
    "# save test_dict to text file just to be safe\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/test_dict.txt', 'w') as f:\n",
    "    for key, value in test_dict.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/', \n",
    "                                transforms = get_transform(train = False))\n",
    "\n",
    "# subset test dataset using test_indices\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size = 1, shuffle = False,\n",
    "                                               collate_fn = utils.collate_fn, num_workers = 0,\n",
    "                                               pin_memory = True)\n",
    "\n",
    "test_coco_ds = get_coco_api_from_dataset(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance = evaluate(model, data_loader_test, test_coco_ds, device=torch.device('cpu'), train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "print(f'Best trial test set mAP: {test_performance.coco_eval[\"bbox\"].stats[0]}') \n",
    "print(f'Best trial test set mAR: {test_performance.coco_eval[\"bbox\"].stats[8]}')\n",
    "print(f'Best trial test set f1 score: {calculate_f1_score(test_performance.coco_eval[\"bbox\"].stats[0], test_performance.coco_eval[\"bbox\"].stats[8])}')\n",
    "\n",
    "# Get per-class metrics\n",
    "test_class_metrics = extract_per_class_metrics(test_performance, test_coco_ds)\n",
    "\n",
    "test_class_metrics = {label_dict[k]: v for k, v in test_class_metrics.items()}\n",
    "\n",
    "print(\"Test Set Class Metrics:\")\n",
    "for class_name, metrics in test_class_metrics.items():\n",
    "    print(f\"Class: {class_name}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate performance metrics on every image in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "results = []\n",
    "\n",
    "metric = MeanAveragePrecision(iou_type=\"bbox\",\n",
    "                              class_metrics=True,\n",
    "                              max_detection_thresholds=[2, 20, 200]\n",
    "                              )\n",
    "\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "for images, targets in data_loader_test:\n",
    "    # use image_id to get image_name from image_names list\n",
    "    image_id = [target['image_id'].item() for target in targets]\n",
    "\n",
    "    # store targets as tensors\n",
    "    targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # filter targets to only include boxes and labels keys\n",
    "    ground_truth = [{k: v for k, v in t.items() if k in ('boxes', 'labels')} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images, targets)\n",
    "\n",
    "    # calculate mAP and mAR from test dataset\n",
    "    metric.update(prediction, ground_truth)\n",
    "    mean_AP = metric.compute()\n",
    "\n",
    "    # append image name to mean_AP\n",
    "    mean_AP['image_name'] = test_dict[image_id[0]]\n",
    "\n",
    "    # Append mean_AP and predictions to results list. \n",
    "    results.append(mean_AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store per-image test dataset metrics as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to create a dataframe of image names and mAP values\n",
    "img_results_df = pd.DataFrame()\n",
    "img_results_df['image_name'] = [result['image_name'] for result in results]\n",
    "img_results_df['mAP'] = [result['map'].item() for result in results]\n",
    "img_results_df['mAP_50'] = [result['map_50'].item() for result in results]\n",
    "img_results_df['mAP_75'] = [result['map_75'].item() for result in results]\n",
    "img_results_df['mAP_small'] = [result['map_small'].item() for result in results]\n",
    "img_results_df['mAP_medium'] = [result['map_medium'].item() for result in results]\n",
    "img_results_df['mAP_large'] = [result['map_large'].item() for result in results]\n",
    "img_results_df['mAR_1'] = [result['mar_1'].item() for result in results]\n",
    "img_results_df['mAR_10'] = [result['mar_10'].item() for result in results]\n",
    "img_results_df['mAR_100'] = [result['mar_100'].item() for result in results]\n",
    "img_results_df['mAR_small'] = [result['mar_small'].item() for result in results]\n",
    "img_results_df['mAR_medium'] = [result['mar_medium'].item() for result in results]\n",
    "img_results_df['mAR_large'] = [result['mar_large'].item() for result in results]\n",
    "\n",
    "# # if value is == -1.0, replace with NaN\n",
    "img_results_df = img_results_df.replace(-1.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric values are running averages in torch metrics, so the last value is the final value.\n",
    "final_metrics = img_results_df.iloc[-1]\n",
    "final_metrics = final_metrics.drop('image_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print per-image metrics for test dataset as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "# create a pretty table object\n",
    "x = PrettyTable()\n",
    "\n",
    "cols = ['Metric', 'Value']  \n",
    "\n",
    "# add column headers\n",
    "x.field_names = cols\n",
    "\n",
    "# values for column one in table are column names from final_metrics, column two are the column values. \n",
    "for i in range(len(final_metrics)):\n",
    "    x.add_row([final_metrics.index[i], f'{final_metrics[i]*100:.2f}%'])\n",
    "\n",
    "# print table\n",
    "print(x)\n",
    "\n",
    "# save table as txt file\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/testDataset_image_summary_table.txt', 'w') as f:\n",
    "    print(x, file = f)\n",
    "\n",
    "# save results_df to csv\n",
    "img_results_df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/per_image_results_test_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store per-class test dataset metrics as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_res_df = pd.DataFrame()\n",
    "\n",
    "# store 'map_per_class' and 'mar_100_per_class' from results in df\n",
    "class_res_df['image_name'] = [result['image_name'] for result in results]\n",
    "class_res_df['classes'] = [result['classes'] for result in results]\n",
    "class_res_df['map_per_class'] = [result['map_per_class'] for result in results]\n",
    "class_res_df['mar_100_per_class'] = [result['mar_100_per_class'] for result in results]\n",
    "\n",
    "# convert tensors to numpy arrays\n",
    "class_res_df['classes'] = class_res_df['classes'].apply(lambda x: x.numpy())\n",
    "class_res_df['map_per_class'] = class_res_df['map_per_class'].apply(lambda x: x.numpy())\n",
    "class_res_df['mar_100_per_class'] = class_res_df['mar_100_per_class'].apply(lambda x: x.numpy())\n",
    "\n",
    "# replace integer labels in classes column with labels using label_dict\n",
    "class_res_df['classes'] = class_res_df['classes'].apply(lambda x: [label_dict.get(i) for i in x])\n",
    "\n",
    "# replace -1.0 values in map_per_class and mar_100_per_class with NaN\n",
    "class_res_df['map_per_class'] = class_res_df['map_per_class'].apply(lambda x: np.where(x == -1.0, np.nan, x))\n",
    "class_res_df['mar_100_per_class'] = class_res_df['mar_100_per_class'].apply(lambda x: np.where(x == -1.0, np.nan, x))\n",
    "\n",
    "# if map_per_class or mar_100_per_class is NaN, delete value from list. Also delete corresponding class label.\n",
    "class_res_df['classes'] = class_res_df.apply(lambda x: [i for i, j in zip(x['classes'], x['map_per_class']) if not np.isnan(j)], axis = 1)\n",
    "class_res_df['map_per_class'] = class_res_df['map_per_class'].apply(lambda x: [i for i in x if not np.isnan(i)])\n",
    "class_res_df['mar_100_per_class'] = class_res_df['mar_100_per_class'].apply(lambda x: [i for i in x if not np.isnan(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric values are running averages in TorchMetrics. Store map and mar from last image in dataset\n",
    "classes = class_res_df['classes'].iloc[-1]\n",
    "class_map = class_res_df['map_per_class'].iloc[-1]\n",
    "class_mar_100 = class_res_df['mar_100_per_class'].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print per-class metrics for every image in test dataset as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = 'value' and all unique classes\n",
    "cols = ['Class', 'mAP', 'mAR_100']\n",
    "\n",
    "# create a pretty table object\n",
    "x = PrettyTable()\n",
    "\n",
    "# add column headers\n",
    "x.field_names = cols\n",
    "\n",
    "# classes go in first column, class_map in second column, and class_mar_100 in third column\n",
    "for i in range(len(classes)):\n",
    "    x.add_row([classes[i], f'{class_map[i]*100:.2f}%', f'{class_mar_100[i]*100:.2f}%'])\n",
    "\n",
    "# print table\n",
    "print(x)\n",
    "\n",
    "# save table as txt file\n",
    "with open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/testDataset_class_summary_table.txt', 'w') as f:\n",
    "    print(x, file = f)\n",
    "\n",
    "# save results_df to csv\n",
    "class_res_df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/per_class_results_test_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load test data into one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entire test dataset into one batch\n",
    "data_loader_test_singleBatch = torch.utils.data.DataLoader(dataset_test, batch_size = len(dataset_test), shuffle = False,\n",
    "                                                collate_fn = utils.collate_fn, num_workers = 0)\n",
    "\n",
    "# run predictions on all images in the test dataset\n",
    "images, targets = next(iter(data_loader_test_singleBatch))\n",
    "\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "# convert boxes in targets to tensors\n",
    "targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(images, targets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-process model predictions for plotting on original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image in the batch, remove all predicted boxes with scores below 0.5\n",
    "for i in range(len(predictions)):\n",
    "    predictions[i]['boxes'] = predictions[i]['boxes'][predictions[i]['scores'] > 0.5]\n",
    "    predictions[i]['labels'] = predictions[i]['labels'][predictions[i]['scores'] > 0.5]\n",
    "    predictions[i]['scores'] = predictions[i]['scores'][predictions[i]['scores'] > 0.5]\n",
    "\n",
    "# resize boxes to original image shape\n",
    "for i in range(len(images)):\n",
    "    tran_w, tran_h = images[i].shape[1], images[i].shape[2]\n",
    "    \n",
    "    images[i] = Image.open('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/' + test_images[i])\n",
    "\n",
    "    orig_w, orig_h = images[i].size\n",
    "\n",
    "    predictions[i]['boxes'] = predictions[i]['boxes'] * torch.tensor([orig_w/tran_w, \n",
    "                                                                      orig_h/tran_h, \n",
    "                                                                      orig_w/tran_w,\n",
    "                                                                      orig_h/tran_h]).view(1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>**Plot Model Predictions for Images in Test Dataset**</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bbox_predicted(ax, boxes, labels, scores): # modify plot_bbox to add confidence scores\n",
    "    # add box to the image and use label_color_map to color-code by bounding box class if exists else 'black'\n",
    "    ax.add_patch(plt.Rectangle((boxes[:, 0], boxes[:, 1]), boxes[:, 2] - boxes[:, 0], boxes[:, 3] - boxes[:, 1],\n",
    "                    fill = False,\n",
    "                    color = label_color_map[labels.item()] if labels.item() in label_color_map else 'black', \n",
    "                    linewidth = 1.5))\n",
    "    \n",
    "    # add label and score to the bounding box. concatenate label and score to one string. \n",
    "    # use label_dict to replace class numbers with class names\n",
    "    ax.text(boxes[:, 0], boxes[:, 1] - 100,\n",
    "        s = f\"{label_dict[labels.item()]} {scores.item():.2f}\",\n",
    "        color = 'black',\n",
    "        fontsize = 6,\n",
    "        verticalalignment = 'top',\n",
    "        bbox = {'color': label_color_map[labels.item()] if labels.item() in label_color_map else 'black', 'pad': 0})\n",
    "    return ax\n",
    "\n",
    "\n",
    "# function for plotting all predictions on images\n",
    "def plot_predictions(image, boxes, labels, scores, ax = None):\n",
    "    ax = img_show(image, ax = ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = get_box(boxes[i])\n",
    "        plot_bbox_predicted(ax, box, labels[i], scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 32 samples from batch in a grid of subplots.\n",
    "plt.figure(figsize = (24, 36))\n",
    "for i in range(0, 32):\n",
    "    ax = plt.subplot(8, 4, 1 + i)\n",
    "    plot_predictions(images[i], predictions[i]['boxes'], predictions[i]['labels'], predictions[i]['scores'], ax = ax)\n",
    "    plt.axis('off')\n",
    "    plt.title(test_images[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run inference on full dataset to get model estimates of abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = MAVdroneDataset(csv_file = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/preprocessed_annotations.csv',\n",
    "                                root_dir = 'C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/filtered_images/', \n",
    "                                transforms = get_transform(train = False))\n",
    "\n",
    "data_loader_all = torch.utils.data.DataLoader(dataset_all, batch_size = 1, shuffle = False,\n",
    "                                            collate_fn = utils.collate_fn, num_workers = 0,\n",
    "                                            pin_memory = True)\n",
    "\n",
    "# get model predictions for every image in data_loader_all\n",
    "model_predictions_all = []\n",
    "\n",
    "for images, targets in data_loader_all:\n",
    "    # use image_id to get image_name from image_names list\n",
    "    image_id = [target['image_id'].item() for target in targets]\n",
    "\n",
    "    # convert boxes in targets to tensors\n",
    "    targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images, targets)\n",
    "\n",
    "    # append image name to prediction\n",
    "    prediction['image_name'] = test_dict[image_id[0]]\n",
    "\n",
    "    # Append mean_AP and predictions to results list. \n",
    "    model_predictions_all.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model_predictions_all to a dataframe\n",
    "model_predictions_df = pd.DataFrame(model_predictions_all)\n",
    "\n",
    "# save csv for comparison with ground truth\n",
    "model_predictions_df.to_csv('C:/Users/exx/Deep Learning/UAV_Waterfowl_Detection/RetinaNet/model_predictions_full_dataset.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
